{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from globals import BASE_DIR, available_datasets, top_k_eval, valid_popularity, recommendation_dirpart\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from postprocess_baseline_top_k import dataset_metadata\n",
    "from evaluation_metrics import ndcg, calculate_arp_poplift, evaluation_user_group_means, jensen_shannon\n",
    "import pingouin as pg\n",
    "\n",
    "\n",
    "dataset = \"foursquaretky\" # perform for each dataset individually\n",
    "\n",
    "len(available_datasets) # make sure the available datasets are correct since this affects the Bonferroni correction of the t-tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_top_k_json(input_file, output_file, k=10):\n",
    "    \"\"\"\n",
    "    Process top-k recommendations from a JSON file, keeping only the item IDs for each user.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        with open(input_file, \"r\") as infile:\n",
    "            data = json.load(infile)\n",
    "\n",
    "        top_k_result = {}\n",
    "        for user_id, recommendations in data.items():\n",
    "            if recommendations and isinstance(recommendations[0], dict):\n",
    "                item_ids = recommendations[0][\"item_id\"][:k]\n",
    "                top_k_result[user_id] = item_ids\n",
    "\n",
    "        with open(output_file, \"w\") as outfile:\n",
    "            json.dump(top_k_result, outfile, indent=4)\n",
    "        print(f\"Processed file saved to: {output_file}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {input_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_directories(dataset, data, base_dir, recommendation_dirpart):\n",
    "    \"\"\"\n",
    "    Create output directories\n",
    "    \"\"\"\n",
    "    model_directories = {}\n",
    "    methods = [\"baseline\", \"cp\", \"cp_min_js\", \"upd\"]\n",
    "\n",
    "    def recommender_dir_combiner(dataset, modelpart, method):\n",
    "        return f\"{base_dir}{dataset}_dataset/{recommendation_dirpart}/{modelpart}/{method + '/'}top_k_recommendations.json\"\n",
    "\n",
    "    for result in data:\n",
    "        model_name = result[\"model\"]\n",
    "        model_directories[model_name] = {}\n",
    "        \n",
    "        for method in methods:\n",
    "            model_directories[model_name][method] = recommender_dir_combiner(dataset, result[\"directory\"], method)\n",
    "    \n",
    "    return model_directories\n",
    "\n",
    "def open_ground_truth_user_group(dataset, valid_popularity=valid_popularity):\n",
    "    \"\"\"Perform data splitting and user group creation.\"\"\"\n",
    "\n",
    "    train_data = pd.read_csv(f\"{BASE_DIR}{dataset}_dataset/processed_data_recbole/{dataset}_sample.train.inter\", sep=\"\\t\")\n",
    "    test_data = pd.read_csv(f\"{BASE_DIR}{dataset}_dataset/processed_data_recbole/{dataset}_sample.test.inter\", sep=\"\\t\")\n",
    "    valid_data = pd.read_csv(f\"{BASE_DIR}{dataset}_dataset/processed_data_recbole/{dataset}_sample.valid.inter\", sep=\"\\t\")\n",
    "\n",
    "    print(\"Train data, test data and valid data shapes:\")\n",
    "    print(train_data.shape, test_data.shape, valid_data.shape)\n",
    "    train_data = pd.concat([train_data, valid_data])\n",
    "    user_group_dir = f\"{BASE_DIR}{dataset}_dataset/{dataset}_user_id_popularity.json\"\n",
    "    with open(user_group_dir) as f:\n",
    "        user_groups = json.load(f)\n",
    "\n",
    "    user_groups[\"HighPop\"] = user_groups.pop(\"high\")\n",
    "    user_groups[\"LowPop\"] = user_groups.pop(\"low\")\n",
    "    user_groups[\"MedPop\"] = user_groups.pop(\"medium\")\n",
    "\n",
    "\n",
    "\n",
    "    checkin_df = train_data.copy()\n",
    "    \n",
    "    # Calculate item popularity\n",
    "    value_counts = checkin_df[\"item_id:token\"].value_counts().reset_index()\n",
    "    value_counts.columns = [\"item_id:token\", \"count\"]\n",
    "    value_counts[valid_popularity] = value_counts[\"count\"] / len(value_counts)\n",
    "    checkin_df = checkin_df.merge(\n",
    "        value_counts[[\"item_id:token\", valid_popularity]],\n",
    "        on=\"item_id:token\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    checkin_df.sort_values(by=valid_popularity, ascending=False, inplace=True)\n",
    "    item_popularity = checkin_df.drop_duplicates(subset=\"item_id:token\", keep=\"first\")[\n",
    "        [\"item_id:token\", valid_popularity]\n",
    "    ]\n",
    "\n",
    "    h_group = item_popularity.head(int(len(item_popularity) * 0.2))\n",
    "    h_group[\"item_pop_group\"] = \"h\"\n",
    "    t_group = item_popularity.tail(int(len(item_popularity) * 0.2))\n",
    "    t_group[\"item_pop_group\"] = \"t\"\n",
    "    m_group = item_popularity[\n",
    "        ~item_popularity[\"item_id:token\"].isin(h_group[\"item_id:token\"]) &\n",
    "        ~item_popularity[\"item_id:token\"].isin(t_group[\"item_id:token\"])\n",
    "    ]\n",
    "    m_group[\"item_pop_group\"] = \"m\"\n",
    "\n",
    "    item_popularity = pd.concat([h_group, m_group, t_group])\n",
    "    item_popularity.sort_values(by=valid_popularity, inplace=True, ascending=False)\n",
    "\n",
    "    upts = checkin_df.groupby(\"user_id:token\")[valid_popularity].mean().reset_index()\n",
    "    upts.columns = [\"user_id:token\", \"upts\"]\n",
    "    return train_data, test_data, user_groups, item_popularity, upts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset_metadata(dataset, recommendation_dirpart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dirs = create_model_directories(dataset, data, BASE_DIR, recommendation_dirpart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unstack_recommendations(df):\n",
    "    \"\"\"Unstack the recommendations for each user into separate rows.\"\"\"\n",
    "    unstacked_df = df.explode([\"item_id:token\"]).reset_index(drop=True)\n",
    "    return unstacked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_to_df(recommender_dir, top_k_eval=top_k_eval):\n",
    "    \"\"\" This function reads in a JSOn and returns an unstacked DataFrame with the top-k recommendations for each user.\"\"\"\n",
    "    with open(recommender_dir) as f:\n",
    "        data = json.load(f)\n",
    "    base_recommendations = []\n",
    "\n",
    "\n",
    "    for user, items in data.items():\n",
    "        for item in items:\n",
    "            base_recommendations.append({\n",
    "                \"user_id:token\": user,\n",
    "                \"item_id:token\": item\n",
    "            })\n",
    "\n",
    "    base_df = pd.DataFrame(base_recommendations)\n",
    "    base_df = unstack_recommendations(base_df)\n",
    "\n",
    "    df = base_df.groupby(\"user_id:token\").head(top_k_eval)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, user_groups, item_popularity, upts = open_ground_truth_user_group(dataset)\n",
    "user_groups[\"All\"] = user_groups[\"HighPop\"] + user_groups[\"MedPop\"] + user_groups[\"LowPop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pop_distributions(data):\n",
    "    \"\"\"Create a DataFrame with the distribution of item popularity for different user groups.\"\"\"\n",
    "    data = data.merge(item_popularity, on=\"item_id:token\", how=\"left\")\n",
    "    g1 = data.loc[data[\"user_id:token\"].isin(user_groups[\"HighPop\"])].value_counts(\"item_pop_group\", normalize=True).rename(\"g1\")\n",
    "    g2 = data.loc[data[\"user_id:token\"].isin(user_groups[\"MedPop\"])].value_counts(\"item_pop_group\", normalize=True).rename(\"g2\")\n",
    "    g3 = data.loc[data[\"user_id:token\"].isin(user_groups[\"LowPop\"])].value_counts(\"item_pop_group\", normalize=True).rename(\"g3\")\n",
    "    all = data.value_counts(\"item_pop_group\", normalize=True).rename(\"all\")\n",
    "    distr_df = pd.DataFrame([g1, g2, g3, all]).fillna(0)\n",
    "    distr_df.rename(index={\"g1\":\"HighPop\", \"g2\":\"MedPop\", \"g3\":\"LowPop\", \"all\":\"All\"}, inplace=True)\n",
    "    return distr_df\n",
    "\n",
    "\n",
    "ground_truth_distr = create_pop_distributions(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_distr(distr_df):\n",
    "    \"\"\"Preprocess the distribution DataFrame for plotting.\"\"\"\n",
    "    #distr_df.rename(index={\"g1\":\"HighPop\", \"g2\":\"MedPop\", \"g3\":\"LowPop\", \"all\":\"All\"}, inplace=True) # COMMENT IN FOR CALCULATING RESULTS, COMMENT OUT FOR PLOTTING\n",
    "    data = distr_df.to_dict()\n",
    "    result = []\n",
    "    user_groups = data[\"h\"].keys()\n",
    "\n",
    "    for group in user_groups:\n",
    "        h_value = data.get(\"h\", {}).get(group, 0)\n",
    "        m_value = data.get(\"m\", {}).get(group, 0)\n",
    "        t_value = data.get(\"t\", {}).get(group, 0)\n",
    "        \n",
    "        result.append({\n",
    "            \"user_group\": group,\n",
    "            \"h_ratio\": h_value,\n",
    "            \"m_ratio\": m_value,\n",
    "            \"t_ratio\": t_value\n",
    "        })\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting popularity distribution (no legend inside this function)\n",
    "def plot_popularity_distribution(ax, distr_df, label):\n",
    "    \"\"\" Plot the popularity distribution of items for different user groups.\"\"\"\n",
    "    desired_order = [\"LowPop\", \"MedPop\", \"HighPop\", \"All\"]\n",
    "    distr_df = distr_df.reindex(desired_order)\n",
    "    colors = plt.cm.viridis([0.1, 0.5, 0.9])\n",
    "    bars = distr_df.plot(kind=\"bar\", stacked=True, ax=ax, color=colors, legend=False, edgecolor=\"black\", linewidth=0, width=0.6)\n",
    "    if label is not None:\n",
    "        ax.set_title(f\"{label}\", fontsize=10)\n",
    "    ax.set_xlabel(\"User Groups\")\n",
    "    return distr_df\n",
    "\n",
    "def plot_line_chart(results, model_name, ax, metric):\n",
    "    \"\"\"\n",
    "    Plot scores (nDCG, ARP, PopLift) as line plots for each user group, comparing methods.\n",
    "    \"\"\"\n",
    "    methods = [\"baseline\", \"cp\", \"cp_min_js\"]\n",
    "    user_groups = [\"HighPop\", \"MedPop\", \"LowPop\", \"All\"]\n",
    "    group_labels = [\"HighPop\", \"MedPop\", \"LowPop\", \"All\"]\n",
    "\n",
    "    metric_values = {\n",
    "        method: [results[model_name][method][group][f\"{metric}\"] for group in user_groups]\n",
    "        for method in methods\n",
    "    }\n",
    "\n",
    "    metric_transposed = list(zip(*metric_values.values()))  # Group-wise scores across methods\n",
    "\n",
    "    # Plot each user group\"s metric scores as a line plot\n",
    "    colors = plt.cm.viridis([0.1, 0.5, 0.9, 0.3])\n",
    "    for idx, group_label in enumerate(group_labels):\n",
    "        ax.plot(methods, metric_transposed[idx], marker=\"o\", label=group_label, color=colors[idx])\n",
    "\n",
    "    ax.set_ylabel(f\"{metric.upper()} Score\", fontsize=12)\n",
    "    ax.grid(True)\n",
    "    ax.tick_params(axis=\"x\", labelsize=16)\n",
    "    ax.tick_params(axis=\"y\", labelsize=16)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_t_test_between_user_groups(group_scores):\n",
    "    \"\"\" Calculate t-test for two-sample t-test between low and high groups. \n",
    "    Source t-test: https://www.geeksforgeeks.org/how-to-conduct-a-two-sample-t-test-in-python/\"\"\"\n",
    "    \n",
    "    ttest = {}\n",
    "    # Conducting two-sample ttest\n",
    "    result_low_high = pg.ttest(list(group_scores[\"LowPop\"].values()), \n",
    "                    list(group_scores[\"HighPop\"].values()),\n",
    "                    correction=True)\n",
    "    \n",
    "    result_low_med = pg.ttest(list(group_scores[\"LowPop\"].values()), \n",
    "                    list(group_scores[\"MedPop\"].values()),\n",
    "                    correction=True)\n",
    "    \n",
    "    ttest[\"low_high\"] = float(result_low_high[\"p-val\"])\n",
    "    ttest[\"low_medium\"] = float(result_low_med[\"p-val\"])\n",
    "\n",
    "    return ttest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Looping through the models and methods to plot the popularity distribution \"\"\"\n",
    "fig, axs = plt.subplots(2, 4, figsize=(7, 3.5))  # 3 models x 4 methods\n",
    "method_names_for_title = [\"$Base$\", \"$CP_H$\", \"$CP_\\Im$\"]\n",
    "\n",
    "# model_dirs_plot = copy.deepcopy(model_dirs)\n",
    "for methods in model_dirs.values():\n",
    "    methods.pop(\"upd\", None)\n",
    "\n",
    "for i, (model_name, methods) in enumerate(model_dirs.items()):\n",
    "    if model_name not in [\"SimpleX\", \"USG\"]:\n",
    "        \n",
    "        ax = axs[i, 0]\n",
    "        plot_popularity_distribution(ax, ground_truth_distr, None)  # no title here\n",
    "        if i == 0:\n",
    "            ax.set_title(\"User Profile\", fontsize=11)\n",
    "\n",
    "        for j, (method_name, json_file) in enumerate(methods.items()):\n",
    "            df = top_k_to_df(json_file)\n",
    "            distr_df = create_pop_distributions(df)\n",
    "\n",
    "            ax = axs[i, j + 1]\n",
    "            plot_popularity_distribution(ax, distr_df, None)  # no title here\n",
    "            if i == 0:\n",
    "                ax.set_title(method_names_for_title[j], fontsize=11)\n",
    "\n",
    "\n",
    "        axs[i, 0].set_ylabel(model_name, fontsize=10, weight=\"bold\", labelpad=0)\n",
    "\n",
    "        if i == 3:\n",
    "                    for k in range(4):\n",
    "                        axs[i, k].set_xticks(range(4))\n",
    "                        axs[i, k].set_xlabel(\"User Groups\")\n",
    "        else:\n",
    "            for k in range(4):\n",
    "                axs[i, k].set_xlabel(\"\")\n",
    "\n",
    "for ax in axs[-1, :]:\n",
    "    ax.set_xticks(range(4))\n",
    "    ax.set_xticklabels([\"LowPop\", \"MedPop\", \"HighPop\", \"All\"], rotation=45, fontsize=9)\n",
    "\n",
    "for row in axs[:-1, :]:\n",
    "    for ax in row:\n",
    "        ax.set_xticks([])\n",
    "\n",
    "for row in axs:\n",
    "    for ax in row[1:]:\n",
    "        ax.set_yticks([])\n",
    "\n",
    "\n",
    "fig.text(0.55, 0.03, f\"User Groups ({dataset.capitalize()})\", ha=\"center\", fontsize=11)\n",
    "fig.text(0.04, 0.5, \"Item Group Ratios\", va=\"center\", rotation=\"vertical\", fontsize=11)\n",
    "\n",
    "handles = [plt.Line2D([0], [0], color=color, lw=4) for color in plt.cm.viridis([0.9, 0.5, 0.1])]\n",
    "labels = [\"T\", \"M\", \"H\"]\n",
    "\n",
    "\n",
    "fig.legend(handles, labels, title=\"Item Groups\", loc=\"center right\", bbox_to_anchor=(1.12, 0.5), ncol=1, title_fontsize=8)\n",
    "#fig.suptitle(f\"Popularity Distribution for {dataset.capitalize()} Dataset\", fontsize=12, weight=\"bold\", y=0.87, x=0.57)\n",
    "plt.tight_layout(rect=[0.05, 0.03, 1, 0.91])\n",
    "plt.savefig(f\"{BASE_DIR}/{dataset}_dataset/plots/popularity_distribution_paper.png\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Looping through the models and methods to store and process the results \"\"\"\n",
    "group_scores_ndcg = {}\n",
    "group_scores_arp = {}\n",
    "group_scores_poplift = {}\n",
    "results = {}\n",
    "ttest_results = {}\n",
    "\n",
    "distr_dict_ground_truth = preprocess_distr(ground_truth_distr)\n",
    "\n",
    "for i, (model_name, methods) in enumerate(model_dirs.items()):\n",
    "    if model_name not in [\"SimpleX\"]:\n",
    "        results[model_name] = {}\n",
    "        ttest_results[model_name] = {}\n",
    "\n",
    "        group_scores_ndcg[model_name] = {}\n",
    "        group_scores_arp[model_name] = {}\n",
    "        group_scores_poplift[model_name] = {}\n",
    "\n",
    "        for j, (method_name, json_file) in enumerate(methods.items()):\n",
    "\n",
    "            ttest_results[model_name][method_name] = {}\n",
    "  \n",
    "            df = top_k_to_df(json_file)\n",
    "\n",
    "            ndcg_scores = ndcg(df, test_data, top_k_eval)\n",
    "            df_with_item_pop = df.merge(item_popularity, on=\"item_id:token\", how=\"left\")\n",
    "            arp_scores, poplift_scores = calculate_arp_poplift(df_with_item_pop, item_popularity, upts, valid_popularity)\n",
    "            distr_df = create_pop_distributions(df)\n",
    "            distr_dict_recs = preprocess_distr(distr_df)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            jsd_group = {}\n",
    "            for group_gt, group_recs in zip(distr_dict_ground_truth, distr_dict_recs):\n",
    "                jsd_group[group_gt[\"user_group\"]] = jensen_shannon(group_gt, group_recs)\n",
    "\n",
    "            group_eval, group_ndcg_scores, group_arp_scores, group_poplift_scores = evaluation_user_group_means(ndcg_scores, arp_scores, poplift_scores, user_groups, df)\n",
    "\n",
    "            group_scores_ndcg[model_name][method_name] = group_ndcg_scores\n",
    "            group_scores_arp[model_name][method_name] = group_arp_scores\n",
    "            group_scores_poplift[model_name][method_name] = group_poplift_scores\n",
    "\n",
    "            # ttests between low and high / low and medium to check whether LowPop users are disadvantaged\n",
    "            # ttest_results[model_name][method_name][\"ndcg\"] = calculate_t_test_between_user_groups(group_ndcg_scores)\n",
    "            # ttest_results[model_name][method_name][\"arp\"] = calculate_t_test_between_user_groups(group_arp_scores)\n",
    "            # ttest_results[model_name][method_name][\"poplift\"] = calculate_t_test_between_user_groups(group_poplift_scores)\n",
    "        \n",
    "            for group_name, jsd_value in jsd_group.items():\n",
    "                if group_name in group_eval:\n",
    "                    group_eval[group_name][\"js\"] = jsd_value\n",
    "                else:\n",
    "                    # In case the group exists in jsd_group but not in group_eval\n",
    "                    group_eval[group_name] = {\"js\": jsd_value}\n",
    "\n",
    "            results[model_name][method_name] = group_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_tests_rq1(metric_scores, model_1=\"BPR\", other_models=[\"LORE\", \"USG\"]):\n",
    "    \"\"\" Perform t-tests between algorithms/methods to answer RQ1 (Compare Individual Effects of Context-Aware Recommendation and CP to general Baseline)\n",
    "    Source t-test: https://www.geeksforgeeks.org/how-to-conduct-a-two-sample-t-test-in-python/\"\"\"\n",
    "\n",
    "    ttest_results = {}\n",
    "\n",
    "    # Compare to other models (LORE, USG)\n",
    "    for other_model in other_models:\n",
    "        ttest_results[other_model] = {}\n",
    "        for group in [\"HighPop\", \"MedPop\", \"LowPop\", \"All\"]:\n",
    "            baseline_1 = metric_scores.get(model_1, {}).get(\"baseline\", {}).get(group, {})\n",
    "            baseline_2 = metric_scores.get(other_model, {}).get(\"baseline\", {}).get(group, {})\n",
    "\n",
    "            common_users = set(baseline_1.keys()) & set(baseline_2.keys())\n",
    "\n",
    "            if common_users:\n",
    "                vals_1 = [baseline_1[u] for u in common_users]\n",
    "                vals_2 = [baseline_2[u] for u in common_users]\n",
    "\n",
    "                test = pg.ttest(vals_1, vals_2, paired=True, correction=True)\n",
    "                delta = np.mean(vals_2) - np.mean(vals_1)\n",
    "\n",
    "                ttest_results[other_model][group] = {\n",
    "                    \"p_val\": float(test[\"p-val\"].values[0]),\n",
    "                    \"delta\": float(delta),\n",
    "                }\n",
    "            else:\n",
    "                ttest_results[other_model][group] = {\n",
    "                    \"p_val\": None,\n",
    "                    \"delta\": None,\n",
    "                }\n",
    "\n",
    "    # Compare BPR baseline to its own variants\n",
    "    for variant in [\"cp\", \"cp_min_js\"]:\n",
    "        ttest_results[variant] = {}\n",
    "        for group in [\"HighPop\", \"MedPop\", \"LowPop\", \"All\"]:\n",
    "            baseline = metric_scores.get(model_1, {}).get(\"baseline\", {}).get(group, {})\n",
    "            variant_scores = metric_scores.get(model_1, {}).get(variant, {}).get(group, {})\n",
    "\n",
    "            common_users = set(baseline.keys()) & set(variant_scores.keys())\n",
    "\n",
    "            if common_users:\n",
    "                vals_base = [baseline[u] for u in common_users]\n",
    "                vals_variant = [variant_scores[u] for u in common_users]\n",
    "\n",
    "                test = pg.ttest(vals_base, vals_variant, paired=True, correction=True)\n",
    "                delta = np.mean(vals_variant) - np.mean(vals_base)\n",
    "\n",
    "                ttest_results[variant][group] = {\n",
    "                    \"p_val\": float(test[\"p-val\"].values[0]),\n",
    "                    \"delta\": float(delta),\n",
    "                }\n",
    "            else:\n",
    "                ttest_results[variant][group] = {\n",
    "                    \"p_val\": None,\n",
    "                    \"delta\": None,\n",
    "                }\n",
    "\n",
    "    return ttest_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_tests_rq2(group_scores):\n",
    "    \"\"\" Perform t-tests between algorithms/methods to answer RQ2 (Compare Combined Effects of Context-Aware Recommendation and CP to the Respective Context-Aware Baseline) \"\"\"\n",
    "    ttest_results = {}\n",
    "\n",
    "    for model_name, methods in group_scores.items():\n",
    "        ttest_results[model_name] = {}\n",
    "\n",
    "        for group in [\"HighPop\", \"MedPop\", \"LowPop\", \"All\"]:\n",
    "            ttest_results[model_name][group] = {}\n",
    "\n",
    "            baseline = methods.get(\"baseline\", {}).get(group, {})\n",
    "            cp = methods.get(\"cp\", {}).get(group, {})\n",
    "            cp_min_js = methods.get(\"cp_min_js\", {}).get(group, {})\n",
    "\n",
    "            def clean(d):\n",
    "                return [float(v) for v in d.values() if isinstance(v, (float, int))]\n",
    "\n",
    "            baseline_vals = clean(baseline)\n",
    "            cp_vals = clean(cp)\n",
    "            cp_min_js_vals = clean(cp_min_js)\n",
    "\n",
    "            # Compare baseline vs cp\n",
    "            if baseline_vals and cp_vals:\n",
    "                result_cp = pg.ttest(baseline_vals, cp_vals, correction=True)\n",
    "                delta_cp = np.mean(cp_vals) - np.mean(baseline_vals)\n",
    "                ttest_results[model_name][group][\"cp\"] = {\n",
    "                    \"p_val\": float(result_cp[\"p-val\"].values[0]),\n",
    "                    \"delta\": float(delta_cp)\n",
    "                }\n",
    "            else:\n",
    "                ttest_results[model_name][group][\"cp\"] = {\n",
    "                    \"p_val\": None,\n",
    "                    \"delta\": None\n",
    "                }\n",
    "\n",
    "            # Compare baseline vs cp_min_js\n",
    "            if baseline_vals and cp_min_js_vals:\n",
    "                result_cp_min_js = pg.ttest(baseline_vals, cp_min_js_vals, correction=True)\n",
    "                delta_cp_min_js = np.mean(cp_min_js_vals) - np.mean(baseline_vals)\n",
    "                ttest_results[model_name][group][\"cp_min_js\"] = {\n",
    "                    \"p_val\": float(result_cp_min_js[\"p-val\"].values[0]),\n",
    "                    \"delta\": float(delta_cp_min_js)\n",
    "                }\n",
    "            else:\n",
    "                ttest_results[model_name][group][\"cp_min_js\"] = {\n",
    "                    \"p_val\": None,\n",
    "                    \"delta\": None\n",
    "                }\n",
    "\n",
    "    return ttest_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_rq1_ndcg = t_tests_rq1(group_scores_ndcg)\n",
    "ttest_rq1_arp = t_tests_rq1(group_scores_arp)\n",
    "ttest_rq1_poplift = t_tests_rq1(group_scores_poplift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_rq2_ndcg = t_tests_rq2(group_scores_ndcg)\n",
    "ttest_rq2_arp = t_tests_rq2(group_scores_arp)\n",
    "ttest_rq2_poplift = t_tests_rq2(group_scores_poplift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_results_rq1_with_deltas(\n",
    "    general_results,\n",
    "    ttest_rq1_ndcg,\n",
    "    ttest_rq1_arp,\n",
    "    ttest_rq1_poplift\n",
    "):\n",
    "    \n",
    "    \"\"\" Format the results to put them into a table inlcuding the Delta in Percent and +/-\"\"\"\n",
    "    \n",
    "    updated_results = copy.deepcopy(general_results[\"BPR\"][\"baseline\"])\n",
    "\n",
    "    ttest_dicts = {\n",
    "        \"ndcg\": ttest_rq1_ndcg,\n",
    "        \"arp\": ttest_rq1_arp,\n",
    "        \"poplift\": ttest_rq1_poplift,\n",
    "    }\n",
    "\n",
    "    formatted_table = {}\n",
    "\n",
    "    for group in updated_results:\n",
    "        formatted_table[group] = {}\n",
    "        for metric in [\"ndcg\", \"arp\", \"poplift\"]:\n",
    "            base_val = updated_results[group][metric]\n",
    "            base_str = f\"{base_val:.4f}\"\n",
    "            row = {\"BPR\": base_str}\n",
    "\n",
    "            for model in [\"LORE\", \"USG\"]:\n",
    "                delta = ttest_dicts[metric][model][group][\"delta\"]\n",
    "                p_val = ttest_dicts[metric][model][group][\"p_val\"]\n",
    "\n",
    "                if base_val != 0:\n",
    "                    percent_delta = (delta / base_val) * 100\n",
    "                    delta_str = f\"{percent_delta:+.2f}%\"\n",
    "                else:\n",
    "                    delta_str = \"N/A\"\n",
    "\n",
    "                if p_val is not None and p_val < 0.05/(16*len(available_datasets)):\n",
    "                    delta_str += \" **\"\n",
    "\n",
    "                row[model] = delta_str\n",
    "\n",
    "            # Now compare BPR(cp) and BPR(cp_min_js) to BPR(baseline)\n",
    "            for variant in [\"cp\", \"cp_min_js\"]:\n",
    "                delta = ttest_dicts[metric][variant][group][\"delta\"]\n",
    "                p_val = ttest_dicts[metric][variant][group][\"p_val\"]\n",
    "\n",
    "                # Get the BPR baseline again for consistent comparison\n",
    "                if base_val != 0:\n",
    "                    percent_delta = (delta / base_val) * 100\n",
    "                    delta_str = f\"{percent_delta:+.2f}%\"\n",
    "                else:\n",
    "                    delta_str = \"N/A\"\n",
    "\n",
    "                if p_val is not None and p_val < 0.05/(16*len(available_datasets)):\n",
    "                    delta_str += \" **\"\n",
    "\n",
    "                row[variant] = delta_str\n",
    "\n",
    "            formatted_table[group][metric] = row\n",
    "\n",
    "    return formatted_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rq1_results = format_results_rq1_with_deltas(results, ttest_rq1_ndcg, ttest_rq1_arp, ttest_rq1_poplift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_results_rq2_with_deltas(\n",
    "    general_results,\n",
    "    ttest_rq2_ndcg,\n",
    "    ttest_rq2_arp,\n",
    "    ttest_rq2_poplift\n",
    "):\n",
    "    \"\"\" Format the results to put them into a table inlcuding the Delta in Percent and +/-\"\"\"\n",
    "    updated_results = copy.deepcopy(general_results)\n",
    "    ttest_dicts = {\n",
    "        \"ndcg\": ttest_rq2_ndcg,\n",
    "        \"arp\": ttest_rq2_arp,\n",
    "        \"poplift\": ttest_rq2_poplift,\n",
    "    }\n",
    "\n",
    "    for model in general_results:\n",
    "        for group in general_results[model][\"baseline\"]:\n",
    "            for metric in [\"ndcg\", \"arp\", \"poplift\", \"gini\", \"js\"]:\n",
    "                base_val = general_results[model][\"baseline\"][group][metric]\n",
    "                # Format baseline value to 4 decimal places\n",
    "                base_str = f\"{base_val:.4f}\"\n",
    "                updated_results[model][\"baseline\"][group][metric] = base_str\n",
    "\n",
    "            for method in [\"cp\", \"cp_min_js\"]:\n",
    "                if method not in general_results[model]:\n",
    "                    continue\n",
    "                if group not in general_results[model][method]:\n",
    "                    continue\n",
    "\n",
    "                for metric in [\"ndcg\", \"arp\", \"poplift\"]:\n",
    "                    cur_val = general_results[model][method][group][metric]\n",
    "                    ttest_dict = ttest_dicts[metric]\n",
    "                    try:\n",
    "                        delta = ttest_dict[model][group][method][\"delta\"]\n",
    "                        p_val = ttest_dict[model][group][method][\"p_val\"]\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "\n",
    "                    # Convert delta to percent relative to baseline\n",
    "                    base_val = float(general_results[model][\"baseline\"][group][metric])\n",
    "                    delta_percent = ((cur_val - base_val) / abs(base_val)) * 100 if base_val != 0 else 0\n",
    "                    delta_str = f\"{delta_percent:+.2f}%\"\n",
    "                    if p_val < 0.05/32:       # Bonferroni correction for multiple comparisons\n",
    "                        delta_str += \"**\"\n",
    "\n",
    "                    # Format the current value to 4 decimal places and add the delta\n",
    "                    formatted = f\"{cur_val:.4f} ({delta_str})\"\n",
    "                    updated_results[model][method][group][metric] = formatted\n",
    "\n",
    "    return updated_results\n",
    "\n",
    "\n",
    "df_results_rq2 = format_results_rq2_with_deltas(results, ttest_rq2_ndcg, ttest_rq2_arp, ttest_rq2_poplift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the data for RQ1\n",
    "rows = []\n",
    "for group_name, metrics in rq1_results.items():\n",
    "    for model in [\"BPR\", \"LORE\", \"USG\", \"cp\", \"cp_min_js\"]:\n",
    "        row = {\n",
    "            \"method\": model,\n",
    "            \"user_group\": group_name,\n",
    "            \"ndcg\": metrics[\"ndcg\"][model],\n",
    "            \"arp\": metrics[\"arp\"][model],\n",
    "            \"poplift\": metrics[\"poplift\"][model]\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "df_results_rq1 = pd.DataFrame(rows)\n",
    "df_results_rq1.to_csv(f\"{BASE_DIR}/{dataset}_dataset/evaluation_results_rq1.csv\", index=False)\n",
    "\n",
    "\n",
    "# Saving the data for RQ2\n",
    "rows = []\n",
    "for model_name, methods in df_results_rq2.items():\n",
    "    for method_name, group_eval in methods.items():\n",
    "        for group_name, metrics in group_eval.items():\n",
    "            row = {\n",
    "                \"dataset\": dataset,\n",
    "                \"model\": model_name,\n",
    "                \"method\": method_name,\n",
    "                \"user_group\": group_name,\n",
    "                \"ndcg\": metrics[\"ndcg\"],\n",
    "                \"arp\": metrics[\"arp\"],\n",
    "                \"poplift\": metrics[\"poplift\"],\n",
    "                \"js\": metrics[\"js\"],\n",
    "                \"gini\": metrics[\"gini\"],\n",
    "                \n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "df_results_rq2 = pd.DataFrame(rows)\n",
    "df_results_rq2.to_csv(f\"{BASE_DIR}/{dataset}_dataset/evaluation_results_rq2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_rq1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_rq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(results, metric, dataset):\n",
    "    \"\"\"Plot a line chart with the evolution of each metric\"\"\"\n",
    "    n_models = len(results)\n",
    "    n_cols = 3\n",
    "    n_rows = math.ceil(n_models / n_cols)\n",
    "\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(15, 3 * n_rows))\n",
    "    axs = axs.flatten()  # Flatten to simplify indexing\n",
    "\n",
    "    for i, (model_name, methods) in enumerate(results.items()):\n",
    "        plot_line_chart(results, model_name, axs[i], metric)\n",
    "        axs[i].set_ylabel(model_name, fontsize=14, weight=\"bold\")\n",
    "\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        fig.delaxes(axs[j])  # Remove unused subplots\n",
    "\n",
    "    fig.suptitle(f\"{metric.upper()} Comparison for {dataset}\", fontsize=18, weight=\"bold\", x=0.5)\n",
    "\n",
    "    ndcg_handles = [plt.Line2D([0], [0], color=color, lw=2, marker=\"o\") for color in plt.cm.viridis([0.2, 0.6, 0.8, 0.3])]\n",
    "    ndcg_labels = [\"HighPop\", \"MedPop\", \"LowPop\", \"All\"]\n",
    "    fig.legend(ndcg_handles, ndcg_labels, title=\"User Groups\", loc=\"upper center\", bbox_to_anchor=(0.5, 0.92), ncol=4)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.02, 1, 0.88])  # Reduced bottom space\n",
    "    plt.savefig(f\"{BASE_DIR}{dataset}_dataset/plots/{dataset}_{metric}_comparison.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plot_popularity_distributions(model_dirs, ground_truth_distr, dataset)\n",
    "plot_metric(results, \"ndcg\", dataset)\n",
    "plot_metric(results, \"arp\", dataset)\n",
    "plot_metric(results, \"poplift\", dataset)\n",
    "plot_metric(results, \"js\", dataset)\n",
    "plot_metric(results, \"gini\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
