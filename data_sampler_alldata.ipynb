{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from globals import BASE_DIR\n",
    "\n",
    "available_datasets = [\"foursquaretky\", \"yelp\", \"gowalla\", \"brightkite\", \"snowcard\"]\n",
    "\n",
    "\n",
    "dataset = \"gowalla\" # beware: opening the yelp file with pandas will take a lot of time, approx 10 min on my machine\n",
    "include_categories = False # for context-aware recommendation\n",
    "\n",
    "DATASET_DIR = f\"{BASE_DIR}{dataset}_dataset/\"\n",
    "#DATASET_DIR = f\"/Users/andreaforster/Documents/data_thesis/{dataset}_dataset/\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_big_json(file_path):\n",
    "    \"\"\"This function is used to open the Yelp data\"\"\"\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_unix_timestamp(df, column_name):\n",
    "    \"\"\"\n",
    "    Convert a column of timestamps in a DataFrame to Unix timestamps.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the timestamp column.\n",
    "        column_name (str): The name of the column with timestamps in \"%Y-%m-%d %H:%M:%S\" format.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with an additional column for Unix timestamps.\n",
    "    \"\"\"\n",
    "    df[column_name] = pd.to_datetime(df[column_name], format=\"mixed\")\n",
    "    \n",
    "    df[f'{column_name}'] = df[column_name].apply(lambda x: x.timestamp())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset-specific preprocessing\n",
    "if dataset == \"snowcard\":\n",
    "    checkin_df = pd.read_csv(DATASET_DIR+\"TSC_EEL_EXPORT.csv\", encoding=\"latin1\", sep=\";\", header=None, names=[\"timestamp:float\", \"user_id:token\", \"category_id:token\", \"category_name:token_seq\", \"name:token_seq\", \"user_type:token_seq\"])\n",
    "    checkin_df[\"item_id:token\"], item_id = pd.factorize(checkin_df[\"name:token_seq\"])\n",
    "    user_df = checkin_df[[\"user_id:token\", \"user_type:token_seq\"]].drop_duplicates(subset=[\"user_id:token\"])\n",
    "    poi_df = checkin_df[[\"item_id:token\", \"name:token_seq\", \"category_id:token\", \"category_name:token_seq\"]].drop_duplicates(subset=[\"item_id:token\"])\n",
    "    coordinates_df = pd.read_excel(DATASET_DIR+\"snowcard_lifts.xlsx\")\n",
    "    coordinates_df[[\"lat:float\", \"lon:float\"]] = coordinates_df[\"lat_lon\"].str.split(', ', expand=True)\n",
    "    coordinates_df.drop(columns=[\"lat_lon\"], inplace=True)\n",
    "    poi_df = pd.merge(poi_df, coordinates_df[['category_name:token_seq', 'lat:float', 'lon:float']], on='category_name:token_seq', how='left')\n",
    "    checkin_df = checkin_df[[\"user_id:token\", \"item_id:token\", \"timestamp:float\"]]\n",
    "\n",
    "elif dataset == \"foursquarenyc\" or dataset == \"foursquaretky\":\n",
    "    checkin_df = pd.read_csv(DATASET_DIR + \"foursquare_data.csv\", sep=\",\")\n",
    "    checkin_df = checkin_df.drop(columns=[\"timezoneOffset\"])\n",
    "    checkin_df = checkin_df.rename(columns={\"venueId\": \"item_id:token\", \"venueCategoryId\": \"category_id:token\", \"venueCategory\": \"category_name:token_seq\", \"userId\": \"user_id:token\", \"utcTimestamp\": \"timestamp:float\", \"latitude\": \"lat:float\", \"longitude\": \"lon:float\"})\n",
    "    user_df = checkin_df[[\"user_id:token\"]].drop_duplicates()\n",
    "\n",
    "    poi_df = checkin_df[[\"item_id:token\", \"category_id:token\", \"category_name:token_seq\", \"lat:float\", \"lon:float\"]].drop_duplicates(subset=[\"item_id:token\"])\n",
    "    checkin_df = checkin_df[[\"user_id:token\", \"item_id:token\", \"timestamp:float\"]]\n",
    "\n",
    "elif dataset == \"gowalla\" or dataset == \"brightkite\":\n",
    "    checkin_df = pd.read_csv(DATASET_DIR + f\"loc-{dataset}_totalCheckins.txt\", sep=\"\\t\", header=None, names=['user_id:token', 'timestamp:float', 'lat:float', 'lon:float', 'item_id:token'])\n",
    "    checkin_df = checkin_df[~checkin_df['item_id:token'].isin([\"00000000000000000000000000000000\", \"ede07eeea22411dda0ef53e233ec57ca\"])]\n",
    "    user_df = pd.read_csv(DATASET_DIR + f\"loc-{dataset}_edges.txt\", sep=\"\\t\", header=None, names=['user_id:token', 'friends:token_seq'])\n",
    "    user_df = user_df.groupby('user_id:token')['friends:token_seq'].apply(lambda x: ','.join(map(str, x))).reset_index()\n",
    "    user_df.columns = ['user_id:token', 'friends:token_seq']\n",
    "    poi_df = checkin_df[['item_id:token', \"lat:float\", \"lon:float\"]].drop_duplicates(subset=\"item_id:token\")\n",
    "    checkin_df = checkin_df.drop(columns=[\"lat:float\", \"lon:float\"])\n",
    "\n",
    "elif dataset == \"yelp\":\n",
    "    poi_df = pd.read_json(DATASET_DIR + \"yelp_academic_dataset_business.json\", lines=True)\n",
    "    poi_df = poi_df.loc[poi_df['is_open'] == 1]\n",
    "    poi_df = poi_df.drop(columns=[\"review_count\", \"stars\", \"hours\", \"is_open\", \"city\", \"state\", \"postal_code\", \"attributes\", \"address\"])\n",
    "    poi_df = poi_df.rename(columns={\"latitude\": \"lat:float\", \"longitude\": \"lon:float\", \"business_id\": \"item_id:token\", \"name\":\"name:token_seq\", \"categories\":\"category_name:token_seq\"})\n",
    "    user_df = open_big_json(DATASET_DIR + \"yelp_academic_dataset_user.json\")\n",
    "    user_df = user_df.drop(columns=[\"review_count\", \"name\", \"yelping_since\", \"useful\", \"funny\", \"cool\", \"elite\", \"fans\", \"compliment_hot\", \"average_stars\", \"compliment_more\", \"compliment_profile\", \"compliment_cute\", \"compliment_list\", \"compliment_note\", \"compliment_plain\", \"compliment_cool\", \"compliment_funny\", \"compliment_writer\", \"compliment_photos\"])\n",
    "    user_df = user_df.rename(columns={\"user_id\": \"user_id:token\", \"friends\": \"friends:token_seq\"})\n",
    "    checkin_df = open_big_json(DATASET_DIR + \"yelp_academic_dataset_review.json\")\n",
    "    checkin_df = checkin_df.drop(columns=[\"text\", \"cool\", \"stars\", \"useful\", \"funny\", \"review_id\"])\n",
    "    checkin_df = checkin_df.rename(columns={\"user_id\": \"user_id:token\", \"business_id\": \"item_id:token\", \"date\": \"timestamp:float\"})\n",
    "    checkin_df['timestamp'] = pd.to_datetime(checkin_df['timestamp:float'], errors='coerce')\n",
    "\n",
    "    checkin_df['year'] = checkin_df['timestamp'].dt.year      # Extract the year from the 'timestamp' column\n",
    "    checkin_df = checkin_df[checkin_df['year'] >= 2018]       # Keep only the check-ins from 2018 and 2019\n",
    "    checkin_df = checkin_df[checkin_df['year'] < 2020]\n",
    "    checkin_df.drop(columns=[\"year\", \"timestamp\"], inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df.sort_values(by=\"timestamp:float\", ascending=True, inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id:token</th>\n",
       "      <th>lat:float</th>\n",
       "      <th>lon:float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22847</td>\n",
       "      <td>30.235909</td>\n",
       "      <td>-97.795140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>420315</td>\n",
       "      <td>30.269103</td>\n",
       "      <td>-97.749395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316637</td>\n",
       "      <td>30.255731</td>\n",
       "      <td>-97.763386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16516</td>\n",
       "      <td>30.263418</td>\n",
       "      <td>-97.757597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5535878</td>\n",
       "      <td>30.274292</td>\n",
       "      <td>-97.740523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6442859</th>\n",
       "      <td>1111497</td>\n",
       "      <td>51.521618</td>\n",
       "      <td>-0.148219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6442860</th>\n",
       "      <td>1044625</td>\n",
       "      <td>51.883750</td>\n",
       "      <td>-1.757373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6442861</th>\n",
       "      <td>845387</td>\n",
       "      <td>51.917208</td>\n",
       "      <td>-0.660990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6442865</th>\n",
       "      <td>1341442</td>\n",
       "      <td>51.712110</td>\n",
       "      <td>-0.050458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6442891</th>\n",
       "      <td>4555073</td>\n",
       "      <td>50.027812</td>\n",
       "      <td>8.785098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1280969 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         item_id:token  lat:float  lon:float\n",
       "0                22847  30.235909 -97.795140\n",
       "1               420315  30.269103 -97.749395\n",
       "2               316637  30.255731 -97.763386\n",
       "3                16516  30.263418 -97.757597\n",
       "4              5535878  30.274292 -97.740523\n",
       "...                ...        ...        ...\n",
       "6442859        1111497  51.521618  -0.148219\n",
       "6442860        1044625  51.883750  -1.757373\n",
       "6442861         845387  51.917208  -0.660990\n",
       "6442865        1341442  51.712110  -0.050458\n",
       "6442891        4555073  50.027812   8.785098\n",
       "\n",
       "[1280969 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id:token</th>\n",
       "      <th>friends:token_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0,2,9,52,53,55,68,88,97,111,116,154,169,185,20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0,1,3,5,22,36,37,41,44,53,66,67,82,83,88,111,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0,2,22,41,88,97,111,145,154,162,178,185,186,19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0,154,191,234,235,267,347,405,464,557,3527,352...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196586</th>\n",
       "      <td>196586</td>\n",
       "      <td>196539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196587</th>\n",
       "      <td>196587</td>\n",
       "      <td>196540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196588</th>\n",
       "      <td>196588</td>\n",
       "      <td>196540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196589</th>\n",
       "      <td>196589</td>\n",
       "      <td>196547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196590</th>\n",
       "      <td>196590</td>\n",
       "      <td>196561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196591 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id:token                                  friends:token_seq\n",
       "0                   0  1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,1...\n",
       "1                   1  0,2,9,52,53,55,68,88,97,111,116,154,169,185,20...\n",
       "2                   2  0,1,3,5,22,36,37,41,44,53,66,67,82,83,88,111,1...\n",
       "3                   3  0,2,22,41,88,97,111,145,154,162,178,185,186,19...\n",
       "4                   4  0,154,191,234,235,267,347,405,464,557,3527,352...\n",
       "...               ...                                                ...\n",
       "196586         196586                                             196539\n",
       "196587         196587                                             196540\n",
       "196588         196588                                             196540\n",
       "196589         196589                                             196547\n",
       "196590         196590                                             196561\n",
       "\n",
       "[196591 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id:token</th>\n",
       "      <th>timestamp:float</th>\n",
       "      <th>item_id:token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33619</th>\n",
       "      <td>138</td>\n",
       "      <td>2009-02-04T05:17:38Z</td>\n",
       "      <td>9056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6282</th>\n",
       "      <td>24</td>\n",
       "      <td>2009-02-05T06:27:43Z</td>\n",
       "      <td>8904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33618</th>\n",
       "      <td>138</td>\n",
       "      <td>2009-02-17T19:28:30Z</td>\n",
       "      <td>8957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33617</th>\n",
       "      <td>138</td>\n",
       "      <td>2009-02-17T19:31:59Z</td>\n",
       "      <td>8956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33616</th>\n",
       "      <td>138</td>\n",
       "      <td>2009-02-18T18:50:55Z</td>\n",
       "      <td>9208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6364845</th>\n",
       "      <td>186372</td>\n",
       "      <td>2010-10-23T04:17:47Z</td>\n",
       "      <td>5452252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6435132</th>\n",
       "      <td>195001</td>\n",
       "      <td>2010-10-23T04:37:34Z</td>\n",
       "      <td>5977757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6439961</th>\n",
       "      <td>196005</td>\n",
       "      <td>2010-10-23T04:38:02Z</td>\n",
       "      <td>5577722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6422034</th>\n",
       "      <td>192853</td>\n",
       "      <td>2010-10-23T05:18:36Z</td>\n",
       "      <td>349086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6440430</th>\n",
       "      <td>196061</td>\n",
       "      <td>2010-10-23T05:22:06Z</td>\n",
       "      <td>155746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6442892 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id:token       timestamp:float  item_id:token\n",
       "33619              138  2009-02-04T05:17:38Z           9056\n",
       "6282                24  2009-02-05T06:27:43Z           8904\n",
       "33618              138  2009-02-17T19:28:30Z           8957\n",
       "33617              138  2009-02-17T19:31:59Z           8956\n",
       "33616              138  2009-02-18T18:50:55Z           9208\n",
       "...                ...                   ...            ...\n",
       "6364845         186372  2010-10-23T04:17:47Z        5452252\n",
       "6435132         195001  2010-10-23T04:37:34Z        5977757\n",
       "6439961         196005  2010-10-23T04:38:02Z        5577722\n",
       "6422034         192853  2010-10-23T05:18:36Z         349086\n",
       "6440430         196061  2010-10-23T05:22:06Z         155746\n",
       "\n",
       "[6442892 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_timestamp = checkin_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Group by user_id and business_id and count check-ins\n",
    "checkin_df['checkin_count:float'] = checkin_df.groupby(['user_id:token', 'item_id:token'])['item_id:token'].transform('count')\n",
    "checkin_df = checkin_df.drop_duplicates(subset=[\"user_id:token\", \"item_id:token\"], keep=\"first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users, number of POIs 107092 1280969\n",
      "Sparsity: 0.9999709776240456\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of users, number of POIs\", len(checkin_df[\"user_id:token\"].unique()), len(checkin_df[\"item_id:token\"].unique())\n",
    ")\n",
    "print(\"Sparsity:\", 1 - len(checkin_df) / (len(checkin_df[\"user_id:token\"].unique()) * len(checkin_df[\"item_id:token\"].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    107092.000000\n",
       "mean         37.176764\n",
       "std          85.736437\n",
       "min           1.000000\n",
       "25%           5.000000\n",
       "50%          17.000000\n",
       "75%          38.000000\n",
       "max        2064.000000\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df[\"user_id:token\"].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.280969e+06\n",
       "mean     3.108064e+00\n",
       "std      1.069730e+01\n",
       "min      1.000000e+00\n",
       "25%      1.000000e+00\n",
       "50%      2.000000e+00\n",
       "75%      3.000000e+00\n",
       "max      2.631000e+03\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df[\"item_id:token\"].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df, min_reviews_user=15, min_reviews_business=10):\n",
    "    while True:\n",
    "\n",
    "        # Filter users with at least min_reviews reviews\n",
    "        user_counts = df['user_id:token'].value_counts()\n",
    "        user_mask = df['user_id:token'].map(user_counts) >= min_reviews_user\n",
    "        df_filtered = df.loc[user_mask]\n",
    "\n",
    "        # Filter businesses with at least min_reviews reviews\n",
    "        business_counts = df_filtered[\"item_id:token\"].value_counts()\n",
    "        business_mask = df_filtered['item_id:token'].map(business_counts) >= min_reviews_business\n",
    "        df_filtered = df_filtered.loc[business_mask]\n",
    "\n",
    "        \n",
    "\n",
    "        # If the size of the filtered DataFrame didn't change, break the loop\n",
    "        if df_filtered.shape[0] == df.shape[0]:\n",
    "            break\n",
    "\n",
    "        # Update the DataFrame for the next iteration\n",
    "        df = df_filtered\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_filtered = filter_df(checkin_df, 15, 20) # for gowalla i used business min 20 & user min 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df_filtered[\"user_id:token\"].value_counts().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10196"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df_filtered[\"user_id:token\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10196, 7781)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df_filtered[\"user_id:token\"].nunique(), checkin_df_filtered[\"item_id:token\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id:token</th>\n",
       "      <th>timestamp:float</th>\n",
       "      <th>item_id:token</th>\n",
       "      <th>checkin_count:float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33619</th>\n",
       "      <td>138</td>\n",
       "      <td>2009-02-04T05:17:38Z</td>\n",
       "      <td>9056</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33618</th>\n",
       "      <td>138</td>\n",
       "      <td>2009-02-17T19:28:30Z</td>\n",
       "      <td>8957</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33617</th>\n",
       "      <td>138</td>\n",
       "      <td>2009-02-17T19:31:59Z</td>\n",
       "      <td>8956</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33616</th>\n",
       "      <td>138</td>\n",
       "      <td>2009-02-18T18:50:55Z</td>\n",
       "      <td>9208</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33614</th>\n",
       "      <td>138</td>\n",
       "      <td>2009-02-23T18:45:30Z</td>\n",
       "      <td>8988</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3994831</th>\n",
       "      <td>70375</td>\n",
       "      <td>2010-10-22T20:58:17Z</td>\n",
       "      <td>10059</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3917551</th>\n",
       "      <td>66012</td>\n",
       "      <td>2010-10-22T21:58:05Z</td>\n",
       "      <td>26650</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051972</th>\n",
       "      <td>154875</td>\n",
       "      <td>2010-10-22T23:06:50Z</td>\n",
       "      <td>24736</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4284756</th>\n",
       "      <td>89231</td>\n",
       "      <td>2010-10-23T01:01:04Z</td>\n",
       "      <td>10745</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6329006</th>\n",
       "      <td>180989</td>\n",
       "      <td>2010-10-23T01:18:24Z</td>\n",
       "      <td>10745</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>391687 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id:token       timestamp:float  item_id:token  \\\n",
       "33619              138  2009-02-04T05:17:38Z           9056   \n",
       "33618              138  2009-02-17T19:28:30Z           8957   \n",
       "33617              138  2009-02-17T19:31:59Z           8956   \n",
       "33616              138  2009-02-18T18:50:55Z           9208   \n",
       "33614              138  2009-02-23T18:45:30Z           8988   \n",
       "...                ...                   ...            ...   \n",
       "3994831          70375  2010-10-22T20:58:17Z          10059   \n",
       "3917551          66012  2010-10-22T21:58:05Z          26650   \n",
       "6051972         154875  2010-10-22T23:06:50Z          24736   \n",
       "4284756          89231  2010-10-23T01:01:04Z          10745   \n",
       "6329006         180989  2010-10-23T01:18:24Z          10745   \n",
       "\n",
       "         checkin_count:float  \n",
       "33619                      5  \n",
       "33618                      5  \n",
       "33617                      5  \n",
       "33616                      4  \n",
       "33614                      4  \n",
       "...                      ...  \n",
       "3994831                    1  \n",
       "3917551                    1  \n",
       "6051972                    1  \n",
       "4284756                    1  \n",
       "6329006                    1  \n",
       "\n",
       "[391687 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = checkin_df_filtered['item_id:token'].value_counts().reset_index()\n",
    "value_counts.columns = ['item_id:token', 'count']\n",
    "\n",
    "max_count = value_counts['count'].max()\n",
    "value_counts['business_popularity:float'] = value_counts['count'] / max_count\n",
    " \n",
    "checkin_df_filtered = checkin_df_filtered.merge(value_counts[['item_id:token', 'business_popularity:float']], on = \"item_id:token\", how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_popularity_sample_calculator(checkin_df_filtered, poi_df, user_df, sep_num, checkin_df_timestamp):\n",
    "    # Calculate average popularity per user\n",
    "    average_popularity_per_user = checkin_df_filtered.groupby('user_id:token')['business_popularity:float'].mean().reset_index() # try out median of item popularities in user profile instead of mean \n",
    "    average_popularity_per_user.columns = ['user_id:token', 'average_popularity']\n",
    "\n",
    "    average_popularity_per_user = average_popularity_per_user.sort_values(by=\"average_popularity\", ascending=False)\n",
    "\n",
    "    # Get top users\n",
    "    high_pop_user_df_sample = average_popularity_per_user.head(sep_num)\n",
    "    \n",
    "    # Get the users around the median\n",
    "    median_index = len(average_popularity_per_user) // 2\n",
    "    start_med_index = max(median_index -int (sep_num*1.5), 0)\n",
    "    end_med_index = min(median_index + int(sep_num*1.5), len(average_popularity_per_user))\n",
    "    med_pop_user_df_sample = average_popularity_per_user.iloc[start_med_index:end_med_index]\n",
    "    \n",
    "    # Get the lowest users\n",
    "    low_pop_user_df_sample = average_popularity_per_user.tail(sep_num)\n",
    "\n",
    "    unique_users = list(set(high_pop_user_df_sample[\"user_id:token\"].tolist() + med_pop_user_df_sample[\"user_id:token\"].tolist() + low_pop_user_df_sample[\"user_id:token\"].tolist()))\n",
    "\n",
    "    checkin_df_sample = checkin_df_filtered[checkin_df_filtered[\"user_id:token\"].isin(unique_users)]\n",
    "    checkin_df_sample = checkin_df_sample[checkin_df_sample[\"user_id:token\"].isin(unique_users)]\n",
    "\n",
    "    user_df_sample = user_df[user_df[\"user_id:token\"].isin(unique_users)]\n",
    "    poi_df_sample = poi_df[poi_df[\"item_id:token\"].isin(checkin_df_sample[\"item_id:token\"])]\n",
    "\n",
    "    checkin_df_sample = checkin_df_sample[checkin_df_sample[\"item_id:token\"].isin(poi_df_sample[\"item_id:token\"])]\n",
    "\n",
    "    checkin_df_timestamp = checkin_df_timestamp[checkin_df_timestamp[\"user_id:token\"].isin(unique_users)]\n",
    "    checkin_df_timestamp = checkin_df_timestamp[checkin_df_timestamp[\"item_id:token\"].isin(poi_df_sample[\"item_id:token\"])]\n",
    "\n",
    "    \n",
    "    return checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample, checkin_df_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sep_num = 1500//5\n",
    "\n",
    "sep_num*1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "if checkin_df_filtered[\"user_id:token\"].nunique() > 1500:\n",
    "    sep_num = 1500 // 5\n",
    "else:\n",
    "    sep_num = checkin_df_filtered[\"user_id:token\"].nunique() // 5\n",
    "\n",
    "print(sep_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample, checkin_df_timestamp = user_popularity_sample_calculator(checkin_df_filtered, poi_df, user_df, sep_num, checkin_df_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_factorizer(checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample, checkin_df_timestamp):\n",
    "    \"\"\"Overwriting the actual ID with a factorized ID so that we can use the same ID both in RecBole and CAPRI\"\"\"\n",
    "    checkin_df_sample['user_id:token'], user_id_map = pd.factorize(checkin_df_sample['user_id:token'])\n",
    "    checkin_df_sample['item_id:token'], business_id_map = pd.factorize(checkin_df_sample['item_id:token'])\n",
    "\n",
    "    # Create mapping dictionaries\n",
    "    user_id_mapping = {original: i for i, original in enumerate(user_id_map)}\n",
    "    business_id_mapping = {original: j for j, original in enumerate(business_id_map)}\n",
    "\n",
    "    high_pop_user_df_sample['user_id:token'] = high_pop_user_df_sample['user_id:token'].map(user_id_mapping)\n",
    "    med_pop_user_df_sample['user_id:token'] = med_pop_user_df_sample['user_id:token'].map(user_id_mapping)\n",
    "    low_pop_user_df_sample['user_id:token'] = low_pop_user_df_sample['user_id:token'].map(user_id_mapping)\n",
    "\n",
    "    checkin_df_timestamp[\"user_id:token\"] = checkin_df_timestamp[\"user_id:token\"].map(user_id_mapping)\n",
    "    checkin_df_timestamp[\"item_id:token\"] = checkin_df_timestamp[\"item_id:token\"].map(business_id_mapping)\n",
    "\n",
    "    user_df_sample['user_id:token'] = user_df_sample['user_id:token'].map(user_id_mapping)\n",
    "    poi_df_sample['item_id:token'] = poi_df_sample['item_id:token'].map(business_id_mapping)\n",
    "\n",
    "\n",
    "    return checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample, checkin_df_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9q/p0b6nwnd5071040x3849mdvc0000gn/T/ipykernel_67662/4057195747.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  user_df_sample['user_id:token'] = user_df_sample['user_id:token'].map(user_id_mapping)\n",
      "/var/folders/9q/p0b6nwnd5071040x3849mdvc0000gn/T/ipykernel_67662/4057195747.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  poi_df_sample['item_id:token'] = poi_df_sample['item_id:token'].map(business_id_mapping)\n"
     ]
    }
   ],
   "source": [
    "checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample, checkin_df_timestamp = id_factorizer(checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample, checkin_df_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df_sample[\"user_id:token\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_id_token_adder(df, column_name_list = [\"user_id:token\", \"item_id:token\"]):\n",
    "    \"\"\" Recbole needs a token (string) instead of a number for the user and item ID\"\"\"\n",
    "    for column_name in column_name_list:\n",
    "        try:\n",
    "            df[column_name] = df[column_name].astype(int)\n",
    "            df[column_name] = df[column_name].astype(str) + \"_x\"\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9q/p0b6nwnd5071040x3849mdvc0000gn/T/ipykernel_67662/1064257110.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].astype(int)\n",
      "/var/folders/9q/p0b6nwnd5071040x3849mdvc0000gn/T/ipykernel_67662/1064257110.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].astype(str) + \"_x\"\n",
      "/var/folders/9q/p0b6nwnd5071040x3849mdvc0000gn/T/ipykernel_67662/1064257110.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].astype(int)\n",
      "/var/folders/9q/p0b6nwnd5071040x3849mdvc0000gn/T/ipykernel_67662/1064257110.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].astype(str) + \"_x\"\n"
     ]
    }
   ],
   "source": [
    "checkin_df_sample = user_id_token_adder(checkin_df_sample)\n",
    "high_pop_user_df_sample = user_id_token_adder(high_pop_user_df_sample)\n",
    "med_pop_user_df_sample = user_id_token_adder(med_pop_user_df_sample)\n",
    "low_pop_user_df_sample = user_id_token_adder(low_pop_user_df_sample)\n",
    "user_df_sample = user_id_token_adder(user_df_sample)\n",
    "poi_df_sample = user_id_token_adder(poi_df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a json with the user id's of the respective popularity groups\n",
    "user_id_popularity = {}\n",
    "user_id_popularity[\"high\"] = high_pop_user_df_sample[\"user_id:token\"].tolist()\n",
    "user_id_popularity[\"medium\"] = med_pop_user_df_sample[\"user_id:token\"].tolist()\n",
    "user_id_popularity[\"low\"] = low_pop_user_df_sample[\"user_id:token\"].tolist()\n",
    "json.dump(user_id_popularity, open(f\"{DATASET_DIR}/{dataset}_user_id_popularity.json\", \"w\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_saver_recbole(df, framework, suffix):\n",
    "    \n",
    "    if not os.path.exists(DATASET_DIR + \"processed_data_\" + framework):\n",
    "        os.makedirs(DATASET_DIR + \"processed_data_\" + framework)\n",
    "\n",
    "    df.to_csv(f\"{DATASET_DIR}processed_data_{framework}/{dataset}_sample.{suffix}\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_sample['review_id:token'] = range(1, len(checkin_df_sample) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_sample = convert_to_unix_timestamp(checkin_df_sample, \"timestamp:float\")\n",
    "checkin_df_timestamp = convert_to_unix_timestamp(checkin_df_timestamp, \"timestamp:float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_sample.sort_values(by=\"checkin_count:float\", ascending=False)\n",
    "# very important: keeping the duplicate check-ins for the context aware recommendation to have the timestamps saved\n",
    "\n",
    "\n",
    "# very important: dropping duplicate check-ins \n",
    "checkin_df_sample = checkin_df_sample.drop_duplicates(subset=[\"user_id:token\", \"item_id:token\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df_sample = user_df_sample[[\"user_id:token\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would be the correct splits if we let recbole do the splitting\n",
    "# data_saver_recbole(checkin_df_sample, \"recbole\", \"inter\")\n",
    "# data_saver_recbole(user_df_sample, \"recbole\", \"user\")\n",
    "# data_saver_recbole(poi_df_sample, \"recbole\", \"item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_timestamp = checkin_df_timestamp[[\"user_id:token\", \"item_id:token\", \"timestamp:float\"]] # FINAL\n",
    "checkins_capri_train_test_tune = checkin_df_sample[[\"user_id:token\", \"item_id:token\", \"timestamp:float\", \"checkin_count:float\"]]\n",
    "try:\n",
    "    poi_df_sample_capri = poi_df_sample[[\"item_id:token\", \"lat:float\", \"lon:float\"]] # FINAL\n",
    "except KeyError: # in the snowcard data the coordinates are not given\n",
    "    poi_df_sample_capri = poi_df_sample[[\"item_id:token\"]]\n",
    "datasize_capri = pd.DataFrame(data={\"num_users\" : [len(checkins_capri_train_test_tune[\"user_id:token\"].unique())], \"num_items\" : [len(checkins_capri_train_test_tune[\"item_id:token\"].unique())]}) # FINAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating train, test and val splits (user-based temporal split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set:\n",
      "   user_id:token item_id:token  checkin_count:float\n",
      "14           0_x           0_x                    5\n",
      "21           0_x           1_x                    2\n",
      "29           0_x           2_x                    1\n",
      "30           0_x           3_x                    2\n",
      "35           0_x           4_x                    5\n",
      "\n",
      "Validation Set:\n",
      "       user_id:token item_id:token  checkin_count:float\n",
      "110371           0_x        5032_x                    1\n",
      "112576           0_x        2458_x                    1\n",
      "113139           0_x        4928_x                    1\n",
      "113429           0_x        2141_x                    2\n",
      "113438           0_x        5371_x                    1\n",
      "\n",
      "Test Set:\n",
      "       user_id:token item_id:token  checkin_count:float\n",
      "170218           0_x        5889_x                    1\n",
      "173696           0_x        3932_x                    1\n",
      "192072           0_x        6085_x                    8\n",
      "195127           0_x        6122_x                    1\n",
      "212712           0_x        6589_x                    5\n"
     ]
    }
   ],
   "source": [
    "# splitting the data into train, test, and tune\n",
    "checkins_capri_train_test_tune = checkins_capri_train_test_tune.sort_values(by=[\"user_id:token\", \"timestamp:float\"])\n",
    "checkins_capri_train_test_tune = checkins_capri_train_test_tune[[\"user_id:token\", \"item_id:token\", \"checkin_count:float\"]]\n",
    "\n",
    "# Split the data\n",
    "train_list = []\n",
    "val_list = []\n",
    "test_list = []\n",
    "\n",
    "for user, group in checkins_capri_train_test_tune.groupby('user_id:token'):\n",
    "    n = len(group)\n",
    "    train_end = int(n * 0.65)\n",
    "    val_end = int(n * 0.80)\n",
    "    \n",
    "    train_list.append(group.iloc[:train_end])\n",
    "    val_list.append(group.iloc[train_end:val_end])\n",
    "    test_list.append(group.iloc[val_end:])\n",
    "\n",
    "# Combine lists into DataFrames\n",
    "train_df = pd.concat(train_list)\n",
    "val_df = pd.concat(val_list)\n",
    "test_df = pd.concat(test_list)\n",
    "\n",
    "\n",
    "\n",
    "# Check the splits\n",
    "\n",
    "# FINAL 6-8\n",
    "print(\"Train Set:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nValidation Set:\")\n",
    "print(val_df.head())\n",
    "print(\"\\nTest Set:\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasaver_capri(df, filename):\n",
    "    \n",
    "    if not os.path.exists(DATASET_DIR + \"processed_data_capri\"):\n",
    "        os.makedirs(DATASET_DIR + \"processed_data_capri\")\n",
    "    \n",
    "    df.to_csv(DATASET_DIR + \"processed_data_capri/\" + filename + \".txt\", sep='\\t', index=False, header=False)\n",
    "    print(\"Data saved as \" + filename + \".txt\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a category column\n",
    "if include_categories is True:\n",
    "    if dataset == \"yelp\":\n",
    "        # Split the 'category_name' column by commas\n",
    "        poi_df_sample['category_name_unstacked:token_seq'] = poi_df_sample['category_name:token_seq'].str.split(', ')\n",
    "\n",
    "        # Unstack the categories into multiple rows\n",
    "        category_df_sample = poi_df_sample.explode('category_name_unstacked:token_seq')\n",
    "        category_counts = category_df_sample[\"category_name_unstacked:token_seq\"].value_counts()\n",
    "        category_mask = category_df_sample[\"category_name_unstacked:token_seq\"].map(category_counts) >= 25\n",
    "        category_df_sample_filtered = category_df_sample.loc[category_mask]\n",
    "        category_df_sample_filtered[\"category_id:token\"], category_id = pd.factorize(category_df_sample_filtered[\"category_name_unstacked:token_seq\"])\n",
    "        category_df_sample_filtered.dropna(inplace=True)\n",
    "        datasize_capri = pd.DataFrame(data={\"num_users\" : [len(checkins_capri_train_test_tune[\"user_id:token\"].unique())], \"num_items\" : [len(checkins_capri_train_test_tune[\"item_id:token\"].unique())], \"num_categories\" : [len(category_df_sample_filtered[\"category_id:token\"].unique())]}) # FINAL\n",
    "        datasaver_capri(category_df_sample_filtered, \"poiCategories\")\n",
    "\n",
    "\n",
    "    elif dataset == \"foursquarenyc\" or dataset == \"foursquaretky\":\n",
    "        poi_df_sample[\"category_id:token\"], category_id = pd.factorize(poi_df_sample[\"category_name:token_seq\"])\n",
    "        datasize_capri = pd.DataFrame(data={\"num_users\" : [len(checkins_capri_train_test_tune[\"user_id:token\"].unique())], \"num_items\" : [len(checkins_capri_train_test_tune[\"item_id:token\"].unique())], \"num_categories\" : [len(poi_df_sample[\"category_id:token\"].unique())]})\n",
    "        poi_df_categories = poi_df_sample[[\"item_id:token\", \"category_id:token\"]]\n",
    "        datasaver_capri(poi_df_categories, \"poiCategories\")\n",
    "\n",
    "    elif dataset == \"snowcard\":\n",
    "        datasize_capri = pd.DataFrame(data={\"num_users\" : [len(checkins_capri_train_test_tune[\"user_id:token\"].unique())], \"num_items\" : [len(checkins_capri_train_test_tune[\"item_id:token\"].unique())], \"num_categories\" : [len(poi_df_sample[\"category_id:token\"].unique())]})\n",
    "        poi_df_categories = poi_df_sample[[\"item_id:token\", \"category_id:token\"]]\n",
    "        datasaver_capri(poi_df_categories, \"poiCategories\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 1500, 1500)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"user_id:token\"].nunique(), val_df[\"user_id:token\"].nunique(), test_df[\"user_id:token\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the correct split since we perform the splitting ourselves\n",
    "data_saver_recbole(train_df, \"recbole\", \"train.inter\")\n",
    "data_saver_recbole(test_df, \"recbole\", \"test.inter\")\n",
    "data_saver_recbole(val_df, \"recbole\", \"valid.inter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECBOLE DEBIAS PREPROCESSING\n",
    "\n",
    "# ### Make an intervened sample for RecBole debias (not needed for current approach since RecBole Debias not used )\n",
    "# checkins_debias = checkins_capri_train_test_tune.copy()\n",
    "# intervened_set = checkins_debias.sample(frac=0.5, random_state=10002)\n",
    "# normal_set = checkins_debias.drop(intervened_set.index)\n",
    "\n",
    "# original_user_ids = set(checkins_debias[\"user_id:token\"].unique())\n",
    "# intervened_user_ids = set(intervened_set[\"user_id:token\"].unique())\n",
    "# normal_user_ids = set(normal_set[\"user_id:token\"].unique())\n",
    "\n",
    "# missing_in_intervened = original_user_ids - intervened_user_ids\n",
    "# missing_in_normal = original_user_ids - normal_user_ids\n",
    "\n",
    "# if not missing_in_intervened and not missing_in_normal:\n",
    "#     print(\"All user IDs are present in both subsets.\")\n",
    "# else:\n",
    "#     print(\"Missing user IDs in intervened set:\", missing_in_intervened)\n",
    "#     print(\"Missing user IDs in normal set:\", missing_in_normal)\n",
    "\n",
    "\n",
    "# # SOURCE: https://github.com/DavyMorgan/dps/blob/19a3e5fb2cb6932c3f093ed443760ecd3d95bfdb/data_process_service/splitter.py\n",
    "\n",
    "# popularity = (\n",
    "#             intervened_set[[\"item_id:token\", \"user_id:token\"]]\n",
    "#             .groupby(\"item_id:token\")\n",
    "#             .count()\n",
    "#             .reset_index()\n",
    "#             .rename(columns={\"user_id:token\": \"pop\"})\n",
    "#         )\n",
    "# intervened_set = intervened_set.merge(popularity, on=\"item_id:token\", how=\"left\")\n",
    "# intervened_set[\"pop\"] = intervened_set[\"pop\"].apply(lambda x: 1 / x)\n",
    "\n",
    "# intervened_set.sort_values(by=[\"pop\", \"item_id:token\"], ascending=False, inplace=True)\n",
    "# intervened_set.drop(columns=[\"pop\"], inplace=True)\n",
    "\n",
    "# # Lists to collect data for each split\n",
    "# train_list_intervened, val_list_intervened, test_list_intervened = [], [], []\n",
    "\n",
    "# # Loop through each user's data in the sorted intervened set\n",
    "# for user, group in intervened_set.groupby('user_id:token'):\n",
    "#     n = len(group)\n",
    "    \n",
    "#     # Calculate end indices based on the desired split ratios\n",
    "#     test_end = int(n * 0.5)  # Top 50% for test\n",
    "#     train_end = test_end + int(n * 0.25)  # Next 25% for train\n",
    "    \n",
    "#     # Add to respective lists\n",
    "#     test_list_intervened.append(group.iloc[:test_end])  # First 50% goes to test\n",
    "#     train_list_intervened.append(group.iloc[test_end:train_end])  # Next 25% goes to train\n",
    "#     val_list_intervened.append(group.iloc[train_end:])  # Remaining 25% goes to validation\n",
    "\n",
    "# # Concatenate lists into DataFrames\n",
    "# train_df_intervened = pd.concat(train_list_intervened, ignore_index=True)\n",
    "# train_df_intervened = pd.concat([train_df_intervened, normal_set], ignore_index=True).sort_values(by=[\"user_id:token\"])\n",
    "# val_df_intervened = pd.concat(val_list_intervened, ignore_index=True)\n",
    "# test_df_intervened = pd.concat(test_list_intervened, ignore_index=True)\n",
    "\n",
    "# # Output the sizes of each set\n",
    "# print(\"Intervened Training Set Size:\", len(train_df_intervened))\n",
    "# print(\"Intervened Validation Set Size:\", len(val_df_intervened))\n",
    "# print(\"Intervened Test Set Size:\", len(test_df_intervened))\n",
    "\n",
    "# data_saver_recbole(train_df_intervened, \"recbole_debias\", \"train.inter\")\n",
    "# data_saver_recbole(test_df_intervened, \"recbole_debias\", \"test.inter\")\n",
    "# data_saver_recbole(val_df_intervened, \"recbole_debias\", \"valid.inter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAPRI (Context-Aware POI Recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_id_cleaner(df, column_name_list = [\"user_id:token\", \"item_id:token\"]):\n",
    "    \"\"\"Save for CAPRI without _x since they require integers as IDs\"\"\"\n",
    "    for column_name in column_name_list:\n",
    "        df[column_name] = df[column_name].str.split(\"_\")\n",
    "        df[column_name] = df[column_name].apply(lambda x: x[0])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_df_sample_capri = user_id_cleaner(poi_df_sample_capri, [\"item_id:token\"])\n",
    "train_df = user_id_cleaner(train_df)\n",
    "val_df = user_id_cleaner(val_df)\n",
    "test_df = user_id_cleaner(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id:token</th>\n",
       "      <th>item_id:token</th>\n",
       "      <th>checkin_count:float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>269964</th>\n",
       "      <td>1278</td>\n",
       "      <td>999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9636</th>\n",
       "      <td>127</td>\n",
       "      <td>999</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22914</th>\n",
       "      <td>215</td>\n",
       "      <td>999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57720</th>\n",
       "      <td>425</td>\n",
       "      <td>999</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59055</th>\n",
       "      <td>170</td>\n",
       "      <td>998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93951</th>\n",
       "      <td>186</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89373</th>\n",
       "      <td>681</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93824</th>\n",
       "      <td>491</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101660</th>\n",
       "      <td>552</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34177 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id:token item_id:token  checkin_count:float\n",
       "269964           1278           999                    9\n",
       "9636              127           999                   58\n",
       "22914             215           999                    1\n",
       "57720             425           999                    3\n",
       "59055             170           998                    1\n",
       "...               ...           ...                  ...\n",
       "93951             186             0                    1\n",
       "89373             681             0                    6\n",
       "93824             491             0                    1\n",
       "101660            552             0                    1\n",
       "14                  0             0                    5\n",
       "\n",
       "[34177 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"user_id:token\"] = train_df[\"user_id:token\"].astype(int)\n",
    "train_df.sort_values(by=\"item_id:token\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"checkin_count:float\"] = train_df[\"checkin_count:float\"].astype(int)\n",
    "test_df[\"checkin_count:float\"] = test_df[\"checkin_count:float\"].astype(int)\n",
    "val_df[\"checkin_count:float\"] = val_df[\"checkin_count:float\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id:token</th>\n",
       "      <th>item_id:token</th>\n",
       "      <th>checkin_count:float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175985</th>\n",
       "      <td>9</td>\n",
       "      <td>2199</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180628</th>\n",
       "      <td>9</td>\n",
       "      <td>935</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180707</th>\n",
       "      <td>9</td>\n",
       "      <td>315</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180991</th>\n",
       "      <td>9</td>\n",
       "      <td>938</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181906</th>\n",
       "      <td>9</td>\n",
       "      <td>2481</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34177 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id:token item_id:token  checkin_count:float\n",
       "14                  0             0                    5\n",
       "21                  0             1                    2\n",
       "29                  0             2                    1\n",
       "30                  0             3                    2\n",
       "35                  0             4                    5\n",
       "...               ...           ...                  ...\n",
       "175985              9          2199                    1\n",
       "180628              9           935                    1\n",
       "180707              9           315                    1\n",
       "180991              9           938                    1\n",
       "181906              9          2481                    1\n",
       "\n",
       "[34177 rows x 3 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_users</th>\n",
       "      <th>num_items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1500</td>\n",
       "      <td>7579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_users  num_items\n",
       "0       1500       7579"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasize_capri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved as checkins.txt\n",
      "Data saved as dataSize.txt\n",
      "Data saved as poiCoos.txt\n",
      "Data saved as train.txt\n",
      "Data saved as tune.txt\n",
      "Data saved as test.txt\n"
     ]
    }
   ],
   "source": [
    "datasaver_capri(checkin_df_timestamp, \"checkins\")\n",
    "datasaver_capri(datasize_capri, \"dataSize\")\n",
    "datasaver_capri(poi_df_sample_capri, \"poiCoos\")\n",
    "datasaver_capri(train_df, \"train\")\n",
    "datasaver_capri(val_df, \"tune\")\n",
    "datasaver_capri(test_df, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
