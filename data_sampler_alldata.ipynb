{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "dataset = \"foursquaretky\"\n",
    "\n",
    "DATASET_DIR = f\"/Volumes/Forster Neu/Masterarbeit Data/{dataset}_dataset/\"\n",
    "\n",
    "\n",
    "available_datasets = [\"foursquarenyc\", \"foursquaretky\", \"yelp\", \"gowalla\", \"brightkite\", \"snowcard\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_big_json(file_path):\n",
    "    data = []\n",
    "\n",
    "    # Open the file and read it line by line\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Parse the JSON data and append to the list\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    # Create a DataFrame from the list of records\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Print the first few rows of the DataFram\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_unix_timestamp(df, column_name):\n",
    "    \"\"\"\n",
    "    Convert a column of timestamps in a DataFrame to Unix timestamps.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the timestamp column.\n",
    "        column_name (str): The name of the column with timestamps in \"%Y-%m-%d %H:%M:%S\" format.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with an additional column for Unix timestamps.\n",
    "    \"\"\"\n",
    "    # Convert the column to datetime objects\n",
    "    df[column_name] = pd.to_datetime(df[column_name], format=\"mixed\")\n",
    "    \n",
    "    # Convert datetime objects to Unix timestamps\n",
    "    df[f'{column_name}'] = df[column_name].apply(lambda x: x.timestamp())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"snowcard\":\n",
    "    checkin_df = pd.read_csv(DATASET_DIR+\"TSC_EEL_EXPORT.csv\", encoding=\"latin1\", sep=\";\", header=None, names=[\"timestamp:float\", \"user_id:token\", \"category_id:token\", \"category_name:token_seq\", \"name:token_seq\", \"user_type:token_seq\"])\n",
    "    checkin_df[\"item_id:token\"], item_id = pd.factorize(checkin_df[\"name:token_seq\"])\n",
    "    user_df = checkin_df[[\"user_id:token\", \"user_type:token_seq\"]].drop_duplicates(subset=[\"user_id:token\"])\n",
    "    poi_df = checkin_df[[\"item_id:token\", \"name:token_seq\", \"category_id:token\", \"category_name:token_seq\"]].drop_duplicates(subset=[\"item_id:token\"])\n",
    "    checkin_df = checkin_df[[\"user_id:token\", \"item_id:token\", \"timestamp:float\"]]\n",
    "\n",
    "elif dataset == \"foursquarenyc\" or dataset == \"foursquaretky\":\n",
    "    checkin_df = pd.read_csv(DATASET_DIR + \"foursquare_data.csv\", sep=\",\")\n",
    "    checkin_df = checkin_df.drop(columns=[\"timezoneOffset\"])\n",
    "    checkin_df = checkin_df.rename(columns={\"venueId\": \"item_id:token\", \"venueCategoryId\": \"category_id:token\", \"venueCategory\": \"category_name:token_seq\", \"userId\": \"user_id:token\", \"utcTimestamp\": \"timestamp:float\", \"latitude\": \"lat:float\", \"longitude\": \"lon:float\"})\n",
    "    user_df = checkin_df[[\"user_id:token\"]].drop_duplicates()\n",
    "\n",
    "    poi_df = checkin_df[[\"item_id:token\", \"category_id:token\", \"category_name:token_seq\", \"lat:float\", \"lon:float\"]].drop_duplicates(subset=[\"item_id:token\"])\n",
    "    checkin_df = checkin_df[[\"user_id:token\", \"item_id:token\", \"timestamp:float\"]]\n",
    "\n",
    "elif dataset == \"gowalla\" or dataset == \"brightkite\":\n",
    "    checkin_df = pd.read_csv(DATASET_DIR + f\"loc-{dataset}_totalCheckins.txt\", sep=\"\\t\", header=None, names=['user_id:token', 'timestamp:float', 'lat:float', 'lon:float', 'item_id:token'])\n",
    "    checkin_df = checkin_df[~checkin_df['item_id:token'].isin([\"00000000000000000000000000000000\", \"ede07eeea22411dda0ef53e233ec57ca\"])]\n",
    "    user_df = pd.read_csv(DATASET_DIR + f\"loc-{dataset}_edges.txt\", sep=\"\\t\", header=None, names=['user_id:token', 'friends:token_seq'])\n",
    "    user_df = user_df.groupby('user_id:token')['friends:token_seq'].apply(lambda x: ','.join(map(str, x))).reset_index()\n",
    "    user_df.columns = ['user_id:token', 'friends:token_seq']\n",
    "    poi_df = checkin_df[['item_id:token', \"lat:float\", \"lon:float\"]].drop_duplicates(subset=\"item_id:token\")\n",
    "    checkin_df = checkin_df.drop(columns=[\"lat:float\", \"lon:float\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "elif dataset == \"yelp\":\n",
    "    poi_df = pd.read_json(DATASET_DIR + \"yelp_academic_dataset_business.json\", lines=True)\n",
    "    poi_df = poi_df.loc[poi_df['is_open'] == 1]\n",
    "    poi_df = poi_df.drop(columns=[\"review_count\", \"stars\", \"hours\", \"is_open\", \"city\", \"state\", \"postal_code\", \"attributes\", \"address\"])\n",
    "    poi_df = poi_df.rename(columns={\"latitude\": \"lat:float\", \"longitude\": \"lon:float\", \"business_id\": \"item_id:token\", \"name\":\"name:token_seq\", \"categories\":\"category_name:token_seq\"})\n",
    "    user_df = open_big_json(DATASET_DIR + \"yelp_academic_dataset_user.json\")\n",
    "    user_df = user_df.drop(columns=[\"review_count\", \"name\", \"yelping_since\", \"useful\", \"funny\", \"cool\", \"elite\", \"fans\", \"compliment_hot\", \"average_stars\", \"compliment_more\", \"compliment_profile\", \"compliment_cute\", \"compliment_list\", \"compliment_note\", \"compliment_plain\", \"compliment_cool\", \"compliment_funny\", \"compliment_writer\", \"compliment_photos\"])\n",
    "    user_df = user_df.rename(columns={\"user_id\": \"user_id:token\", \"friends\": \"friends:token_seq\"})\n",
    "    checkin_df = open_big_json(DATASET_DIR + \"yelp_academic_dataset_review.json\")\n",
    "    checkin_df = checkin_df.drop(columns=[\"text\", \"cool\", \"stars\", \"useful\", \"funny\", \"review_id\"])\n",
    "    checkin_df = checkin_df.rename(columns={\"user_id\": \"user_id:token\", \"business_id\": \"item_id:token\", \"date\": \"timestamp:float\"})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id:token</th>\n",
       "      <th>category_id:token</th>\n",
       "      <th>category_name:token_seq</th>\n",
       "      <th>lat:float</th>\n",
       "      <th>lon:float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4f0fd5a8e4b03856eeb6c8cb</td>\n",
       "      <td>4bf58dd8d48988d10c951735</td>\n",
       "      <td>Cosmetics Shop</td>\n",
       "      <td>35.705101</td>\n",
       "      <td>139.619590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4b7b884ff964a5207d662fe3</td>\n",
       "      <td>4bf58dd8d48988d1d1941735</td>\n",
       "      <td>Ramen /  Noodle House</td>\n",
       "      <td>35.715581</td>\n",
       "      <td>139.800317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4c16fdda96040f477cc473a5</td>\n",
       "      <td>4d954b0ea243a5684a65b473</td>\n",
       "      <td>Convenience Store</td>\n",
       "      <td>35.714542</td>\n",
       "      <td>139.480065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4c178638c2dfc928651ea869</td>\n",
       "      <td>4bf58dd8d48988d118951735</td>\n",
       "      <td>Food &amp; Drink Shop</td>\n",
       "      <td>35.725592</td>\n",
       "      <td>139.776633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4f568309e4b071452e447afe</td>\n",
       "      <td>4f2a210c4b9023bd5841ed28</td>\n",
       "      <td>Housing Development</td>\n",
       "      <td>35.656083</td>\n",
       "      <td>139.734046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573640</th>\n",
       "      <td>4bb55c4e2f70c9b66f2b8430</td>\n",
       "      <td>4bf58dd8d48988d124941735</td>\n",
       "      <td>Office</td>\n",
       "      <td>35.665447</td>\n",
       "      <td>139.836892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573663</th>\n",
       "      <td>4b91d257f964a520a8d933e3</td>\n",
       "      <td>4bf58dd8d48988d1e0931735</td>\n",
       "      <td>Coffee Shop</td>\n",
       "      <td>35.750061</td>\n",
       "      <td>139.587119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573668</th>\n",
       "      <td>50fcc9a3e4b07380b3aae03e</td>\n",
       "      <td>4bf58dd8d48988d1cc941735</td>\n",
       "      <td>Steakhouse</td>\n",
       "      <td>35.671634</td>\n",
       "      <td>139.857184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573675</th>\n",
       "      <td>4bda94b8c79cc928e5327fe9</td>\n",
       "      <td>4bf58dd8d48988d1f9941735</td>\n",
       "      <td>Food &amp; Drink Shop</td>\n",
       "      <td>35.703205</td>\n",
       "      <td>139.579236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573691</th>\n",
       "      <td>50ada82ce4b0d4508a244756</td>\n",
       "      <td>4bf58dd8d48988d120941735</td>\n",
       "      <td>Bar</td>\n",
       "      <td>35.730150</td>\n",
       "      <td>139.708761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61858 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   item_id:token         category_id:token  \\\n",
       "0       4f0fd5a8e4b03856eeb6c8cb  4bf58dd8d48988d10c951735   \n",
       "1       4b7b884ff964a5207d662fe3  4bf58dd8d48988d1d1941735   \n",
       "2       4c16fdda96040f477cc473a5  4d954b0ea243a5684a65b473   \n",
       "3       4c178638c2dfc928651ea869  4bf58dd8d48988d118951735   \n",
       "4       4f568309e4b071452e447afe  4f2a210c4b9023bd5841ed28   \n",
       "...                          ...                       ...   \n",
       "573640  4bb55c4e2f70c9b66f2b8430  4bf58dd8d48988d124941735   \n",
       "573663  4b91d257f964a520a8d933e3  4bf58dd8d48988d1e0931735   \n",
       "573668  50fcc9a3e4b07380b3aae03e  4bf58dd8d48988d1cc941735   \n",
       "573675  4bda94b8c79cc928e5327fe9  4bf58dd8d48988d1f9941735   \n",
       "573691  50ada82ce4b0d4508a244756  4bf58dd8d48988d120941735   \n",
       "\n",
       "       category_name:token_seq  lat:float   lon:float  \n",
       "0               Cosmetics Shop  35.705101  139.619590  \n",
       "1        Ramen /  Noodle House  35.715581  139.800317  \n",
       "2            Convenience Store  35.714542  139.480065  \n",
       "3            Food & Drink Shop  35.725592  139.776633  \n",
       "4          Housing Development  35.656083  139.734046  \n",
       "...                        ...        ...         ...  \n",
       "573640                  Office  35.665447  139.836892  \n",
       "573663             Coffee Shop  35.750061  139.587119  \n",
       "573668              Steakhouse  35.671634  139.857184  \n",
       "573675       Food & Drink Shop  35.703205  139.579236  \n",
       "573691                     Bar  35.730150  139.708761  \n",
       "\n",
       "[61858 rows x 5 columns]"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users, number of POIs 2293 61858\n",
      "Sparsity: 0.9959552918331572\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of users, number of POIs\", len(checkin_df[\"user_id:token\"].unique()), len(checkin_df[\"item_id:token\"].unique())\n",
    ")\n",
    "print(\"Sparsity:\", 1 - len(checkin_df) / (len(checkin_df[\"user_id:token\"].unique()) * len(checkin_df[\"item_id:token\"].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df, min_reviews_user=15, min_reviews_business=10):\n",
    "    while True:\n",
    "        # Filter businesses with at least min_reviews reviews\n",
    "        business_counts = df[\"item_id:token\"].value_counts()\n",
    "        business_mask = df['item_id:token'].map(business_counts) >= min_reviews_business\n",
    "        df_filtered = df.loc[business_mask]\n",
    "\n",
    "        # Filter users with at least min_reviews reviews\n",
    "        user_counts = df_filtered['user_id:token'].value_counts()\n",
    "        user_mask = df_filtered['user_id:token'].map(user_counts) >= min_reviews_user\n",
    "        df_filtered = df_filtered.loc[user_mask]\n",
    "\n",
    "        # If the size of the filtered DataFrame didn't change, break the loop\n",
    "        if df_filtered.shape[0] == df.shape[0]:\n",
    "            break\n",
    "\n",
    "        # Update the DataFrame for the next iteration\n",
    "        df = df_filtered\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_filtered = filter_df(checkin_df, min_reviews_business=10, min_reviews_user=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id:token</th>\n",
       "      <th>item_id:token</th>\n",
       "      <th>timestamp:float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114</td>\n",
       "      <td>4c16fdda96040f477cc473a5</td>\n",
       "      <td>Tue Apr 03 19:12:07 +0000 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>114</td>\n",
       "      <td>4b3eae5cf964a520b4a025e3</td>\n",
       "      <td>Tue Apr 03 19:35:36 +0000 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1635</td>\n",
       "      <td>4cca7bd67965b60c80f0858a</td>\n",
       "      <td>Tue Apr 03 19:51:50 +0000 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2033</td>\n",
       "      <td>4b5c7671f964a520083129e3</td>\n",
       "      <td>Tue Apr 03 19:51:59 +0000 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>589</td>\n",
       "      <td>4b5ed39cf964a520079a29e3</td>\n",
       "      <td>Tue Apr 03 19:59:06 +0000 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573695</th>\n",
       "      <td>2277</td>\n",
       "      <td>4b82669cf964a5209ed130e3</td>\n",
       "      <td>Sat Feb 16 02:33:48 +0000 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573697</th>\n",
       "      <td>2277</td>\n",
       "      <td>4b56c4c5f964a520c41a28e3</td>\n",
       "      <td>Sat Feb 16 02:34:32 +0000 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573698</th>\n",
       "      <td>326</td>\n",
       "      <td>4bab3456f964a5204d993ae3</td>\n",
       "      <td>Sat Feb 16 02:34:35 +0000 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573699</th>\n",
       "      <td>853</td>\n",
       "      <td>4b559c09f964a520efe827e3</td>\n",
       "      <td>Sat Feb 16 02:34:53 +0000 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573702</th>\n",
       "      <td>1050</td>\n",
       "      <td>4b5a7486f964a52027c628e3</td>\n",
       "      <td>Sat Feb 16 02:35:29 +0000 2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>447570 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id:token             item_id:token  \\\n",
       "2                 114  4c16fdda96040f477cc473a5   \n",
       "7                 114  4b3eae5cf964a520b4a025e3   \n",
       "8                1635  4cca7bd67965b60c80f0858a   \n",
       "9                2033  4b5c7671f964a520083129e3   \n",
       "10                589  4b5ed39cf964a520079a29e3   \n",
       "...               ...                       ...   \n",
       "573695           2277  4b82669cf964a5209ed130e3   \n",
       "573697           2277  4b56c4c5f964a520c41a28e3   \n",
       "573698            326  4bab3456f964a5204d993ae3   \n",
       "573699            853  4b559c09f964a520efe827e3   \n",
       "573702           1050  4b5a7486f964a52027c628e3   \n",
       "\n",
       "                       timestamp:float  \n",
       "2       Tue Apr 03 19:12:07 +0000 2012  \n",
       "7       Tue Apr 03 19:35:36 +0000 2012  \n",
       "8       Tue Apr 03 19:51:50 +0000 2012  \n",
       "9       Tue Apr 03 19:51:59 +0000 2012  \n",
       "10      Tue Apr 03 19:59:06 +0000 2012  \n",
       "...                                ...  \n",
       "573695  Sat Feb 16 02:33:48 +0000 2013  \n",
       "573697  Sat Feb 16 02:34:32 +0000 2013  \n",
       "573698  Sat Feb 16 02:34:35 +0000 2013  \n",
       "573699  Sat Feb 16 02:34:53 +0000 2013  \n",
       "573702  Sat Feb 16 02:35:29 +0000 2013  \n",
       "\n",
       "[447570 rows x 3 columns]"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the value counts of `business_id`\n",
    "value_counts = checkin_df_filtered['item_id:token'].value_counts().reset_index()\n",
    "value_counts.columns = ['item_id:token', 'count']\n",
    "\n",
    "# Step 2: Normalize the counts y dividing by the maximum value count\n",
    "max_count = value_counts['count'].max()\n",
    "value_counts['business_popularity:float'] = value_counts['count'] / max_count\n",
    "\n",
    "# Step 3: Merge the normalized counts back into the original DataFrame\n",
    "checkin_df_filtered = checkin_df_filtered.merge(value_counts[['item_id:token', 'business_popularity:float']], on = \"item_id:token\", how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_popularity_sample_calculator(checkin_df_filtered, poi_df, user_df, sep_num):\n",
    "    # Calculate average popularity per user\n",
    "    average_popularity_per_user = checkin_df_filtered.groupby('user_id:token')['business_popularity:float'].mean().reset_index()\n",
    "    average_popularity_per_user.columns = ['user_id:token', 'average_popularity']\n",
    "\n",
    "    average_popularity_per_user = average_popularity_per_user.sort_values(by=\"average_popularity\", ascending=False)\n",
    "\n",
    "    \n",
    "    # Sort by average popularity\n",
    "    \n",
    "\n",
    "    # Get top 1000 users\n",
    "    high_pop_user_df_sample = average_popularity_per_user.head(sep_num)\n",
    "    \n",
    "    # Get the middle 1000 users around the median\n",
    "    median_index = len(average_popularity_per_user) // 2\n",
    "    start_med_index = max(median_index -int (sep_num/2), 0)\n",
    "    end_med_index = min(median_index + int(sep_num/2), len(average_popularity_per_user))\n",
    "    med_pop_user_df_sample = average_popularity_per_user.iloc[start_med_index:end_med_index]\n",
    "    \n",
    "    # Get the lowest 1000 users\n",
    "    low_pop_user_df_sample = average_popularity_per_user.tail(sep_num)\n",
    "\n",
    "    unique_users = list(set(high_pop_user_df_sample[\"user_id:token\"].tolist() + med_pop_user_df_sample[\"user_id:token\"].tolist() + low_pop_user_df_sample[\"user_id:token\"].tolist()))\n",
    "\n",
    "    checkin_df_sample = checkin_df_filtered[checkin_df_filtered[\"user_id:token\"].isin(unique_users)]\n",
    "\n",
    "    checkin_df_sample = checkin_df_sample[checkin_df_sample[\"user_id:token\"].isin(unique_users)]\n",
    "\n",
    "    unique_items = checkin_df_sample[\"item_id:token\"].unique()\n",
    "\n",
    "    user_df_sample = user_df[user_df[\"user_id:token\"].isin(unique_users)]\n",
    "    poi_df_sample = poi_df[poi_df[\"item_id:token\"].isin(unique_items)]\n",
    "\n",
    "\n",
    "    \n",
    "    return checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample = user_popularity_sample_calculator(checkin_df_filtered, poi_df, user_df, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id:token</th>\n",
       "      <th>average_popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>0.692056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.680855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>1039</td>\n",
       "      <td>0.612799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>838</td>\n",
       "      <td>0.610174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>1275</td>\n",
       "      <td>0.605600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>2130</td>\n",
       "      <td>0.150612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2214</th>\n",
       "      <td>2215</td>\n",
       "      <td>0.150303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>339</td>\n",
       "      <td>0.149993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>2084</td>\n",
       "      <td>0.149761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241</th>\n",
       "      <td>2242</td>\n",
       "      <td>0.149548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id:token  average_popularity\n",
       "46               47            0.692056\n",
       "22               23            0.680855\n",
       "1038           1039            0.612799\n",
       "837             838            0.610174\n",
       "1274           1275            0.605600\n",
       "...             ...                 ...\n",
       "2129           2130            0.150612\n",
       "2214           2215            0.150303\n",
       "338             339            0.149993\n",
       "2083           2084            0.149761\n",
       "2241           2242            0.149548\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 725,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_pop_user_df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_factorizer(checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample):\n",
    "    \"\"\"Overwriting the actual ID with a factorized ID so that we can use the same ID both in RecBole and CAPRI\"\"\"\n",
    "    checkin_df_sample['user_id:token'], user_id_map = pd.factorize(checkin_df_sample['user_id:token'])\n",
    "    checkin_df_sample['item_id:token'], business_id_map = pd.factorize(checkin_df_sample['item_id:token'])\n",
    "\n",
    "    # Create mapping dictionaries\n",
    "    user_id_mapping = {original: i for i, original in enumerate(user_id_map)}\n",
    "    business_id_mapping = {original: j for j, original in enumerate(business_id_map)}\n",
    "\n",
    "    high_pop_user_df_sample['user_id:token'] = high_pop_user_df_sample['user_id:token'].map(user_id_mapping)\n",
    "    med_pop_user_df_sample['user_id:token'] = med_pop_user_df_sample['user_id:token'].map(user_id_mapping)\n",
    "    low_pop_user_df_sample['user_id:token'] = low_pop_user_df_sample['user_id:token'].map(user_id_mapping)\n",
    "\n",
    "    user_df_sample['user_id:token'] = user_df_sample['user_id:token'].map(user_id_mapping)\n",
    "    poi_df_sample['item_id:token'] = poi_df_sample['item_id:token'].map(business_id_mapping)\n",
    "\n",
    "    return checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9q/p0b6nwnd5071040x3849mdvc0000gn/T/ipykernel_50644/2675449884.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  user_df_sample['user_id:token'] = user_df_sample['user_id:token'].map(user_id_mapping)\n",
      "/var/folders/9q/p0b6nwnd5071040x3849mdvc0000gn/T/ipykernel_50644/2675449884.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  poi_df_sample['item_id:token'] = poi_df_sample['item_id:token'].map(business_id_mapping)\n"
     ]
    }
   ],
   "source": [
    "checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample = id_factorizer(checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id:token</th>\n",
       "      <th>item_id:token</th>\n",
       "      <th>timestamp:float</th>\n",
       "      <th>business_popularity:float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Tue Apr 03 19:12:07 +0000 2012</td>\n",
       "      <td>0.006790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Tue Apr 03 19:35:36 +0000 2012</td>\n",
       "      <td>0.068623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Tue Apr 03 19:51:50 +0000 2012</td>\n",
       "      <td>0.010346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Tue Apr 03 19:51:59 +0000 2012</td>\n",
       "      <td>0.001536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Tue Apr 03 19:59:06 +0000 2012</td>\n",
       "      <td>0.043970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447561</th>\n",
       "      <td>816</td>\n",
       "      <td>24</td>\n",
       "      <td>Sat Feb 16 02:30:20 +0000 2013</td>\n",
       "      <td>0.961122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447562</th>\n",
       "      <td>881</td>\n",
       "      <td>24</td>\n",
       "      <td>Sat Feb 16 02:32:51 +0000 2013</td>\n",
       "      <td>0.961122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447564</th>\n",
       "      <td>1312</td>\n",
       "      <td>4300</td>\n",
       "      <td>Sat Feb 16 02:33:18 +0000 2013</td>\n",
       "      <td>0.001778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447567</th>\n",
       "      <td>464</td>\n",
       "      <td>1575</td>\n",
       "      <td>Sat Feb 16 02:34:35 +0000 2013</td>\n",
       "      <td>0.003071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447568</th>\n",
       "      <td>1490</td>\n",
       "      <td>88</td>\n",
       "      <td>Sat Feb 16 02:34:53 +0000 2013</td>\n",
       "      <td>0.045587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>279329 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id:token  item_id:token                 timestamp:float  \\\n",
       "0                   0              0  Tue Apr 03 19:12:07 +0000 2012   \n",
       "1                   0              1  Tue Apr 03 19:35:36 +0000 2012   \n",
       "2                   1              2  Tue Apr 03 19:51:50 +0000 2012   \n",
       "3                   2              3  Tue Apr 03 19:51:59 +0000 2012   \n",
       "4                   3              4  Tue Apr 03 19:59:06 +0000 2012   \n",
       "...               ...            ...                             ...   \n",
       "447561            816             24  Sat Feb 16 02:30:20 +0000 2013   \n",
       "447562            881             24  Sat Feb 16 02:32:51 +0000 2013   \n",
       "447564           1312           4300  Sat Feb 16 02:33:18 +0000 2013   \n",
       "447567            464           1575  Sat Feb 16 02:34:35 +0000 2013   \n",
       "447568           1490             88  Sat Feb 16 02:34:53 +0000 2013   \n",
       "\n",
       "        business_popularity:float  \n",
       "0                        0.006790  \n",
       "1                        0.068623  \n",
       "2                        0.010346  \n",
       "3                        0.001536  \n",
       "4                        0.043970  \n",
       "...                           ...  \n",
       "447561                   0.961122  \n",
       "447562                   0.961122  \n",
       "447564                   0.001778  \n",
       "447567                   0.003071  \n",
       "447568                   0.045587  \n",
       "\n",
       "[279329 rows x 4 columns]"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_saver(df, filename, framework):\n",
    "    if not os.path.exists(DATASET_DIR + \"processed_data_\" + framework):\n",
    "        os.makedirs(DATASET_DIR + \"processed_data_\" + framework)\n",
    "    \n",
    "    df.to_csv(DATASET_DIR + \"processed_data_\" + framework + \"/\" + filename + \".csv\")\n",
    "    print(\"Data saved as \" + framework + filename + \".csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved as cornacuser_events.csv\n",
      "Data saved as cornachigh_pop_user_sample.csv\n",
      "Data saved as cornacmedium_pop_user_sample.csv\n",
      "Data saved as cornaclow_pop_user_sample.csv\n"
     ]
    }
   ],
   "source": [
    "# first of all saving data for cornac\n",
    "data_saver(checkin_df_sample, \"user_events\", \"cornac\")\n",
    "data_saver(high_pop_user_df_sample, \"high_pop_user_sample\", \"cornac\")\n",
    "data_saver(med_pop_user_df_sample, \"medium_pop_user_sample\", \"cornac\")\n",
    "data_saver(low_pop_user_df_sample, \"low_pop_user_sample\", \"cornac\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_saver_recbole(df, framework, suffix):\n",
    "    \n",
    "    if not os.path.exists(DATASET_DIR + \"processed_data_\" + framework):\n",
    "        os.makedirs(DATASET_DIR + \"processed_data_\" + framework)\n",
    "\n",
    "    df.to_csv(f\"{DATASET_DIR}processed_data_{framework}/{dataset}_sample.{suffix}\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_sample['review_id:token'] = range(1, len(checkin_df_sample) + 1)\n",
    "# Step 1: Group by user_id and business_id and count check-ins\n",
    "checkin_df_sample['checkin_count:float'] = checkin_df_sample.groupby(['user_id:token', 'item_id:token'])['item_id:token'].transform('count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id:token</th>\n",
       "      <th>item_id:token</th>\n",
       "      <th>timestamp:float</th>\n",
       "      <th>business_popularity:float</th>\n",
       "      <th>review_id:token</th>\n",
       "      <th>checkin_count:float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Tue Apr 03 19:12:07 +0000 2012</td>\n",
       "      <td>0.006790</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Tue Apr 03 19:35:36 +0000 2012</td>\n",
       "      <td>0.068623</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Tue Apr 03 19:51:50 +0000 2012</td>\n",
       "      <td>0.010346</td>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Tue Apr 03 19:51:59 +0000 2012</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Tue Apr 03 19:59:06 +0000 2012</td>\n",
       "      <td>0.043970</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447561</th>\n",
       "      <td>816</td>\n",
       "      <td>24</td>\n",
       "      <td>Sat Feb 16 02:30:20 +0000 2013</td>\n",
       "      <td>0.961122</td>\n",
       "      <td>279325</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447562</th>\n",
       "      <td>881</td>\n",
       "      <td>24</td>\n",
       "      <td>Sat Feb 16 02:32:51 +0000 2013</td>\n",
       "      <td>0.961122</td>\n",
       "      <td>279326</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447564</th>\n",
       "      <td>1312</td>\n",
       "      <td>4300</td>\n",
       "      <td>Sat Feb 16 02:33:18 +0000 2013</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>279327</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447567</th>\n",
       "      <td>464</td>\n",
       "      <td>1575</td>\n",
       "      <td>Sat Feb 16 02:34:35 +0000 2013</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>279328</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447568</th>\n",
       "      <td>1490</td>\n",
       "      <td>88</td>\n",
       "      <td>Sat Feb 16 02:34:53 +0000 2013</td>\n",
       "      <td>0.045587</td>\n",
       "      <td>279329</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>279329 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id:token  item_id:token                 timestamp:float  \\\n",
       "0                   0              0  Tue Apr 03 19:12:07 +0000 2012   \n",
       "1                   0              1  Tue Apr 03 19:35:36 +0000 2012   \n",
       "2                   1              2  Tue Apr 03 19:51:50 +0000 2012   \n",
       "3                   2              3  Tue Apr 03 19:51:59 +0000 2012   \n",
       "4                   3              4  Tue Apr 03 19:59:06 +0000 2012   \n",
       "...               ...            ...                             ...   \n",
       "447561            816             24  Sat Feb 16 02:30:20 +0000 2013   \n",
       "447562            881             24  Sat Feb 16 02:32:51 +0000 2013   \n",
       "447564           1312           4300  Sat Feb 16 02:33:18 +0000 2013   \n",
       "447567            464           1575  Sat Feb 16 02:34:35 +0000 2013   \n",
       "447568           1490             88  Sat Feb 16 02:34:53 +0000 2013   \n",
       "\n",
       "        business_popularity:float  review_id:token  checkin_count:float  \n",
       "0                        0.006790                1                    1  \n",
       "1                        0.068623                2                   25  \n",
       "2                        0.010346                3                  128  \n",
       "3                        0.001536                4                    1  \n",
       "4                        0.043970                5                   15  \n",
       "...                           ...              ...                  ...  \n",
       "447561                   0.961122           279325                  143  \n",
       "447562                   0.961122           279326                   11  \n",
       "447564                   0.001778           279327                    3  \n",
       "447567                   0.003071           279328                    1  \n",
       "447568                   0.045587           279329                    1  \n",
       "\n",
       "[279329 rows x 6 columns]"
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_sample = convert_to_unix_timestamp(checkin_df_sample, \"timestamp:float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_sample.sort_values(by=\"checkin_count:float\", ascending=False)\n",
    "# very important: keeping the duplicate check-ins for the context aware recommendation to have the timestamps saved\n",
    "checkin_df_timestamps = checkin_df_sample.copy()\n",
    "\n",
    "# very important: dropping duplicate check-ins \n",
    "checkin_df_sample = checkin_df_sample.drop_duplicates(subset=[\"user_id:token\", \"item_id:token\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_saver_recbole(checkin_df_sample, \"recbole\", \"inter\")\n",
    "data_saver_recbole(user_df_sample, \"recbole\", \"user\")\n",
    "data_saver_recbole(poi_df_sample, \"recbole\", \"item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id:token</th>\n",
       "      <th>category_id:token</th>\n",
       "      <th>category_name:token_seq</th>\n",
       "      <th>lat:float</th>\n",
       "      <th>lon:float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4d954b0ea243a5684a65b473</td>\n",
       "      <td>Convenience Store</td>\n",
       "      <td>35.714542</td>\n",
       "      <td>139.480065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>4bf58dd8d48988d129951735</td>\n",
       "      <td>Train Station</td>\n",
       "      <td>35.700253</td>\n",
       "      <td>139.480255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>4bf58dd8d48988d162941735</td>\n",
       "      <td>Other Great Outdoors</td>\n",
       "      <td>35.755759</td>\n",
       "      <td>139.733573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>4bf58dd8d48988d1d1941735</td>\n",
       "      <td>Ramen /  Noodle House</td>\n",
       "      <td>35.693121</td>\n",
       "      <td>139.699447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>4bf58dd8d48988d1eb931735</td>\n",
       "      <td>Airport</td>\n",
       "      <td>35.548963</td>\n",
       "      <td>139.784611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509497</th>\n",
       "      <td>7288</td>\n",
       "      <td>4d954b0ea243a5684a65b473</td>\n",
       "      <td>Convenience Store</td>\n",
       "      <td>35.600404</td>\n",
       "      <td>139.592516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510129</th>\n",
       "      <td>7289</td>\n",
       "      <td>4bf58dd8d48988d1d2941735</td>\n",
       "      <td>Sushi Restaurant</td>\n",
       "      <td>35.697197</td>\n",
       "      <td>139.815391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519107</th>\n",
       "      <td>7293</td>\n",
       "      <td>4bf58dd8d48988d103941735</td>\n",
       "      <td>Home (private)</td>\n",
       "      <td>35.650417</td>\n",
       "      <td>139.757541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528409</th>\n",
       "      <td>7298</td>\n",
       "      <td>4bf58dd8d48988d1fe931735</td>\n",
       "      <td>Bus Station</td>\n",
       "      <td>35.698772</td>\n",
       "      <td>139.617411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530536</th>\n",
       "      <td>7300</td>\n",
       "      <td>4bf58dd8d48988d1f9931735</td>\n",
       "      <td>Road</td>\n",
       "      <td>35.529572</td>\n",
       "      <td>139.735708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7324 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_id:token         category_id:token category_name:token_seq  \\\n",
       "2                   0  4d954b0ea243a5684a65b473       Convenience Store   \n",
       "7                   1  4bf58dd8d48988d129951735           Train Station   \n",
       "8                   2  4bf58dd8d48988d162941735    Other Great Outdoors   \n",
       "9                   3  4bf58dd8d48988d1d1941735   Ramen /  Noodle House   \n",
       "10                  4  4bf58dd8d48988d1eb931735                 Airport   \n",
       "...               ...                       ...                     ...   \n",
       "509497           7288  4d954b0ea243a5684a65b473       Convenience Store   \n",
       "510129           7289  4bf58dd8d48988d1d2941735        Sushi Restaurant   \n",
       "519107           7293  4bf58dd8d48988d103941735          Home (private)   \n",
       "528409           7298  4bf58dd8d48988d1fe931735             Bus Station   \n",
       "530536           7300  4bf58dd8d48988d1f9931735                    Road   \n",
       "\n",
       "        lat:float   lon:float  \n",
       "2       35.714542  139.480065  \n",
       "7       35.700253  139.480255  \n",
       "8       35.755759  139.733573  \n",
       "9       35.693121  139.699447  \n",
       "10      35.548963  139.784611  \n",
       "...           ...         ...  \n",
       "509497  35.600404  139.592516  \n",
       "510129  35.697197  139.815391  \n",
       "519107  35.650417  139.757541  \n",
       "528409  35.698772  139.617411  \n",
       "530536  35.529572  139.735708  \n",
       "\n",
       "[7324 rows x 5 columns]"
      ]
     },
     "execution_count": 737,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poi_df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_timestamps = checkin_df_timestamps[[\"user_id:token\", \"item_id:token\", \"timestamp:float\"]] # FINAL\n",
    "checkins_capri_train_test_tune = checkin_df_sample[[\"user_id:token\", \"item_id:token\", \"timestamp:float\", \"checkin_count:float\"]]\n",
    "try:\n",
    "    poi_df_sample_capri = poi_df_sample[[\"item_id:token\", \"lat:float\", \"lon:float\"]] # FINAL\n",
    "except KeyError: # in the snowcard data the coordinates are not given\n",
    "    poi_df_sample_capri = poi_df_sample[[\"item_id:token\"]]\n",
    "datasize_capri = pd.DataFrame(data={\"num_users\" : [len(checkins_capri_train_test_tune[\"user_id:token\"].unique())], \"num_items\" : [len(checkins_capri_train_test_tune[\"item_id:token\"].unique())]}) # FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set:\n",
      "      user_id:token  item_id:token  checkin_count:float\n",
      "0                 0              0                    1\n",
      "1                 0              1                   25\n",
      "16                0             12                    1\n",
      "5686              0             79                    3\n",
      "5712              0           1793                    1\n",
      "\n",
      "Validation Set:\n",
      "        user_id:token  item_id:token  checkin_count:float\n",
      "121461              0           4224                    1\n",
      "121505              0            124                    1\n",
      "121525              0           3891                    1\n",
      "121558              0           1887                    1\n",
      "121779              0           3239                    1\n",
      "\n",
      "Test Set:\n",
      "        user_id:token  item_id:token  checkin_count:float\n",
      "127808              0           6265                    2\n",
      "138497              0           2223                    1\n",
      "138696              0           5780                    1\n",
      "139514              0            425                    1\n",
      "157393              0             31                    1\n"
     ]
    }
   ],
   "source": [
    "# splitting the data into train, test, and tune\n",
    "checkins_capri_train_test_tune = checkins_capri_train_test_tune.sort_values(by=[\"user_id:token\", \"timestamp:float\"])\n",
    "checkins_capri_train_test_tune = checkins_capri_train_test_tune[[\"user_id:token\", \"item_id:token\", \"checkin_count:float\"]]\n",
    "\n",
    "# Split the data\n",
    "train_list = []\n",
    "val_list = []\n",
    "test_list = []\n",
    "\n",
    "for user, group in checkins_capri_train_test_tune.groupby('user_id:token'):\n",
    "    n = len(group)\n",
    "    train_end = int(n * 0.65)\n",
    "    val_end = int(n * 0.80)\n",
    "    \n",
    "    train_list.append(group.iloc[:train_end])\n",
    "    val_list.append(group.iloc[train_end:val_end])\n",
    "    test_list.append(group.iloc[val_end:])\n",
    "\n",
    "# Combine lists into DataFrames\n",
    "train_df = pd.concat(train_list)\n",
    "val_df = pd.concat(val_list)\n",
    "test_df = pd.concat(test_list)\n",
    "\n",
    "\n",
    "\n",
    "# Check the splits\n",
    "\n",
    "# FINAL 6-8\n",
    "print(\"Train Set:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nValidation Set:\")\n",
    "print(val_df.head())\n",
    "print(\"\\nTest Set:\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasaver_capri(df, filename):\n",
    "    \n",
    "    if not os.path.exists(DATASET_DIR + \"processed_data_capri\"):\n",
    "        os.makedirs(DATASET_DIR + \"processed_data_capri\")\n",
    "    \n",
    "    df.to_csv(DATASET_DIR + \"processed_data_capri/\" + filename + \".txt\", sep='\\t', index=False, header=False)\n",
    "    print(\"Data saved as \" + filename + \".txt\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved as poiCategories.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9q/p0b6nwnd5071040x3849mdvc0000gn/T/ipykernel_50644/2368300871.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  poi_df_sample[\"category_id:token\"], category_id = pd.factorize(poi_df_sample[\"category_name:token_seq\"])\n"
     ]
    }
   ],
   "source": [
    "# adding a category column\n",
    "\n",
    "if dataset == \"yelp\":\n",
    "    # Split the 'category_name' column by commas\n",
    "    poi_df_sample['category_name_unstacked:token_seq'] = poi_df_sample['category_name:token_seq'].str.split(', ')\n",
    "\n",
    "    # Unstack the categories into multiple rows\n",
    "    category_df_sample = poi_df_sample.explode('category_name_unstacked:token_seq')\n",
    "    category_counts = category_df_sample[\"category_name_unstacked:token_seq\"].value_counts()\n",
    "    category_mask = category_df_sample[\"category_name_unstacked:token_seq\"].map(category_counts) >= 25\n",
    "    category_df_sample_filtered = category_df_sample.loc[category_mask]\n",
    "    category_df_sample_filtered[\"category_id:token\"], category_id = pd.factorize(category_df_sample_filtered[\"category_name_unstacked:token_seq\"])\n",
    "    category_df_sample_filtered.dropna(inplace=True)\n",
    "    datasize_capri = pd.DataFrame(data={\"num_users\" : [len(checkins_capri_train_test_tune[\"user_id:token\"].unique())], \"num_items\" : [len(checkins_capri_train_test_tune[\"item_id:token\"].unique())], \"num_categories\" : [len(category_df_sample_filtered[\"category_id:token\"].unique())]}) # FINAL\n",
    "    datasaver_capri(category_df_sample_filtered, \"poiCategories\")\n",
    "\n",
    "\n",
    "elif dataset == \"foursquarenyc\" or dataset == \"foursquaretky\":\n",
    "    poi_df_sample[\"category_id:token\"], category_id = pd.factorize(poi_df_sample[\"category_name:token_seq\"])\n",
    "    datasize_capri = pd.DataFrame(data={\"num_users\" : [len(checkins_capri_train_test_tune[\"user_id:token\"].unique())], \"num_items\" : [len(checkins_capri_train_test_tune[\"item_id:token\"].unique())], \"num_categories\" : [len(poi_df_sample[\"category_id:token\"].unique())]})\n",
    "    poi_df_categories = poi_df_sample[[\"item_id:token\", \"category_id:token\"]]\n",
    "    datasaver_capri(poi_df_categories, \"poiCategories\")\n",
    "\n",
    "elif dataset == \"snowcard\":\n",
    "    datasize_capri = pd.DataFrame(data={\"num_users\" : [len(checkins_capri_train_test_tune[\"user_id:token\"].unique())], \"num_items\" : [len(checkins_capri_train_test_tune[\"item_id:token\"].unique())], \"num_categories\" : [len(poi_df_sample[\"category_id:token\"].unique())]})\n",
    "    poi_df_categories = poi_df_sample[[\"item_id:token\", \"category_id:token\"]]\n",
    "    datasaver_capri(poi_df_categories, \"poiCategories\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved as checkins.txt\n",
      "Data saved as dataSize.txt\n",
      "Data saved as poiCoos.txt\n",
      "Data saved as train.txt\n",
      "Data saved as tune.txt\n",
      "Data saved as test.txt\n"
     ]
    }
   ],
   "source": [
    "datasaver_capri(checkin_df_timestamps, \"checkins\")\n",
    "datasaver_capri(datasize_capri, \"dataSize\")\n",
    "datasaver_capri(poi_df_sample_capri, \"poiCoos\")\n",
    "datasaver_capri(train_df, \"train\")\n",
    "datasaver_capri(val_df, \"tune\")\n",
    "datasaver_capri(test_df, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prep_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
