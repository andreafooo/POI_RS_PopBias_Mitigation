{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from globals import BASE_DIR\n",
    "\n",
    "available_datasets = [\"foursquaretky\", \"yelp\", \"gowalla\", \"brightkite\", \"snowcard\"]\n",
    "\n",
    "\n",
    "dataset = \"brightkite\" # beware: opening the yelp file with pandas will take a lot of time, approx 10 min on my machine\n",
    "include_categories = False # for context-aware recommendation\n",
    "\n",
    "DATASET_DIR = f\"{BASE_DIR}{dataset}_dataset/\"\n",
    "#DATASET_DIR = f\"/Users/andreaforster/Documents/data_thesis/{dataset}_dataset/\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_big_json(file_path):\n",
    "    \"\"\"This function is used to open the Yelp data\"\"\"\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_unix_timestamp(df, column_name):\n",
    "    \"\"\"\n",
    "    Convert a column of timestamps in a DataFrame to Unix timestamps.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the timestamp column.\n",
    "        column_name (str): The name of the column with timestamps in \"%Y-%m-%d %H:%M:%S\" format.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with an additional column for Unix timestamps.\n",
    "    \"\"\"\n",
    "    df[column_name] = pd.to_datetime(df[column_name], format=\"mixed\")\n",
    "    \n",
    "    df[f'{column_name}'] = df[column_name].apply(lambda x: x.timestamp())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset-specific preprocessing\n",
    "if dataset == \"snowcard\":\n",
    "    checkin_df = pd.read_csv(DATASET_DIR+\"TSC_EEL_EXPORT.csv\", encoding=\"latin1\", sep=\";\", header=None, names=[\"timestamp:float\", \"user_id:token\", \"category_id:token\", \"category_name:token_seq\", \"name:token_seq\", \"user_type:token_seq\"])\n",
    "    checkin_df[\"item_id:token\"], item_id = pd.factorize(checkin_df[\"name:token_seq\"])\n",
    "    user_df = checkin_df[[\"user_id:token\", \"user_type:token_seq\"]].drop_duplicates(subset=[\"user_id:token\"])\n",
    "    poi_df = checkin_df[[\"item_id:token\", \"name:token_seq\", \"category_id:token\", \"category_name:token_seq\"]].drop_duplicates(subset=[\"item_id:token\"])\n",
    "    coordinates_df = pd.read_excel(DATASET_DIR+\"snowcard_lifts.xlsx\")\n",
    "    coordinates_df[[\"lat:float\", \"lon:float\"]] = coordinates_df[\"lat_lon\"].str.split(', ', expand=True)\n",
    "    coordinates_df.drop(columns=[\"lat_lon\"], inplace=True)\n",
    "    poi_df = pd.merge(poi_df, coordinates_df[['category_name:token_seq', 'lat:float', 'lon:float']], on='category_name:token_seq', how='left')\n",
    "    checkin_df = checkin_df[[\"user_id:token\", \"item_id:token\", \"timestamp:float\"]]\n",
    "\n",
    "elif dataset == \"foursquarenyc\" or dataset == \"foursquaretky\":\n",
    "    checkin_df = pd.read_csv(DATASET_DIR + \"foursquare_data.csv\", sep=\",\")\n",
    "    checkin_df = checkin_df.drop(columns=[\"timezoneOffset\"])\n",
    "    checkin_df = checkin_df.rename(columns={\"venueId\": \"item_id:token\", \"venueCategoryId\": \"category_id:token\", \"venueCategory\": \"category_name:token_seq\", \"userId\": \"user_id:token\", \"utcTimestamp\": \"timestamp:float\", \"latitude\": \"lat:float\", \"longitude\": \"lon:float\"})\n",
    "    user_df = checkin_df[[\"user_id:token\"]].drop_duplicates()\n",
    "\n",
    "    poi_df = checkin_df[[\"item_id:token\", \"category_id:token\", \"category_name:token_seq\", \"lat:float\", \"lon:float\"]].drop_duplicates(subset=[\"item_id:token\"])\n",
    "    checkin_df = checkin_df[[\"user_id:token\", \"item_id:token\", \"timestamp:float\"]]\n",
    "\n",
    "elif dataset == \"gowalla\" or dataset == \"brightkite\":\n",
    "    checkin_df = pd.read_csv(DATASET_DIR + f\"loc-{dataset}_totalCheckins.txt\", sep=\"\\t\", header=None, names=['user_id:token', 'timestamp:float', 'lat:float', 'lon:float', 'item_id:token'])\n",
    "    checkin_df = checkin_df[~checkin_df['item_id:token'].isin([\"00000000000000000000000000000000\", \"ede07eeea22411dda0ef53e233ec57ca\"])]\n",
    "    user_df = pd.read_csv(DATASET_DIR + f\"loc-{dataset}_edges.txt\", sep=\"\\t\", header=None, names=['user_id:token', 'friends:token_seq'])\n",
    "    user_df = user_df.groupby('user_id:token')['friends:token_seq'].apply(lambda x: ','.join(map(str, x))).reset_index()\n",
    "    user_df.columns = ['user_id:token', 'friends:token_seq']\n",
    "    poi_df = checkin_df[['item_id:token', \"lat:float\", \"lon:float\"]].drop_duplicates(subset=\"item_id:token\")\n",
    "    checkin_df = checkin_df.drop(columns=[\"lat:float\", \"lon:float\"])\n",
    "\n",
    "elif dataset == \"yelp\":\n",
    "    poi_df = pd.read_json(DATASET_DIR + \"yelp_academic_dataset_business.json\", lines=True)\n",
    "    poi_df = poi_df.loc[poi_df['is_open'] == 1]\n",
    "    poi_df = poi_df.drop(columns=[\"review_count\", \"stars\", \"hours\", \"is_open\", \"city\", \"state\", \"postal_code\", \"attributes\", \"address\"])\n",
    "    poi_df = poi_df.rename(columns={\"latitude\": \"lat:float\", \"longitude\": \"lon:float\", \"business_id\": \"item_id:token\", \"name\":\"name:token_seq\", \"categories\":\"category_name:token_seq\"})\n",
    "    user_df = open_big_json(DATASET_DIR + \"yelp_academic_dataset_user.json\")\n",
    "    user_df = user_df.drop(columns=[\"review_count\", \"name\", \"yelping_since\", \"useful\", \"funny\", \"cool\", \"elite\", \"fans\", \"compliment_hot\", \"average_stars\", \"compliment_more\", \"compliment_profile\", \"compliment_cute\", \"compliment_list\", \"compliment_note\", \"compliment_plain\", \"compliment_cool\", \"compliment_funny\", \"compliment_writer\", \"compliment_photos\"])\n",
    "    user_df = user_df.rename(columns={\"user_id\": \"user_id:token\", \"friends\": \"friends:token_seq\"})\n",
    "    checkin_df = open_big_json(DATASET_DIR + \"yelp_academic_dataset_review.json\")\n",
    "    checkin_df = checkin_df.drop(columns=[\"text\", \"cool\", \"stars\", \"useful\", \"funny\", \"review_id\"])\n",
    "    checkin_df = checkin_df.rename(columns={\"user_id\": \"user_id:token\", \"business_id\": \"item_id:token\", \"date\": \"timestamp:float\"})\n",
    "    checkin_df['timestamp'] = pd.to_datetime(checkin_df['timestamp:float'], errors='coerce')\n",
    "\n",
    "    checkin_df['year'] = checkin_df['timestamp'].dt.year      # Extract the year from the 'timestamp' column\n",
    "    checkin_df = checkin_df[checkin_df['year'] >= 2018]       # Keep only the check-ins from 2018 and 2019\n",
    "    checkin_df = checkin_df[checkin_df['year'] < 2020]\n",
    "    checkin_df.drop(columns=[\"year\", \"timestamp\"], inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df.sort_values(by=\"timestamp:float\", ascending=True, inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id:token</th>\n",
       "      <th>lat:float</th>\n",
       "      <th>lon:float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88c46bf20db295831bd2d1718ad7e6f5</td>\n",
       "      <td>39.747652</td>\n",
       "      <td>-104.992510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7a0f88982aa015062b95e3b4843f9ca2</td>\n",
       "      <td>39.891383</td>\n",
       "      <td>-105.070814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dd7cd3d264c2d063832db506fba8bf79</td>\n",
       "      <td>39.891077</td>\n",
       "      <td>-105.068532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9848afcc62e500a01cf6fbf24b797732f8963683</td>\n",
       "      <td>39.750469</td>\n",
       "      <td>-104.999073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2ef143e12038c870038df53e0478cefc</td>\n",
       "      <td>39.752713</td>\n",
       "      <td>-104.996337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747270</th>\n",
       "      <td>be2946b81c8a4c8486744c3952058381</td>\n",
       "      <td>13.736849</td>\n",
       "      <td>100.554218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747271</th>\n",
       "      <td>7361759796acea309f90842a3248df7f</td>\n",
       "      <td>13.746168</td>\n",
       "      <td>100.531083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747272</th>\n",
       "      <td>9453076485b111ddb2ca003048c0801e</td>\n",
       "      <td>57.685848</td>\n",
       "      <td>11.918092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747275</th>\n",
       "      <td>efa55056a22411dd9a9fc3d14b4b8edd</td>\n",
       "      <td>33.853611</td>\n",
       "      <td>35.577222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747279</th>\n",
       "      <td>6ba679dce99c11dd8e2f003048c10834</td>\n",
       "      <td>33.855255</td>\n",
       "      <td>35.578156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>772965 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    item_id:token  lat:float   lon:float\n",
       "0                88c46bf20db295831bd2d1718ad7e6f5  39.747652 -104.992510\n",
       "1                7a0f88982aa015062b95e3b4843f9ca2  39.891383 -105.070814\n",
       "2                dd7cd3d264c2d063832db506fba8bf79  39.891077 -105.068532\n",
       "3        9848afcc62e500a01cf6fbf24b797732f8963683  39.750469 -104.999073\n",
       "4                2ef143e12038c870038df53e0478cefc  39.752713 -104.996337\n",
       "...                                           ...        ...         ...\n",
       "4747270          be2946b81c8a4c8486744c3952058381  13.736849  100.554218\n",
       "4747271          7361759796acea309f90842a3248df7f  13.746168  100.531083\n",
       "4747272          9453076485b111ddb2ca003048c0801e  57.685848   11.918092\n",
       "4747275          efa55056a22411dd9a9fc3d14b4b8edd  33.853611   35.577222\n",
       "4747279          6ba679dce99c11dd8e2f003048c10834  33.855255   35.578156\n",
       "\n",
       "[772965 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id:token</th>\n",
       "      <th>friends:token_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0,3,4,5,7,9,23,26,29,30,35,38,40,42,46,56,74,9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0,10,12,13,20,29,30,32,33,34,40,45,47,50,55,63...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0,1,4,6,7,9,11,20,26,27,30,36,38,42,46,49,54,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0,1,3,5,7,9,11,23,26,30,38,42,46,74,91,93,123,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58223</th>\n",
       "      <td>58223</td>\n",
       "      <td>58212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58224</th>\n",
       "      <td>58224</td>\n",
       "      <td>58219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58225</th>\n",
       "      <td>58225</td>\n",
       "      <td>58220,58226,58227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58226</th>\n",
       "      <td>58226</td>\n",
       "      <td>58220,58225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58227</th>\n",
       "      <td>58227</td>\n",
       "      <td>58225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58228 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id:token                                  friends:token_seq\n",
       "0                  0  1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,1...\n",
       "1                  1  0,3,4,5,7,9,23,26,29,30,35,38,40,42,46,56,74,9...\n",
       "2                  2  0,10,12,13,20,29,30,32,33,34,40,45,47,50,55,63...\n",
       "3                  3  0,1,4,6,7,9,11,20,26,27,30,36,38,42,46,49,54,1...\n",
       "4                  4  0,1,3,5,7,9,11,23,26,30,38,42,46,74,91,93,123,...\n",
       "...              ...                                                ...\n",
       "58223          58223                                              58212\n",
       "58224          58224                                              58219\n",
       "58225          58225                                  58220,58226,58227\n",
       "58226          58226                                        58220,58225\n",
       "58227          58227                                              58225\n",
       "\n",
       "[58228 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id:token</th>\n",
       "      <th>timestamp:float</th>\n",
       "      <th>item_id:token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1324853</th>\n",
       "      <td>4570</td>\n",
       "      <td>2008-03-21T20:36:21Z</td>\n",
       "      <td>0ade7c2cf97f75d009975f4d720d1fa6c19f4897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15411</th>\n",
       "      <td>12</td>\n",
       "      <td>2008-03-22T05:01:29Z</td>\n",
       "      <td>7b52009b64fd0a2a49e6d8a939753077792b0554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15410</th>\n",
       "      <td>12</td>\n",
       "      <td>2008-03-22T05:50:55Z</td>\n",
       "      <td>fa35e192121eabf3dabf9f5ea6abdbcbc107ac3b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15409</th>\n",
       "      <td>12</td>\n",
       "      <td>2008-03-22T06:17:35Z</td>\n",
       "      <td>fa35e192121eabf3dabf9f5ea6abdbcbc107ac3b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15408</th>\n",
       "      <td>12</td>\n",
       "      <td>2008-03-22T06:34:37Z</td>\n",
       "      <td>fa35e192121eabf3dabf9f5ea6abdbcbc107ac3b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909859</th>\n",
       "      <td>7573</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909960</th>\n",
       "      <td>7573</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909961</th>\n",
       "      <td>7573</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3745451</th>\n",
       "      <td>22819</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3745452</th>\n",
       "      <td>22819</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4475772 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id:token       timestamp:float  \\\n",
       "1324853           4570  2008-03-21T20:36:21Z   \n",
       "15411               12  2008-03-22T05:01:29Z   \n",
       "15410               12  2008-03-22T05:50:55Z   \n",
       "15409               12  2008-03-22T06:17:35Z   \n",
       "15408               12  2008-03-22T06:34:37Z   \n",
       "...                ...                   ...   \n",
       "1909859           7573                   NaN   \n",
       "1909960           7573                   NaN   \n",
       "1909961           7573                   NaN   \n",
       "3745451          22819                   NaN   \n",
       "3745452          22819                   NaN   \n",
       "\n",
       "                                    item_id:token  \n",
       "1324853  0ade7c2cf97f75d009975f4d720d1fa6c19f4897  \n",
       "15411    7b52009b64fd0a2a49e6d8a939753077792b0554  \n",
       "15410    fa35e192121eabf3dabf9f5ea6abdbcbc107ac3b  \n",
       "15409    fa35e192121eabf3dabf9f5ea6abdbcbc107ac3b  \n",
       "15408    fa35e192121eabf3dabf9f5ea6abdbcbc107ac3b  \n",
       "...                                           ...  \n",
       "1909859                                       NaN  \n",
       "1909960                                       NaN  \n",
       "1909961                                       NaN  \n",
       "3745451                                       NaN  \n",
       "3745452                                       NaN  \n",
       "\n",
       "[4475772 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_timestamp = checkin_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Group by user_id and business_id and count check-ins\n",
    "checkin_df['checkin_count:float'] = checkin_df.groupby(['user_id:token', 'item_id:token'])['item_id:token'].transform('count')\n",
    "checkin_df = checkin_df.drop_duplicates(subset=[\"user_id:token\", \"item_id:token\"], keep=\"first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users, number of POIs 50506 772965\n",
      "Sparsity: 0.9999725464834079\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of users, number of POIs\", len(checkin_df[\"user_id:token\"].unique()), len(checkin_df[\"item_id:token\"].unique())\n",
    ")\n",
    "print(\"Sparsity:\", 1 - len(checkin_df) / (len(checkin_df[\"user_id:token\"].unique()) * len(checkin_df[\"item_id:token\"].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    50506.000000\n",
       "mean        21.220607\n",
       "std         53.821086\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          4.000000\n",
       "75%         16.000000\n",
       "max       1293.000000\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df[\"user_id:token\"].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    772964.000000\n",
       "mean          1.386567\n",
       "std           4.545214\n",
       "min           1.000000\n",
       "25%           1.000000\n",
       "50%           1.000000\n",
       "75%           1.000000\n",
       "max        1437.000000\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df[\"item_id:token\"].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df, min_reviews_user=15, min_reviews_business=10):\n",
    "    while True:\n",
    "\n",
    "        # Filter users with at least min_reviews reviews\n",
    "        user_counts = df['user_id:token'].value_counts()\n",
    "        user_mask = df['user_id:token'].map(user_counts) >= min_reviews_user\n",
    "        df_filtered = df.loc[user_mask]\n",
    "\n",
    "        # Filter businesses with at least min_reviews reviews\n",
    "        business_counts = df_filtered[\"item_id:token\"].value_counts()\n",
    "        business_mask = df_filtered['item_id:token'].map(business_counts) >= min_reviews_business\n",
    "        df_filtered = df_filtered.loc[business_mask]\n",
    "\n",
    "        \n",
    "\n",
    "        # If the size of the filtered DataFrame didn't change, break the loop\n",
    "        if df_filtered.shape[0] == df.shape[0]:\n",
    "            break\n",
    "\n",
    "        # Update the DataFrame for the next iteration\n",
    "        df = df_filtered\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_filtered = filter_df(checkin_df) # for gowalla i used business min 20 & user min 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df_filtered[\"user_id:token\"].value_counts().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "601"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df_filtered[\"user_id:token\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(601, 794)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df_filtered[\"user_id:token\"].nunique(), checkin_df_filtered[\"item_id:token\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id:token</th>\n",
       "      <th>timestamp:float</th>\n",
       "      <th>item_id:token</th>\n",
       "      <th>checkin_count:float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48774</th>\n",
       "      <td>45</td>\n",
       "      <td>2008-03-25T00:26:05Z</td>\n",
       "      <td>ee8b1d0ea22411ddb074dbd65f1665cf</td>\n",
       "      <td>179.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564931</th>\n",
       "      <td>1587</td>\n",
       "      <td>2008-03-25T19:26:39Z</td>\n",
       "      <td>ee903e42a22411dda467ff8194c6b95d</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564930</th>\n",
       "      <td>1587</td>\n",
       "      <td>2008-03-26T18:27:55Z</td>\n",
       "      <td>ee8a3de4a22411dd874bf31dc846041d</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564929</th>\n",
       "      <td>1587</td>\n",
       "      <td>2008-03-28T02:25:44Z</td>\n",
       "      <td>eeb1943ea22411dd8482fba3141f3ee0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564927</th>\n",
       "      <td>1587</td>\n",
       "      <td>2008-03-29T06:29:51Z</td>\n",
       "      <td>eeb366e2a22411ddad663356f84ff6df</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101099</th>\n",
       "      <td>8155</td>\n",
       "      <td>2010-10-18T13:41:45Z</td>\n",
       "      <td>eefadd1aa22411ddb0fd7f1c9c809c0c</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101001</th>\n",
       "      <td>8155</td>\n",
       "      <td>2010-10-18T13:59:55Z</td>\n",
       "      <td>897351e744768373429b9df673db690f</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100996</th>\n",
       "      <td>8155</td>\n",
       "      <td>2010-10-18T14:00:49Z</td>\n",
       "      <td>ecbfba0ca22411ddb71dfb65ad521832</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100970</th>\n",
       "      <td>8155</td>\n",
       "      <td>2010-10-18T14:04:08Z</td>\n",
       "      <td>085887c14731a78bc946b71ac41fcf05</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100906</th>\n",
       "      <td>8155</td>\n",
       "      <td>2010-10-18T14:10:02Z</td>\n",
       "      <td>ece2e860a22411ddb74ad3c06c39a58f</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15361 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id:token       timestamp:float  \\\n",
       "48774               45  2008-03-25T00:26:05Z   \n",
       "564931            1587  2008-03-25T19:26:39Z   \n",
       "564930            1587  2008-03-26T18:27:55Z   \n",
       "564929            1587  2008-03-28T02:25:44Z   \n",
       "564927            1587  2008-03-29T06:29:51Z   \n",
       "...                ...                   ...   \n",
       "2101099           8155  2010-10-18T13:41:45Z   \n",
       "2101001           8155  2010-10-18T13:59:55Z   \n",
       "2100996           8155  2010-10-18T14:00:49Z   \n",
       "2100970           8155  2010-10-18T14:04:08Z   \n",
       "2100906           8155  2010-10-18T14:10:02Z   \n",
       "\n",
       "                            item_id:token  checkin_count:float  \n",
       "48774    ee8b1d0ea22411ddb074dbd65f1665cf                179.0  \n",
       "564931   ee903e42a22411dda467ff8194c6b95d                  1.0  \n",
       "564930   ee8a3de4a22411dd874bf31dc846041d                  9.0  \n",
       "564929   eeb1943ea22411dd8482fba3141f3ee0                 12.0  \n",
       "564927   eeb366e2a22411ddad663356f84ff6df                 28.0  \n",
       "...                                   ...                  ...  \n",
       "2101099  eefadd1aa22411ddb0fd7f1c9c809c0c                  1.0  \n",
       "2101001  897351e744768373429b9df673db690f                  2.0  \n",
       "2100996  ecbfba0ca22411ddb71dfb65ad521832                  1.0  \n",
       "2100970  085887c14731a78bc946b71ac41fcf05                  1.0  \n",
       "2100906  ece2e860a22411ddb74ad3c06c39a58f                  1.0  \n",
       "\n",
       "[15361 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = checkin_df_filtered['item_id:token'].value_counts().reset_index()\n",
    "value_counts.columns = ['item_id:token', 'count']\n",
    "\n",
    "max_count = value_counts['count'].max()\n",
    "value_counts['business_popularity:float'] = value_counts['count'] / max_count\n",
    " \n",
    "checkin_df_filtered = checkin_df_filtered.merge(value_counts[['item_id:token', 'business_popularity:float']], on = \"item_id:token\", how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_popularity_sample_calculator(checkin_df_filtered, poi_df, user_df, sep_num, checkin_df_timestamp):\n",
    "    # Calculate average popularity per user\n",
    "    average_popularity_per_user = checkin_df_filtered.groupby('user_id:token')['business_popularity:float'].mean().reset_index() # try out median of item popularities in user profile instead of mean \n",
    "    average_popularity_per_user.columns = ['user_id:token', 'average_popularity']\n",
    "\n",
    "    average_popularity_per_user = average_popularity_per_user.sort_values(by=\"average_popularity\", ascending=False)\n",
    "\n",
    "    # Get top users\n",
    "    high_pop_user_df_sample = average_popularity_per_user.head(sep_num)\n",
    "    \n",
    "    # Get the users around the median\n",
    "    median_index = len(average_popularity_per_user) // 2\n",
    "    start_med_index = max(median_index -int (sep_num*1.5), 0)\n",
    "    end_med_index = min(median_index + int(sep_num*1.5), len(average_popularity_per_user))\n",
    "    med_pop_user_df_sample = average_popularity_per_user.iloc[start_med_index:end_med_index]\n",
    "    \n",
    "    # Get the lowest users\n",
    "    low_pop_user_df_sample = average_popularity_per_user.tail(sep_num)\n",
    "\n",
    "    unique_users = list(set(high_pop_user_df_sample[\"user_id:token\"].tolist() + med_pop_user_df_sample[\"user_id:token\"].tolist() + low_pop_user_df_sample[\"user_id:token\"].tolist()))\n",
    "\n",
    "    checkin_df_sample = checkin_df_filtered[checkin_df_filtered[\"user_id:token\"].isin(unique_users)]\n",
    "    checkin_df_sample = checkin_df_sample[checkin_df_sample[\"user_id:token\"].isin(unique_users)]\n",
    "\n",
    "    user_df_sample = user_df[user_df[\"user_id:token\"].isin(unique_users)]\n",
    "    poi_df_sample = poi_df[poi_df[\"item_id:token\"].isin(checkin_df_sample[\"item_id:token\"])]\n",
    "\n",
    "    checkin_df_sample = checkin_df_sample[checkin_df_sample[\"item_id:token\"].isin(poi_df_sample[\"item_id:token\"])]\n",
    "\n",
    "    checkin_df_timestamp = checkin_df_timestamp[checkin_df_timestamp[\"user_id:token\"].isin(unique_users)]\n",
    "    checkin_df_timestamp = checkin_df_timestamp[checkin_df_timestamp[\"item_id:token\"].isin(poi_df_sample[\"item_id:token\"])]\n",
    "\n",
    "    \n",
    "    return checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample, checkin_df_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sep_num = 1500//5\n",
    "\n",
    "sep_num*1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "if checkin_df_filtered[\"user_id:token\"].nunique() > 1500:\n",
    "    sep_num = 1500 // 5\n",
    "else:\n",
    "    sep_num = checkin_df_filtered[\"user_id:token\"].nunique() // 5\n",
    "\n",
    "print(sep_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample, checkin_df_timestamp = user_popularity_sample_calculator(checkin_df_filtered, poi_df, user_df, sep_num, checkin_df_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_factorizer(checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample, checkin_df_timestamp):\n",
    "    \"\"\"Overwriting the actual ID with a factorized ID so that we can use the same ID both in RecBole and CAPRI\"\"\"\n",
    "    checkin_df_sample['user_id:token'], user_id_map = pd.factorize(checkin_df_sample['user_id:token'])\n",
    "    checkin_df_sample['item_id:token'], business_id_map = pd.factorize(checkin_df_sample['item_id:token'])\n",
    "\n",
    "    # Create mapping dictionaries\n",
    "    user_id_mapping = {original: i for i, original in enumerate(user_id_map)}\n",
    "    business_id_mapping = {original: j for j, original in enumerate(business_id_map)}\n",
    "\n",
    "    high_pop_user_df_sample['user_id:token'] = high_pop_user_df_sample['user_id:token'].map(user_id_mapping)\n",
    "    med_pop_user_df_sample['user_id:token'] = med_pop_user_df_sample['user_id:token'].map(user_id_mapping)\n",
    "    low_pop_user_df_sample['user_id:token'] = low_pop_user_df_sample['user_id:token'].map(user_id_mapping)\n",
    "\n",
    "    checkin_df_timestamp[\"user_id:token\"] = checkin_df_timestamp[\"user_id:token\"].map(user_id_mapping)\n",
    "    checkin_df_timestamp[\"item_id:token\"] = checkin_df_timestamp[\"item_id:token\"].map(business_id_mapping)\n",
    "\n",
    "    user_df_sample['user_id:token'] = user_df_sample['user_id:token'].map(user_id_mapping)\n",
    "    poi_df_sample['item_id:token'] = poi_df_sample['item_id:token'].map(business_id_mapping)\n",
    "\n",
    "\n",
    "    return checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample, checkin_df_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9q/p0b6nwnd5071040x3849mdvc0000gn/T/ipykernel_71450/4057195747.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  user_df_sample['user_id:token'] = user_df_sample['user_id:token'].map(user_id_mapping)\n",
      "/var/folders/9q/p0b6nwnd5071040x3849mdvc0000gn/T/ipykernel_71450/4057195747.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  poi_df_sample['item_id:token'] = poi_df_sample['item_id:token'].map(business_id_mapping)\n"
     ]
    }
   ],
   "source": [
    "checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample, checkin_df_timestamp = id_factorizer(checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample, checkin_df_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df_sample[\"user_id:token\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_id_token_adder(df, column_name_list = [\"user_id:token\", \"item_id:token\"]):\n",
    "    \"\"\" Recbole needs a token (string) instead of a number for the user and item ID\"\"\"\n",
    "    for column_name in column_name_list:\n",
    "        try:\n",
    "            df[column_name] = df[column_name].astype(int)\n",
    "            df[column_name] = df[column_name].astype(str) + \"_x\"\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9q/p0b6nwnd5071040x3849mdvc0000gn/T/ipykernel_71450/1064257110.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].astype(int)\n",
      "/var/folders/9q/p0b6nwnd5071040x3849mdvc0000gn/T/ipykernel_71450/1064257110.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].astype(str) + \"_x\"\n",
      "/var/folders/9q/p0b6nwnd5071040x3849mdvc0000gn/T/ipykernel_71450/1064257110.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].astype(int)\n",
      "/var/folders/9q/p0b6nwnd5071040x3849mdvc0000gn/T/ipykernel_71450/1064257110.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].astype(str) + \"_x\"\n"
     ]
    }
   ],
   "source": [
    "checkin_df_sample = user_id_token_adder(checkin_df_sample)\n",
    "high_pop_user_df_sample = user_id_token_adder(high_pop_user_df_sample)\n",
    "med_pop_user_df_sample = user_id_token_adder(med_pop_user_df_sample)\n",
    "low_pop_user_df_sample = user_id_token_adder(low_pop_user_df_sample)\n",
    "user_df_sample = user_id_token_adder(user_df_sample)\n",
    "poi_df_sample = user_id_token_adder(poi_df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a json with the user id's of the respective popularity groups\n",
    "user_id_popularity = {}\n",
    "user_id_popularity[\"high\"] = high_pop_user_df_sample[\"user_id:token\"].tolist()\n",
    "user_id_popularity[\"medium\"] = med_pop_user_df_sample[\"user_id:token\"].tolist()\n",
    "user_id_popularity[\"low\"] = low_pop_user_df_sample[\"user_id:token\"].tolist()\n",
    "json.dump(user_id_popularity, open(f\"{DATASET_DIR}/{dataset}_user_id_popularity.json\", \"w\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_saver_recbole(df, framework, suffix):\n",
    "    \n",
    "    if not os.path.exists(DATASET_DIR + \"processed_data_\" + framework):\n",
    "        os.makedirs(DATASET_DIR + \"processed_data_\" + framework)\n",
    "\n",
    "    df.to_csv(f\"{DATASET_DIR}processed_data_{framework}/{dataset}_sample.{suffix}\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_sample['review_id:token'] = range(1, len(checkin_df_sample) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_sample = convert_to_unix_timestamp(checkin_df_sample, \"timestamp:float\")\n",
    "checkin_df_timestamp = convert_to_unix_timestamp(checkin_df_timestamp, \"timestamp:float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_sample.sort_values(by=\"checkin_count:float\", ascending=False)\n",
    "# very important: keeping the duplicate check-ins for the context aware recommendation to have the timestamps saved\n",
    "\n",
    "\n",
    "# very important: dropping duplicate check-ins \n",
    "checkin_df_sample = checkin_df_sample.drop_duplicates(subset=[\"user_id:token\", \"item_id:token\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df_sample = user_df_sample[[\"user_id:token\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would be the correct splits if we let recbole do the splitting\n",
    "# data_saver_recbole(checkin_df_sample, \"recbole\", \"inter\")\n",
    "# data_saver_recbole(user_df_sample, \"recbole\", \"user\")\n",
    "# data_saver_recbole(poi_df_sample, \"recbole\", \"item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_timestamp = checkin_df_timestamp[[\"user_id:token\", \"item_id:token\", \"timestamp:float\"]] # FINAL\n",
    "checkins_capri_train_test_tune = checkin_df_sample[[\"user_id:token\", \"item_id:token\", \"timestamp:float\", \"checkin_count:float\"]]\n",
    "try:\n",
    "    poi_df_sample_capri = poi_df_sample[[\"item_id:token\", \"lat:float\", \"lon:float\"]] # FINAL\n",
    "except KeyError: # in the snowcard data the coordinates are not given\n",
    "    poi_df_sample_capri = poi_df_sample[[\"item_id:token\"]]\n",
    "datasize_capri = pd.DataFrame(data={\"num_users\" : [len(checkins_capri_train_test_tune[\"user_id:token\"].unique())], \"num_items\" : [len(checkins_capri_train_test_tune[\"item_id:token\"].unique())]}) # FINAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating train, test and val splits (user-based temporal split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set:\n",
      "    user_id:token item_id:token  checkin_count:float\n",
      "0             0_x           0_x                179.0\n",
      "5             0_x           5_x                  1.0\n",
      "57            0_x          27_x                  3.0\n",
      "75            0_x          26_x                  1.0\n",
      "469           0_x          48_x                  3.0\n",
      "\n",
      "Validation Set:\n",
      "     user_id:token item_id:token  checkin_count:float\n",
      "6223           0_x         577_x                  5.0\n",
      "6244           0_x         585_x                  4.0\n",
      "6333           0_x         387_x                  2.0\n",
      "6738           0_x         202_x                  4.0\n",
      "7290           0_x          31_x                  4.0\n",
      "\n",
      "Test Set:\n",
      "     user_id:token item_id:token  checkin_count:float\n",
      "7709           0_x         420_x                 20.0\n",
      "8475           0_x         529_x                  4.0\n",
      "8869           0_x         267_x                  5.0\n",
      "8953           0_x         612_x                  2.0\n",
      "9006           0_x         474_x                  3.0\n"
     ]
    }
   ],
   "source": [
    "# splitting the data into train, test, and tune\n",
    "checkins_capri_train_test_tune = checkins_capri_train_test_tune.sort_values(by=[\"user_id:token\", \"timestamp:float\"])\n",
    "checkins_capri_train_test_tune = checkins_capri_train_test_tune[[\"user_id:token\", \"item_id:token\", \"checkin_count:float\"]]\n",
    "\n",
    "# Split the data\n",
    "train_list = []\n",
    "val_list = []\n",
    "test_list = []\n",
    "\n",
    "for user, group in checkins_capri_train_test_tune.groupby('user_id:token'):\n",
    "    n = len(group)\n",
    "    train_end = int(n * 0.65)\n",
    "    val_end = int(n * 0.80)\n",
    "    \n",
    "    train_list.append(group.iloc[:train_end])\n",
    "    val_list.append(group.iloc[train_end:val_end])\n",
    "    test_list.append(group.iloc[val_end:])\n",
    "\n",
    "# Combine lists into DataFrames\n",
    "train_df = pd.concat(train_list)\n",
    "val_df = pd.concat(val_list)\n",
    "test_df = pd.concat(test_list)\n",
    "\n",
    "\n",
    "\n",
    "# Check the splits\n",
    "\n",
    "# FINAL 6-8\n",
    "print(\"Train Set:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nValidation Set:\")\n",
    "print(val_df.head())\n",
    "print(\"\\nTest Set:\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasaver_capri(df, filename):\n",
    "    \n",
    "    if not os.path.exists(DATASET_DIR + \"processed_data_capri\"):\n",
    "        os.makedirs(DATASET_DIR + \"processed_data_capri\")\n",
    "    \n",
    "    df.to_csv(DATASET_DIR + \"processed_data_capri/\" + filename + \".txt\", sep='\\t', index=False, header=False)\n",
    "    print(\"Data saved as \" + filename + \".txt\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a category column\n",
    "if include_categories is True:\n",
    "    if dataset == \"yelp\":\n",
    "        # Split the 'category_name' column by commas\n",
    "        poi_df_sample['category_name_unstacked:token_seq'] = poi_df_sample['category_name:token_seq'].str.split(', ')\n",
    "\n",
    "        # Unstack the categories into multiple rows\n",
    "        category_df_sample = poi_df_sample.explode('category_name_unstacked:token_seq')\n",
    "        category_counts = category_df_sample[\"category_name_unstacked:token_seq\"].value_counts()\n",
    "        category_mask = category_df_sample[\"category_name_unstacked:token_seq\"].map(category_counts) >= 25\n",
    "        category_df_sample_filtered = category_df_sample.loc[category_mask]\n",
    "        category_df_sample_filtered[\"category_id:token\"], category_id = pd.factorize(category_df_sample_filtered[\"category_name_unstacked:token_seq\"])\n",
    "        category_df_sample_filtered.dropna(inplace=True)\n",
    "        datasize_capri = pd.DataFrame(data={\"num_users\" : [len(checkins_capri_train_test_tune[\"user_id:token\"].unique())], \"num_items\" : [len(checkins_capri_train_test_tune[\"item_id:token\"].unique())], \"num_categories\" : [len(category_df_sample_filtered[\"category_id:token\"].unique())]}) # FINAL\n",
    "        datasaver_capri(category_df_sample_filtered, \"poiCategories\")\n",
    "\n",
    "\n",
    "    elif dataset == \"foursquarenyc\" or dataset == \"foursquaretky\":\n",
    "        poi_df_sample[\"category_id:token\"], category_id = pd.factorize(poi_df_sample[\"category_name:token_seq\"])\n",
    "        datasize_capri = pd.DataFrame(data={\"num_users\" : [len(checkins_capri_train_test_tune[\"user_id:token\"].unique())], \"num_items\" : [len(checkins_capri_train_test_tune[\"item_id:token\"].unique())], \"num_categories\" : [len(poi_df_sample[\"category_id:token\"].unique())]})\n",
    "        poi_df_categories = poi_df_sample[[\"item_id:token\", \"category_id:token\"]]\n",
    "        datasaver_capri(poi_df_categories, \"poiCategories\")\n",
    "\n",
    "    elif dataset == \"snowcard\":\n",
    "        datasize_capri = pd.DataFrame(data={\"num_users\" : [len(checkins_capri_train_test_tune[\"user_id:token\"].unique())], \"num_items\" : [len(checkins_capri_train_test_tune[\"item_id:token\"].unique())], \"num_categories\" : [len(poi_df_sample[\"category_id:token\"].unique())]})\n",
    "        poi_df_categories = poi_df_sample[[\"item_id:token\", \"category_id:token\"]]\n",
    "        datasaver_capri(poi_df_categories, \"poiCategories\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 600, 600)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"user_id:token\"].nunique(), val_df[\"user_id:token\"].nunique(), test_df[\"user_id:token\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the correct split since we perform the splitting ourselves\n",
    "data_saver_recbole(train_df, \"recbole\", \"train.inter\")\n",
    "data_saver_recbole(test_df, \"recbole\", \"test.inter\")\n",
    "data_saver_recbole(val_df, \"recbole\", \"valid.inter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECBOLE DEBIAS PREPROCESSING\n",
    "\n",
    "# ### Make an intervened sample for RecBole debias (not needed for current approach since RecBole Debias not used )\n",
    "# checkins_debias = checkins_capri_train_test_tune.copy()\n",
    "# intervened_set = checkins_debias.sample(frac=0.5, random_state=10002)\n",
    "# normal_set = checkins_debias.drop(intervened_set.index)\n",
    "\n",
    "# original_user_ids = set(checkins_debias[\"user_id:token\"].unique())\n",
    "# intervened_user_ids = set(intervened_set[\"user_id:token\"].unique())\n",
    "# normal_user_ids = set(normal_set[\"user_id:token\"].unique())\n",
    "\n",
    "# missing_in_intervened = original_user_ids - intervened_user_ids\n",
    "# missing_in_normal = original_user_ids - normal_user_ids\n",
    "\n",
    "# if not missing_in_intervened and not missing_in_normal:\n",
    "#     print(\"All user IDs are present in both subsets.\")\n",
    "# else:\n",
    "#     print(\"Missing user IDs in intervened set:\", missing_in_intervened)\n",
    "#     print(\"Missing user IDs in normal set:\", missing_in_normal)\n",
    "\n",
    "\n",
    "# # SOURCE: https://github.com/DavyMorgan/dps/blob/19a3e5fb2cb6932c3f093ed443760ecd3d95bfdb/data_process_service/splitter.py\n",
    "\n",
    "# popularity = (\n",
    "#             intervened_set[[\"item_id:token\", \"user_id:token\"]]\n",
    "#             .groupby(\"item_id:token\")\n",
    "#             .count()\n",
    "#             .reset_index()\n",
    "#             .rename(columns={\"user_id:token\": \"pop\"})\n",
    "#         )\n",
    "# intervened_set = intervened_set.merge(popularity, on=\"item_id:token\", how=\"left\")\n",
    "# intervened_set[\"pop\"] = intervened_set[\"pop\"].apply(lambda x: 1 / x)\n",
    "\n",
    "# intervened_set.sort_values(by=[\"pop\", \"item_id:token\"], ascending=False, inplace=True)\n",
    "# intervened_set.drop(columns=[\"pop\"], inplace=True)\n",
    "\n",
    "# # Lists to collect data for each split\n",
    "# train_list_intervened, val_list_intervened, test_list_intervened = [], [], []\n",
    "\n",
    "# # Loop through each user's data in the sorted intervened set\n",
    "# for user, group in intervened_set.groupby('user_id:token'):\n",
    "#     n = len(group)\n",
    "    \n",
    "#     # Calculate end indices based on the desired split ratios\n",
    "#     test_end = int(n * 0.5)  # Top 50% for test\n",
    "#     train_end = test_end + int(n * 0.25)  # Next 25% for train\n",
    "    \n",
    "#     # Add to respective lists\n",
    "#     test_list_intervened.append(group.iloc[:test_end])  # First 50% goes to test\n",
    "#     train_list_intervened.append(group.iloc[test_end:train_end])  # Next 25% goes to train\n",
    "#     val_list_intervened.append(group.iloc[train_end:])  # Remaining 25% goes to validation\n",
    "\n",
    "# # Concatenate lists into DataFrames\n",
    "# train_df_intervened = pd.concat(train_list_intervened, ignore_index=True)\n",
    "# train_df_intervened = pd.concat([train_df_intervened, normal_set], ignore_index=True).sort_values(by=[\"user_id:token\"])\n",
    "# val_df_intervened = pd.concat(val_list_intervened, ignore_index=True)\n",
    "# test_df_intervened = pd.concat(test_list_intervened, ignore_index=True)\n",
    "\n",
    "# # Output the sizes of each set\n",
    "# print(\"Intervened Training Set Size:\", len(train_df_intervened))\n",
    "# print(\"Intervened Validation Set Size:\", len(val_df_intervened))\n",
    "# print(\"Intervened Test Set Size:\", len(test_df_intervened))\n",
    "\n",
    "# data_saver_recbole(train_df_intervened, \"recbole_debias\", \"train.inter\")\n",
    "# data_saver_recbole(test_df_intervened, \"recbole_debias\", \"test.inter\")\n",
    "# data_saver_recbole(val_df_intervened, \"recbole_debias\", \"valid.inter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAPRI (Context-Aware POI Recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_id_cleaner(df, column_name_list = [\"user_id:token\", \"item_id:token\"]):\n",
    "    \"\"\"Save for CAPRI without _x since they require integers as IDs\"\"\"\n",
    "    for column_name in column_name_list:\n",
    "        df[column_name] = df[column_name].str.split(\"_\")\n",
    "        df[column_name] = df[column_name].apply(lambda x: x[0])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_df_sample_capri = user_id_cleaner(poi_df_sample_capri, [\"item_id:token\"])\n",
    "train_df = user_id_cleaner(train_df)\n",
    "val_df = user_id_cleaner(val_df)\n",
    "test_df = user_id_cleaner(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id:token</th>\n",
       "      <th>item_id:token</th>\n",
       "      <th>checkin_count:float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5808</th>\n",
       "      <td>60</td>\n",
       "      <td>99</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>218</td>\n",
       "      <td>99</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10370</th>\n",
       "      <td>457</td>\n",
       "      <td>99</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>78</td>\n",
       "      <td>99</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>49</td>\n",
       "      <td>99</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7979</th>\n",
       "      <td>421</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3360</th>\n",
       "      <td>320</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>179.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9694 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id:token item_id:token  checkin_count:float\n",
       "5808              60            99                  8.0\n",
       "1478             218            99                  2.0\n",
       "10370            457            99                  1.0\n",
       "236               78            99                  7.0\n",
       "284               49            99                  2.0\n",
       "...              ...           ...                  ...\n",
       "1539             181             0                  2.0\n",
       "850              180             0                  6.0\n",
       "7979             421             0                  5.0\n",
       "3360             320             0                 11.0\n",
       "0                  0             0                179.0\n",
       "\n",
       "[9694 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"user_id:token\"] = train_df[\"user_id:token\"].astype(int)\n",
    "train_df.sort_values(by=\"item_id:token\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"checkin_count:float\"] = train_df[\"checkin_count:float\"].astype(int)\n",
    "test_df[\"checkin_count:float\"] = test_df[\"checkin_count:float\"].astype(int)\n",
    "val_df[\"checkin_count:float\"] = val_df[\"checkin_count:float\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id:token</th>\n",
       "      <th>item_id:token</th>\n",
       "      <th>checkin_count:float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>9</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>9</td>\n",
       "      <td>322</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>9</td>\n",
       "      <td>323</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9694 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id:token item_id:token  checkin_count:float\n",
       "0                 0             0                  179\n",
       "5                 0             5                    1\n",
       "57                0            27                    3\n",
       "75                0            26                    1\n",
       "469               0            48                    3\n",
       "...             ...           ...                  ...\n",
       "724               9            26                    1\n",
       "1254              9            92                    1\n",
       "1610              9            23                    1\n",
       "1932              9           322                   17\n",
       "2075              9           323                    3\n",
       "\n",
       "[9694 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_users</th>\n",
       "      <th>num_items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>600</td>\n",
       "      <td>794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_users  num_items\n",
       "0        600        794"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasize_capri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved as checkins.txt\n",
      "Data saved as dataSize.txt\n",
      "Data saved as poiCoos.txt\n",
      "Data saved as train.txt\n",
      "Data saved as tune.txt\n",
      "Data saved as test.txt\n"
     ]
    }
   ],
   "source": [
    "datasaver_capri(checkin_df_timestamp, \"checkins\")\n",
    "datasaver_capri(datasize_capri, \"dataSize\")\n",
    "datasaver_capri(poi_df_sample_capri, \"poiCoos\")\n",
    "datasaver_capri(train_df, \"train\")\n",
    "datasaver_capri(val_df, \"tune\")\n",
    "datasaver_capri(test_df, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
