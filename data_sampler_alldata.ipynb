{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from globals import BASE_DIR\n",
    "\n",
    "available_datasets = [\"foursquaretky\", \"yelp\", \"gowalla\", \"brightkite\", \"snowcard\"]\n",
    "\n",
    "\n",
    "dataset = \"yelp\" # beware: opening the yelp file with pandas will take a lot of time, approx 10 min on my machine\n",
    "include_categories = False # for context-aware recommendation\n",
    "\n",
    "DATASET_DIR = f\"{BASE_DIR}{dataset}_dataset/\"\n",
    "#DATASET_DIR = f\"/Users/andreaforster/Documents/data_thesis/{dataset}_dataset/\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_big_json(file_path):\n",
    "    \"\"\"This function is used to open the Yelp data\"\"\"\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_unix_timestamp(df, column_name):\n",
    "    \"\"\"\n",
    "    Convert a column of timestamps in a DataFrame to Unix timestamps.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the timestamp column.\n",
    "        column_name (str): The name of the column with timestamps in \"%Y-%m-%d %H:%M:%S\" format.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with an additional column for Unix timestamps.\n",
    "    \"\"\"\n",
    "    df[column_name] = pd.to_datetime(df[column_name], format=\"mixed\")\n",
    "    \n",
    "    df[f'{column_name}'] = df[column_name].apply(lambda x: x.timestamp())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset-specific preprocessing\n",
    "if dataset == \"snowcard\":\n",
    "    checkin_df = pd.read_csv(DATASET_DIR+\"TSC_EEL_EXPORT.csv\", encoding=\"latin1\", sep=\";\", header=None, names=[\"timestamp:float\", \"user_id:token\", \"category_id:token\", \"category_name:token_seq\", \"name:token_seq\", \"user_type:token_seq\"])\n",
    "    checkin_df[\"item_id:token\"], item_id = pd.factorize(checkin_df[\"name:token_seq\"])\n",
    "    user_df = checkin_df[[\"user_id:token\", \"user_type:token_seq\"]].drop_duplicates(subset=[\"user_id:token\"])\n",
    "    poi_df = checkin_df[[\"item_id:token\", \"name:token_seq\", \"category_id:token\", \"category_name:token_seq\"]].drop_duplicates(subset=[\"item_id:token\"])\n",
    "    coordinates_df = pd.read_excel(DATASET_DIR+\"snowcard_lifts.xlsx\")\n",
    "    coordinates_df[[\"lat:float\", \"lon:float\"]] = coordinates_df[\"lat_lon\"].str.split(', ', expand=True)\n",
    "    coordinates_df.drop(columns=[\"lat_lon\"], inplace=True)\n",
    "    poi_df = pd.merge(poi_df, coordinates_df[['category_name:token_seq', 'lat:float', 'lon:float']], on='category_name:token_seq', how='left')\n",
    "    checkin_df = checkin_df[[\"user_id:token\", \"item_id:token\", \"timestamp:float\"]]\n",
    "\n",
    "elif dataset == \"foursquarenyc\" or dataset == \"foursquaretky\":\n",
    "    checkin_df = pd.read_csv(DATASET_DIR + \"foursquare_data.csv\", sep=\",\")\n",
    "    checkin_df = checkin_df.drop(columns=[\"timezoneOffset\"])\n",
    "    checkin_df = checkin_df.rename(columns={\"venueId\": \"item_id:token\", \"venueCategoryId\": \"category_id:token\", \"venueCategory\": \"category_name:token_seq\", \"userId\": \"user_id:token\", \"utcTimestamp\": \"timestamp:float\", \"latitude\": \"lat:float\", \"longitude\": \"lon:float\"})\n",
    "    user_df = checkin_df[[\"user_id:token\"]].drop_duplicates()\n",
    "\n",
    "    poi_df = checkin_df[[\"item_id:token\", \"category_id:token\", \"category_name:token_seq\", \"lat:float\", \"lon:float\"]].drop_duplicates(subset=[\"item_id:token\"])\n",
    "    checkin_df = checkin_df[[\"user_id:token\", \"item_id:token\", \"timestamp:float\"]]\n",
    "\n",
    "elif dataset == \"gowalla\" or dataset == \"brightkite\":\n",
    "    checkin_df = pd.read_csv(DATASET_DIR + f\"loc-{dataset}_totalCheckins.txt\", sep=\"\\t\", header=None, names=['user_id:token', 'timestamp:float', 'lat:float', 'lon:float', 'item_id:token'])\n",
    "    checkin_df = checkin_df[~checkin_df['item_id:token'].isin([\"00000000000000000000000000000000\", \"ede07eeea22411dda0ef53e233ec57ca\"])]\n",
    "    user_df = pd.read_csv(DATASET_DIR + f\"loc-{dataset}_edges.txt\", sep=\"\\t\", header=None, names=['user_id:token', 'friends:token_seq'])\n",
    "    user_df = user_df.groupby('user_id:token')['friends:token_seq'].apply(lambda x: ','.join(map(str, x))).reset_index()\n",
    "    user_df.columns = ['user_id:token', 'friends:token_seq']\n",
    "    poi_df = checkin_df[['item_id:token', \"lat:float\", \"lon:float\"]].drop_duplicates(subset=\"item_id:token\")\n",
    "    checkin_df = checkin_df.drop(columns=[\"lat:float\", \"lon:float\"])\n",
    "\n",
    "elif dataset == \"yelp\":\n",
    "    poi_df = pd.read_json(DATASET_DIR + \"yelp_academic_dataset_business.json\", lines=True)\n",
    "    poi_df = poi_df.loc[poi_df['is_open'] == 1]\n",
    "    poi_df = poi_df.drop(columns=[\"review_count\", \"stars\", \"hours\", \"is_open\", \"city\", \"state\", \"postal_code\", \"attributes\", \"address\"])\n",
    "    poi_df = poi_df.rename(columns={\"latitude\": \"lat:float\", \"longitude\": \"lon:float\", \"business_id\": \"item_id:token\", \"name\":\"name:token_seq\", \"categories\":\"category_name:token_seq\"})\n",
    "    user_df = open_big_json(DATASET_DIR + \"yelp_academic_dataset_user.json\")\n",
    "    user_df = user_df.drop(columns=[\"review_count\", \"name\", \"yelping_since\", \"useful\", \"funny\", \"cool\", \"elite\", \"fans\", \"compliment_hot\", \"average_stars\", \"compliment_more\", \"compliment_profile\", \"compliment_cute\", \"compliment_list\", \"compliment_note\", \"compliment_plain\", \"compliment_cool\", \"compliment_funny\", \"compliment_writer\", \"compliment_photos\"])\n",
    "    user_df = user_df.rename(columns={\"user_id\": \"user_id:token\", \"friends\": \"friends:token_seq\"})\n",
    "    checkin_df = open_big_json(DATASET_DIR + \"yelp_academic_dataset_review.json\")\n",
    "    checkin_df = checkin_df.drop(columns=[\"text\", \"cool\", \"stars\", \"useful\", \"funny\", \"review_id\"])\n",
    "    checkin_df = checkin_df.rename(columns={\"user_id\": \"user_id:token\", \"business_id\": \"item_id:token\", \"date\": \"timestamp:float\"})\n",
    "    checkin_df['timestamp'] = pd.to_datetime(checkin_df['timestamp:float'], errors='coerce')\n",
    "\n",
    "    checkin_df['year'] = checkin_df['timestamp'].dt.year      # Extract the year from the 'timestamp' column\n",
    "    checkin_df = checkin_df[checkin_df['year'] >= 2018]       # Keep only the check-ins from 2018 and 2019\n",
    "    checkin_df = checkin_df[checkin_df['year'] < 2020]\n",
    "    checkin_df.drop(columns=[\"year\", \"timestamp\"], inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df.sort_values(by=\"timestamp:float\", ascending=True, inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id:token</th>\n",
       "      <th>name:token_seq</th>\n",
       "      <th>lat:float</th>\n",
       "      <th>lon:float</th>\n",
       "      <th>category_name:token_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mpf3x-BjTdTEA3yCZrAYPw</td>\n",
       "      <td>The UPS Store</td>\n",
       "      <td>38.551126</td>\n",
       "      <td>-90.335695</td>\n",
       "      <td>Shipping Centers, Local Services, Notaries, Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MTSW4McQd7CbVtyjqoe9mw</td>\n",
       "      <td>St Honore Pastries</td>\n",
       "      <td>39.955505</td>\n",
       "      <td>-75.155564</td>\n",
       "      <td>Restaurants, Food, Bubble Tea, Coffee &amp; Tea, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mWMc6_wTdE0EUBKIGXDVfA</td>\n",
       "      <td>Perkiomen Valley Brewery</td>\n",
       "      <td>40.338183</td>\n",
       "      <td>-75.471659</td>\n",
       "      <td>Brewpubs, Breweries, Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CF33F8-E6oudUQ46HnavjQ</td>\n",
       "      <td>Sonic Drive-In</td>\n",
       "      <td>36.269593</td>\n",
       "      <td>-87.058943</td>\n",
       "      <td>Burgers, Fast Food, Sandwiches, Food, Ice Crea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>n_0UpQx1hsNbnPUSlodU8w</td>\n",
       "      <td>Famous Footwear</td>\n",
       "      <td>38.627695</td>\n",
       "      <td>-90.340465</td>\n",
       "      <td>Sporting Goods, Fashion, Shoe Stores, Shopping...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150341</th>\n",
       "      <td>IUQopTMmYQG-qRtBk-8QnA</td>\n",
       "      <td>Binh's Nails</td>\n",
       "      <td>53.468419</td>\n",
       "      <td>-113.492054</td>\n",
       "      <td>Nail Salons, Beauty &amp; Spas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150342</th>\n",
       "      <td>c8GjPIOTGVmIemT7j5_SyQ</td>\n",
       "      <td>Wild Birds Unlimited</td>\n",
       "      <td>36.115118</td>\n",
       "      <td>-86.766925</td>\n",
       "      <td>Pets, Nurseries &amp; Gardening, Pet Stores, Hobby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150343</th>\n",
       "      <td>_QAMST-NrQobXduilWEqSw</td>\n",
       "      <td>Claire's Boutique</td>\n",
       "      <td>39.908707</td>\n",
       "      <td>-86.065088</td>\n",
       "      <td>Shopping, Jewelry, Piercing, Toy Stores, Beaut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150344</th>\n",
       "      <td>mtGm22y5c2UHNXDFAjaPNw</td>\n",
       "      <td>Cyclery &amp; Fitness Center</td>\n",
       "      <td>38.782351</td>\n",
       "      <td>-89.950558</td>\n",
       "      <td>Fitness/Exercise Equipment, Eyewear &amp; Optician...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150345</th>\n",
       "      <td>jV_XOycEzSlTx-65W906pg</td>\n",
       "      <td>Sic Ink</td>\n",
       "      <td>27.771002</td>\n",
       "      <td>-82.394910</td>\n",
       "      <td>Beauty &amp; Spas, Permanent Makeup, Piercing, Tattoo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119698 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 item_id:token            name:token_seq  lat:float  \\\n",
       "1       mpf3x-BjTdTEA3yCZrAYPw             The UPS Store  38.551126   \n",
       "3       MTSW4McQd7CbVtyjqoe9mw        St Honore Pastries  39.955505   \n",
       "4       mWMc6_wTdE0EUBKIGXDVfA  Perkiomen Valley Brewery  40.338183   \n",
       "5       CF33F8-E6oudUQ46HnavjQ            Sonic Drive-In  36.269593   \n",
       "6       n_0UpQx1hsNbnPUSlodU8w           Famous Footwear  38.627695   \n",
       "...                        ...                       ...        ...   \n",
       "150341  IUQopTMmYQG-qRtBk-8QnA              Binh's Nails  53.468419   \n",
       "150342  c8GjPIOTGVmIemT7j5_SyQ      Wild Birds Unlimited  36.115118   \n",
       "150343  _QAMST-NrQobXduilWEqSw         Claire's Boutique  39.908707   \n",
       "150344  mtGm22y5c2UHNXDFAjaPNw  Cyclery & Fitness Center  38.782351   \n",
       "150345  jV_XOycEzSlTx-65W906pg                   Sic Ink  27.771002   \n",
       "\n",
       "         lon:float                            category_name:token_seq  \n",
       "1       -90.335695  Shipping Centers, Local Services, Notaries, Ma...  \n",
       "3       -75.155564  Restaurants, Food, Bubble Tea, Coffee & Tea, B...  \n",
       "4       -75.471659                          Brewpubs, Breweries, Food  \n",
       "5       -87.058943  Burgers, Fast Food, Sandwiches, Food, Ice Crea...  \n",
       "6       -90.340465  Sporting Goods, Fashion, Shoe Stores, Shopping...  \n",
       "...            ...                                                ...  \n",
       "150341 -113.492054                         Nail Salons, Beauty & Spas  \n",
       "150342  -86.766925  Pets, Nurseries & Gardening, Pet Stores, Hobby...  \n",
       "150343  -86.065088  Shopping, Jewelry, Piercing, Toy Stores, Beaut...  \n",
       "150344  -89.950558  Fitness/Exercise Equipment, Eyewear & Optician...  \n",
       "150345  -82.394910  Beauty & Spas, Permanent Makeup, Piercing, Tattoo  \n",
       "\n",
       "[119698 rows x 5 columns]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id:token</th>\n",
       "      <th>friends:token_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qVc8ODYU5SZjKXVBgXdI7w</td>\n",
       "      <td>NSCy54eWehBJyZdG2iE84w, pe42u7DcCH2QmI81NX-8qA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>j14WgRoU_-2ZE1aw1dXrJg</td>\n",
       "      <td>ueRPE0CX75ePGMqOFVj6IQ, 52oH4DrRvzzl8wh5UXyU0A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2WnXYQFK0hXEoTxPtV2zvg</td>\n",
       "      <td>LuO3Bn4f3rlhyHIaNfTlnA, j9B4XdHUhDfTKVecyWQgyA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SZDeASXq7o05mMNLshsdIA</td>\n",
       "      <td>enx1vVPnfdNUdPho6PH_wg, 4wOcvMLtU6a9Lslggq74Vg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hA5lMy-EnncsH4JoR-hFGQ</td>\n",
       "      <td>PBK4q9KEEBHhFvSXCUirIw, 3FWPpM7KU1gXeOM_ZbYMbA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987892</th>\n",
       "      <td>fB3jbHi3m0L2KgGOxBv6uw</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987893</th>\n",
       "      <td>68czcr4BxJyMQ9cJBm6C7Q</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987894</th>\n",
       "      <td>1x3KMskYxOuJCjRz70xOqQ</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987895</th>\n",
       "      <td>ulfGl4tdbrH05xKzh5lnog</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987896</th>\n",
       "      <td>wL5jPrLRVCK_Pmo4lM1zpA</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1987897 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id:token  \\\n",
       "0        qVc8ODYU5SZjKXVBgXdI7w   \n",
       "1        j14WgRoU_-2ZE1aw1dXrJg   \n",
       "2        2WnXYQFK0hXEoTxPtV2zvg   \n",
       "3        SZDeASXq7o05mMNLshsdIA   \n",
       "4        hA5lMy-EnncsH4JoR-hFGQ   \n",
       "...                         ...   \n",
       "1987892  fB3jbHi3m0L2KgGOxBv6uw   \n",
       "1987893  68czcr4BxJyMQ9cJBm6C7Q   \n",
       "1987894  1x3KMskYxOuJCjRz70xOqQ   \n",
       "1987895  ulfGl4tdbrH05xKzh5lnog   \n",
       "1987896  wL5jPrLRVCK_Pmo4lM1zpA   \n",
       "\n",
       "                                         friends:token_seq  \n",
       "0        NSCy54eWehBJyZdG2iE84w, pe42u7DcCH2QmI81NX-8qA...  \n",
       "1        ueRPE0CX75ePGMqOFVj6IQ, 52oH4DrRvzzl8wh5UXyU0A...  \n",
       "2        LuO3Bn4f3rlhyHIaNfTlnA, j9B4XdHUhDfTKVecyWQgyA...  \n",
       "3        enx1vVPnfdNUdPho6PH_wg, 4wOcvMLtU6a9Lslggq74Vg...  \n",
       "4        PBK4q9KEEBHhFvSXCUirIw, 3FWPpM7KU1gXeOM_ZbYMbA...  \n",
       "...                                                    ...  \n",
       "1987892                                               None  \n",
       "1987893                                               None  \n",
       "1987894                                               None  \n",
       "1987895                                               None  \n",
       "1987896                                               None  \n",
       "\n",
       "[1987897 rows x 2 columns]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id:token</th>\n",
       "      <th>item_id:token</th>\n",
       "      <th>timestamp:float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6664443</th>\n",
       "      <td>vYPvl2ngX8UdDB8Pf0JCPA</td>\n",
       "      <td>THJ0i8yRyx1OfvzLsJXgng</td>\n",
       "      <td>2018-01-01 00:00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2542113</th>\n",
       "      <td>PcgyHvdduilIJ-O-z_05Sw</td>\n",
       "      <td>HhZDu-IEC7owaHCeEXSf1g</td>\n",
       "      <td>2018-01-01 00:00:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742901</th>\n",
       "      <td>SsL2nYQGx-l_MVOKoJExJw</td>\n",
       "      <td>D8UM3J3mx2cyYPy84yU9ag</td>\n",
       "      <td>2018-01-01 00:00:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4240067</th>\n",
       "      <td>1T-Y8oA4Frv3eRmGO3ciew</td>\n",
       "      <td>Sf9E4E8yo8actSXHFvrZbQ</td>\n",
       "      <td>2018-01-01 00:02:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4947842</th>\n",
       "      <td>unekdBHjTsiBVtOX5UwbZg</td>\n",
       "      <td>rDr7zhYBOmC3NzJXcZd1BQ</td>\n",
       "      <td>2018-01-01 00:02:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2636555</th>\n",
       "      <td>Yy5uYBI7PH5fVtfQjYI76Q</td>\n",
       "      <td>l4uH-6afJzbm0NFRp7lKog</td>\n",
       "      <td>2019-12-31 23:58:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2677248</th>\n",
       "      <td>IlGYj_XAMG3v75rfmtBs_Q</td>\n",
       "      <td>EagkHaaC-kUozD3MPzbRIw</td>\n",
       "      <td>2019-12-31 23:58:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252247</th>\n",
       "      <td>FhBHx01UWFh3_R_sphucMA</td>\n",
       "      <td>gWJSE8CNWsHS_sUG_sVoRw</td>\n",
       "      <td>2019-12-31 23:58:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2728567</th>\n",
       "      <td>U4iKsl_nFscRfNDxNvdZ8Q</td>\n",
       "      <td>xG9oeXDZldT5_CZLLzABsw</td>\n",
       "      <td>2019-12-31 23:59:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6749016</th>\n",
       "      <td>QRVbQqLnx2hjypOMFGAuTA</td>\n",
       "      <td>oWZklx8pWXVx8Fcp67th4w</td>\n",
       "      <td>2019-12-31 23:59:55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1813646 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id:token           item_id:token      timestamp:float\n",
       "6664443  vYPvl2ngX8UdDB8Pf0JCPA  THJ0i8yRyx1OfvzLsJXgng  2018-01-01 00:00:11\n",
       "2542113  PcgyHvdduilIJ-O-z_05Sw  HhZDu-IEC7owaHCeEXSf1g  2018-01-01 00:00:28\n",
       "4742901  SsL2nYQGx-l_MVOKoJExJw  D8UM3J3mx2cyYPy84yU9ag  2018-01-01 00:00:52\n",
       "4240067  1T-Y8oA4Frv3eRmGO3ciew  Sf9E4E8yo8actSXHFvrZbQ  2018-01-01 00:02:05\n",
       "4947842  unekdBHjTsiBVtOX5UwbZg  rDr7zhYBOmC3NzJXcZd1BQ  2018-01-01 00:02:12\n",
       "...                         ...                     ...                  ...\n",
       "2636555  Yy5uYBI7PH5fVtfQjYI76Q  l4uH-6afJzbm0NFRp7lKog  2019-12-31 23:58:17\n",
       "2677248  IlGYj_XAMG3v75rfmtBs_Q  EagkHaaC-kUozD3MPzbRIw  2019-12-31 23:58:18\n",
       "1252247  FhBHx01UWFh3_R_sphucMA  gWJSE8CNWsHS_sUG_sVoRw  2019-12-31 23:58:52\n",
       "2728567  U4iKsl_nFscRfNDxNvdZ8Q  xG9oeXDZldT5_CZLLzABsw  2019-12-31 23:59:11\n",
       "6749016  QRVbQqLnx2hjypOMFGAuTA  oWZklx8pWXVx8Fcp67th4w  2019-12-31 23:59:55\n",
       "\n",
       "[1813646 rows x 3 columns]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_timestamp = checkin_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Group by user_id and business_id and count check-ins\n",
    "checkin_df['checkin_count:float'] = checkin_df.groupby(['user_id:token', 'item_id:token'])['item_id:token'].transform('count')\n",
    "checkin_df = checkin_df.drop_duplicates(subset=[\"user_id:token\", \"item_id:token\"], keep=\"first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users, number of POIs 747847 119726\n",
      "Sparsity: 0.9999802560021843\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of users, number of POIs\", len(checkin_df[\"user_id:token\"].unique()), len(checkin_df[\"item_id:token\"].unique())\n",
    ")\n",
    "print(\"Sparsity:\", 1 - len(checkin_df) / (len(checkin_df[\"user_id:token\"].unique()) * len(checkin_df[\"item_id:token\"].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df, min_reviews_user=15, min_reviews_business=10):\n",
    "    while True:\n",
    "\n",
    "        # Filter users with at least min_reviews reviews\n",
    "        user_counts = df['user_id:token'].value_counts()\n",
    "        user_mask = df['user_id:token'].map(user_counts) >= min_reviews_user\n",
    "        df_filtered = df.loc[user_mask]\n",
    "\n",
    "        # Filter businesses with at least min_reviews reviews\n",
    "        business_counts = df_filtered[\"item_id:token\"].value_counts()\n",
    "        business_mask = df_filtered['item_id:token'].map(business_counts) >= min_reviews_business\n",
    "        df_filtered = df_filtered.loc[business_mask]\n",
    "\n",
    "        \n",
    "\n",
    "        # If the size of the filtered DataFrame didn't change, break the loop\n",
    "        if df_filtered.shape[0] == df.shape[0]:\n",
    "            break\n",
    "\n",
    "        # Update the DataFrame for the next iteration\n",
    "        df = df_filtered\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_filtered = filter_df(checkin_df, min_reviews_business=10, min_reviews_user=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df_filtered[\"user_id:token\"].value_counts().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4150"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df_filtered[\"user_id:token\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4150, 5259)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df_filtered[\"user_id:token\"].nunique(), checkin_df_filtered[\"item_id:token\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id:token</th>\n",
       "      <th>item_id:token</th>\n",
       "      <th>timestamp:float</th>\n",
       "      <th>checkin_count:float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>796759</th>\n",
       "      <td>-_yn1IOKetBJuPxfK6u8IA</td>\n",
       "      <td>uEe6LCrh8vcPY9-T-CGvGQ</td>\n",
       "      <td>2018-01-01 00:04:39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5640196</th>\n",
       "      <td>wJ57oQxkmdN2Milto9nOmw</td>\n",
       "      <td>4YoVUmkpUBNtdYa804Wzvg</td>\n",
       "      <td>2018-01-01 00:13:15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109088</th>\n",
       "      <td>LEJ0Ux_flXswhbDKBl2alQ</td>\n",
       "      <td>8eDkw7CE0NKqMknPIu26fw</td>\n",
       "      <td>2018-01-01 00:25:04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851990</th>\n",
       "      <td>R2ztwUadqjgqAGTIXvrZtQ</td>\n",
       "      <td>68b9-2VCkQQ_Rj-dYQPfMw</td>\n",
       "      <td>2018-01-01 00:54:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2193768</th>\n",
       "      <td>bHCwwbbDoMLT2OacNQVyOw</td>\n",
       "      <td>8SOgWpYKJgiEfTuyXKGdHw</td>\n",
       "      <td>2018-01-01 00:54:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4756549</th>\n",
       "      <td>C7t1Jaha23kRaudGlotPPg</td>\n",
       "      <td>lOipP8dh4Daqid9uro4xNA</td>\n",
       "      <td>2019-12-31 23:14:18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3385078</th>\n",
       "      <td>qj4eSnrR0aZUK2OPT-qYvA</td>\n",
       "      <td>MgJ_P-xlnTorvkW7l7fXKg</td>\n",
       "      <td>2019-12-31 23:17:16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5362123</th>\n",
       "      <td>xx_kgsWOShVCGMcKYOQFgQ</td>\n",
       "      <td>NL_RDDQ_uPAT8t-x7hINvA</td>\n",
       "      <td>2019-12-31 23:28:14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628448</th>\n",
       "      <td>jvIISCRUSoSafUx9Ak1oew</td>\n",
       "      <td>3StNEgKAwpCFR1q0urmJrw</td>\n",
       "      <td>2019-12-31 23:36:29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2677248</th>\n",
       "      <td>IlGYj_XAMG3v75rfmtBs_Q</td>\n",
       "      <td>EagkHaaC-kUozD3MPzbRIw</td>\n",
       "      <td>2019-12-31 23:58:18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113837 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id:token           item_id:token      timestamp:float  \\\n",
       "796759   -_yn1IOKetBJuPxfK6u8IA  uEe6LCrh8vcPY9-T-CGvGQ  2018-01-01 00:04:39   \n",
       "5640196  wJ57oQxkmdN2Milto9nOmw  4YoVUmkpUBNtdYa804Wzvg  2018-01-01 00:13:15   \n",
       "109088   LEJ0Ux_flXswhbDKBl2alQ  8eDkw7CE0NKqMknPIu26fw  2018-01-01 00:25:04   \n",
       "851990   R2ztwUadqjgqAGTIXvrZtQ  68b9-2VCkQQ_Rj-dYQPfMw  2018-01-01 00:54:00   \n",
       "2193768  bHCwwbbDoMLT2OacNQVyOw  8SOgWpYKJgiEfTuyXKGdHw  2018-01-01 00:54:01   \n",
       "...                         ...                     ...                  ...   \n",
       "4756549  C7t1Jaha23kRaudGlotPPg  lOipP8dh4Daqid9uro4xNA  2019-12-31 23:14:18   \n",
       "3385078  qj4eSnrR0aZUK2OPT-qYvA  MgJ_P-xlnTorvkW7l7fXKg  2019-12-31 23:17:16   \n",
       "5362123  xx_kgsWOShVCGMcKYOQFgQ  NL_RDDQ_uPAT8t-x7hINvA  2019-12-31 23:28:14   \n",
       "628448   jvIISCRUSoSafUx9Ak1oew  3StNEgKAwpCFR1q0urmJrw  2019-12-31 23:36:29   \n",
       "2677248  IlGYj_XAMG3v75rfmtBs_Q  EagkHaaC-kUozD3MPzbRIw  2019-12-31 23:58:18   \n",
       "\n",
       "         checkin_count:float  \n",
       "796759                     1  \n",
       "5640196                    1  \n",
       "109088                     1  \n",
       "851990                     1  \n",
       "2193768                    1  \n",
       "...                      ...  \n",
       "4756549                    1  \n",
       "3385078                    1  \n",
       "5362123                    1  \n",
       "628448                     1  \n",
       "2677248                    1  \n",
       "\n",
       "[113837 rows x 4 columns]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = checkin_df_filtered['item_id:token'].value_counts().reset_index()\n",
    "value_counts.columns = ['item_id:token', 'count']\n",
    "\n",
    "max_count = value_counts['count'].max()\n",
    "value_counts['business_popularity:float'] = value_counts['count'] / max_count\n",
    " \n",
    "checkin_df_filtered = checkin_df_filtered.merge(value_counts[['item_id:token', 'business_popularity:float']], on = \"item_id:token\", how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_popularity_sample_calculator(checkin_df_filtered, poi_df, user_df, sep_num, checkin_df_timestamp):\n",
    "    # Calculate average popularity per user\n",
    "    average_popularity_per_user = checkin_df_filtered.groupby('user_id:token')['business_popularity:float'].mean().reset_index() # try out median of item popularities in user profile instead of mean \n",
    "    average_popularity_per_user.columns = ['user_id:token', 'average_popularity']\n",
    "\n",
    "    average_popularity_per_user = average_popularity_per_user.sort_values(by=\"average_popularity\", ascending=False)\n",
    "\n",
    "    # Get top users\n",
    "    high_pop_user_df_sample = average_popularity_per_user.head(sep_num)\n",
    "    \n",
    "    # Get the users around the median\n",
    "    median_index = len(average_popularity_per_user) // 2\n",
    "    start_med_index = max(median_index -int (sep_num*1.5), 0)\n",
    "    end_med_index = min(median_index + int(sep_num*1.5), len(average_popularity_per_user))\n",
    "    med_pop_user_df_sample = average_popularity_per_user.iloc[start_med_index:end_med_index]\n",
    "    \n",
    "    # Get the lowest users\n",
    "    low_pop_user_df_sample = average_popularity_per_user.tail(sep_num)\n",
    "\n",
    "    unique_users = list(set(high_pop_user_df_sample[\"user_id:token\"].tolist() + med_pop_user_df_sample[\"user_id:token\"].tolist() + low_pop_user_df_sample[\"user_id:token\"].tolist()))\n",
    "\n",
    "    checkin_df_sample = checkin_df_filtered[checkin_df_filtered[\"user_id:token\"].isin(unique_users)]\n",
    "    checkin_df_sample = checkin_df_sample[checkin_df_sample[\"user_id:token\"].isin(unique_users)]\n",
    "\n",
    "    user_df_sample = user_df[user_df[\"user_id:token\"].isin(unique_users)]\n",
    "    poi_df_sample = poi_df[poi_df[\"item_id:token\"].isin(checkin_df_sample[\"item_id:token\"])]\n",
    "\n",
    "    checkin_df_sample = checkin_df_sample[checkin_df_sample[\"item_id:token\"].isin(poi_df_sample[\"item_id:token\"])]\n",
    "\n",
    "    checkin_df_timestamp = checkin_df_timestamp[checkin_df_timestamp[\"user_id:token\"].isin(unique_users)]\n",
    "    checkin_df_timestamp = checkin_df_timestamp[checkin_df_timestamp[\"item_id:token\"].isin(poi_df_sample[\"item_id:token\"])]\n",
    "\n",
    "    \n",
    "    return checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample, checkin_df_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450.0"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sep_num = 1500//5\n",
    "\n",
    "sep_num*1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "if checkin_df_filtered[\"user_id:token\"].nunique() > 1500:\n",
    "    sep_num = 1500 // 5\n",
    "else:\n",
    "    sep_num = checkin_df_filtered[\"user_id:token\"].nunique() // 5\n",
    "\n",
    "print(sep_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample, checkin_df_timestamp = user_popularity_sample_calculator(checkin_df_filtered, poi_df, user_df, sep_num, checkin_df_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_factorizer(checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample, checkin_df_timestamp):\n",
    "    \"\"\"Overwriting the actual ID with a factorized ID so that we can use the same ID both in RecBole and CAPRI\"\"\"\n",
    "    checkin_df_sample['user_id:token'], user_id_map = pd.factorize(checkin_df_sample['user_id:token'])\n",
    "    checkin_df_sample['item_id:token'], business_id_map = pd.factorize(checkin_df_sample['item_id:token'])\n",
    "\n",
    "    # Create mapping dictionaries\n",
    "    user_id_mapping = {original: i for i, original in enumerate(user_id_map)}\n",
    "    business_id_mapping = {original: j for j, original in enumerate(business_id_map)}\n",
    "\n",
    "    high_pop_user_df_sample['user_id:token'] = high_pop_user_df_sample['user_id:token'].map(user_id_mapping)\n",
    "    med_pop_user_df_sample['user_id:token'] = med_pop_user_df_sample['user_id:token'].map(user_id_mapping)\n",
    "    low_pop_user_df_sample['user_id:token'] = low_pop_user_df_sample['user_id:token'].map(user_id_mapping)\n",
    "\n",
    "    checkin_df_timestamp[\"user_id:token\"] = checkin_df_timestamp[\"user_id:token\"].map(user_id_mapping)\n",
    "    checkin_df_timestamp[\"item_id:token\"] = checkin_df_timestamp[\"item_id:token\"].map(business_id_mapping)\n",
    "\n",
    "    user_df_sample['user_id:token'] = user_df_sample['user_id:token'].map(user_id_mapping)\n",
    "    poi_df_sample['item_id:token'] = poi_df_sample['item_id:token'].map(business_id_mapping)\n",
    "\n",
    "\n",
    "    return checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample, checkin_df_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9q/p0b6nwnd5071040x3849mdvc0000gn/T/ipykernel_12442/4057195747.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  user_df_sample['user_id:token'] = user_df_sample['user_id:token'].map(user_id_mapping)\n",
      "/var/folders/9q/p0b6nwnd5071040x3849mdvc0000gn/T/ipykernel_12442/4057195747.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  poi_df_sample['item_id:token'] = poi_df_sample['item_id:token'].map(business_id_mapping)\n"
     ]
    }
   ],
   "source": [
    "checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample, checkin_df_timestamp = id_factorizer(checkin_df_sample, high_pop_user_df_sample, med_pop_user_df_sample, low_pop_user_df_sample, user_df_sample, poi_df_sample, checkin_df_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id:token</th>\n",
       "      <th>item_id:token</th>\n",
       "      <th>timestamp:float</th>\n",
       "      <th>checkin_count:float</th>\n",
       "      <th>business_popularity:float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 00:54:01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.086957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-01 01:08:53</td>\n",
       "      <td>1</td>\n",
       "      <td>0.047826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-01-01 01:19:18</td>\n",
       "      <td>1</td>\n",
       "      <td>0.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-01-01 01:32:55</td>\n",
       "      <td>1</td>\n",
       "      <td>0.086957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-01-01 02:46:20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.134783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113819</th>\n",
       "      <td>662</td>\n",
       "      <td>1796</td>\n",
       "      <td>2019-12-31 20:18:02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.065217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113820</th>\n",
       "      <td>1073</td>\n",
       "      <td>2998</td>\n",
       "      <td>2019-12-31 21:16:55</td>\n",
       "      <td>1</td>\n",
       "      <td>0.173913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113824</th>\n",
       "      <td>852</td>\n",
       "      <td>897</td>\n",
       "      <td>2019-12-31 21:48:14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113834</th>\n",
       "      <td>1029</td>\n",
       "      <td>4492</td>\n",
       "      <td>2019-12-31 23:28:14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.052174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113835</th>\n",
       "      <td>467</td>\n",
       "      <td>900</td>\n",
       "      <td>2019-12-31 23:36:29</td>\n",
       "      <td>1</td>\n",
       "      <td>0.252174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35288 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id:token  item_id:token      timestamp:float  \\\n",
       "4                   0              0  2018-01-01 00:54:01   \n",
       "7                   1              1  2018-01-01 01:08:53   \n",
       "8                   2              2  2018-01-01 01:19:18   \n",
       "11                  3              3  2018-01-01 01:32:55   \n",
       "16                  4              4  2018-01-01 02:46:20   \n",
       "...               ...            ...                  ...   \n",
       "113819            662           1796  2019-12-31 20:18:02   \n",
       "113820           1073           2998  2019-12-31 21:16:55   \n",
       "113824            852            897  2019-12-31 21:48:14   \n",
       "113834           1029           4492  2019-12-31 23:28:14   \n",
       "113835            467            900  2019-12-31 23:36:29   \n",
       "\n",
       "        checkin_count:float  business_popularity:float  \n",
       "4                         1                   0.086957  \n",
       "7                         1                   0.047826  \n",
       "8                         1                   0.130435  \n",
       "11                        1                   0.086957  \n",
       "16                        1                   0.134783  \n",
       "...                     ...                        ...  \n",
       "113819                    1                   0.065217  \n",
       "113820                    1                   0.173913  \n",
       "113824                    1                   0.304348  \n",
       "113834                    1                   0.052174  \n",
       "113835                    1                   0.252174  \n",
       "\n",
       "[35288 rows x 5 columns]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkin_df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_id_token_adder(df, column_name_list = [\"user_id:token\", \"item_id:token\"]):\n",
    "    \"\"\" Recbole needs a token (string) instead of a number for the user and item ID\"\"\"\n",
    "    for column_name in column_name_list:\n",
    "        try:\n",
    "            df[column_name] = df[column_name].astype(int)\n",
    "            df[column_name] = df[column_name].astype(str) + \"_x\"\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9q/p0b6nwnd5071040x3849mdvc0000gn/T/ipykernel_12442/1064257110.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].astype(int)\n",
      "/var/folders/9q/p0b6nwnd5071040x3849mdvc0000gn/T/ipykernel_12442/1064257110.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].astype(str) + \"_x\"\n",
      "/var/folders/9q/p0b6nwnd5071040x3849mdvc0000gn/T/ipykernel_12442/1064257110.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].astype(int)\n",
      "/var/folders/9q/p0b6nwnd5071040x3849mdvc0000gn/T/ipykernel_12442/1064257110.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].astype(str) + \"_x\"\n"
     ]
    }
   ],
   "source": [
    "checkin_df_sample = user_id_token_adder(checkin_df_sample)\n",
    "high_pop_user_df_sample = user_id_token_adder(high_pop_user_df_sample)\n",
    "med_pop_user_df_sample = user_id_token_adder(med_pop_user_df_sample)\n",
    "low_pop_user_df_sample = user_id_token_adder(low_pop_user_df_sample)\n",
    "user_df_sample = user_id_token_adder(user_df_sample)\n",
    "poi_df_sample = user_id_token_adder(poi_df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a json with the user id's of the respective popularity groups\n",
    "user_id_popularity = {}\n",
    "user_id_popularity[\"high\"] = high_pop_user_df_sample[\"user_id:token\"].tolist()\n",
    "user_id_popularity[\"medium\"] = med_pop_user_df_sample[\"user_id:token\"].tolist()\n",
    "user_id_popularity[\"low\"] = low_pop_user_df_sample[\"user_id:token\"].tolist()\n",
    "json.dump(user_id_popularity, open(f\"{DATASET_DIR}/{dataset}_user_id_popularity.json\", \"w\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_saver_recbole(df, framework, suffix):\n",
    "    \n",
    "    if not os.path.exists(DATASET_DIR + \"processed_data_\" + framework):\n",
    "        os.makedirs(DATASET_DIR + \"processed_data_\" + framework)\n",
    "\n",
    "    df.to_csv(f\"{DATASET_DIR}processed_data_{framework}/{dataset}_sample.{suffix}\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_sample['review_id:token'] = range(1, len(checkin_df_sample) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_sample = convert_to_unix_timestamp(checkin_df_sample, \"timestamp:float\")\n",
    "checkin_df_timestamp = convert_to_unix_timestamp(checkin_df_timestamp, \"timestamp:float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_sample.sort_values(by=\"checkin_count:float\", ascending=False)\n",
    "# very important: keeping the duplicate check-ins for the context aware recommendation to have the timestamps saved\n",
    "\n",
    "\n",
    "# very important: dropping duplicate check-ins \n",
    "checkin_df_sample = checkin_df_sample.drop_duplicates(subset=[\"user_id:token\", \"item_id:token\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df_sample = user_df_sample[[\"user_id:token\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would be the correct splits if we let recbole do the splitting\n",
    "# data_saver_recbole(checkin_df_sample, \"recbole\", \"inter\")\n",
    "# data_saver_recbole(user_df_sample, \"recbole\", \"user\")\n",
    "# data_saver_recbole(poi_df_sample, \"recbole\", \"item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df_timestamp = checkin_df_timestamp[[\"user_id:token\", \"item_id:token\", \"timestamp:float\"]] # FINAL\n",
    "checkins_capri_train_test_tune = checkin_df_sample[[\"user_id:token\", \"item_id:token\", \"timestamp:float\", \"checkin_count:float\"]]\n",
    "try:\n",
    "    poi_df_sample_capri = poi_df_sample[[\"item_id:token\", \"lat:float\", \"lon:float\"]] # FINAL\n",
    "except KeyError: # in the snowcard data the coordinates are not given\n",
    "    poi_df_sample_capri = poi_df_sample[[\"item_id:token\"]]\n",
    "datasize_capri = pd.DataFrame(data={\"num_users\" : [len(checkins_capri_train_test_tune[\"user_id:token\"].unique())], \"num_items\" : [len(checkins_capri_train_test_tune[\"item_id:token\"].unique())]}) # FINAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating train, test and val splits (user-based temporal split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set:\n",
      "      user_id:token item_id:token  checkin_count:float\n",
      "4               0_x           0_x                    1\n",
      "1620            0_x         451_x                    1\n",
      "2151            0_x         569_x                    1\n",
      "10078           0_x        1810_x                    1\n",
      "13923           0_x         311_x                    1\n",
      "\n",
      "Validation Set:\n",
      "      user_id:token item_id:token  checkin_count:float\n",
      "50004           0_x          52_x                    1\n",
      "52294           0_x        3869_x                    1\n",
      "69045           0_x        2749_x                    1\n",
      "87441        1000_x        1419_x                    1\n",
      "89454        1000_x        3346_x                    2\n",
      "\n",
      "Test Set:\n",
      "      user_id:token item_id:token  checkin_count:float\n",
      "75051           0_x         532_x                    1\n",
      "83036           0_x        1872_x                    1\n",
      "84999           0_x        2269_x                    1\n",
      "88813           0_x        2619_x                    1\n",
      "92581           0_x        3689_x                    1\n"
     ]
    }
   ],
   "source": [
    "# splitting the data into train, test, and tune\n",
    "checkins_capri_train_test_tune = checkins_capri_train_test_tune.sort_values(by=[\"user_id:token\", \"timestamp:float\"])\n",
    "checkins_capri_train_test_tune = checkins_capri_train_test_tune[[\"user_id:token\", \"item_id:token\", \"checkin_count:float\"]]\n",
    "\n",
    "# Split the data\n",
    "train_list = []\n",
    "val_list = []\n",
    "test_list = []\n",
    "\n",
    "for user, group in checkins_capri_train_test_tune.groupby('user_id:token'):\n",
    "    n = len(group)\n",
    "    train_end = int(n * 0.65)\n",
    "    val_end = int(n * 0.80)\n",
    "    \n",
    "    train_list.append(group.iloc[:train_end])\n",
    "    val_list.append(group.iloc[train_end:val_end])\n",
    "    test_list.append(group.iloc[val_end:])\n",
    "\n",
    "# Combine lists into DataFrames\n",
    "train_df = pd.concat(train_list)\n",
    "val_df = pd.concat(val_list)\n",
    "test_df = pd.concat(test_list)\n",
    "\n",
    "\n",
    "\n",
    "# Check the splits\n",
    "\n",
    "# FINAL 6-8\n",
    "print(\"Train Set:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nValidation Set:\")\n",
    "print(val_df.head())\n",
    "print(\"\\nTest Set:\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasaver_capri(df, filename):\n",
    "    \n",
    "    if not os.path.exists(DATASET_DIR + \"processed_data_capri\"):\n",
    "        os.makedirs(DATASET_DIR + \"processed_data_capri\")\n",
    "    \n",
    "    df.to_csv(DATASET_DIR + \"processed_data_capri/\" + filename + \".txt\", sep='\\t', index=False, header=False)\n",
    "    print(\"Data saved as \" + filename + \".txt\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a category column\n",
    "if include_categories is True:\n",
    "    if dataset == \"yelp\":\n",
    "        # Split the 'category_name' column by commas\n",
    "        poi_df_sample['category_name_unstacked:token_seq'] = poi_df_sample['category_name:token_seq'].str.split(', ')\n",
    "\n",
    "        # Unstack the categories into multiple rows\n",
    "        category_df_sample = poi_df_sample.explode('category_name_unstacked:token_seq')\n",
    "        category_counts = category_df_sample[\"category_name_unstacked:token_seq\"].value_counts()\n",
    "        category_mask = category_df_sample[\"category_name_unstacked:token_seq\"].map(category_counts) >= 25\n",
    "        category_df_sample_filtered = category_df_sample.loc[category_mask]\n",
    "        category_df_sample_filtered[\"category_id:token\"], category_id = pd.factorize(category_df_sample_filtered[\"category_name_unstacked:token_seq\"])\n",
    "        category_df_sample_filtered.dropna(inplace=True)\n",
    "        datasize_capri = pd.DataFrame(data={\"num_users\" : [len(checkins_capri_train_test_tune[\"user_id:token\"].unique())], \"num_items\" : [len(checkins_capri_train_test_tune[\"item_id:token\"].unique())], \"num_categories\" : [len(category_df_sample_filtered[\"category_id:token\"].unique())]}) # FINAL\n",
    "        datasaver_capri(category_df_sample_filtered, \"poiCategories\")\n",
    "\n",
    "\n",
    "    elif dataset == \"foursquarenyc\" or dataset == \"foursquaretky\":\n",
    "        poi_df_sample[\"category_id:token\"], category_id = pd.factorize(poi_df_sample[\"category_name:token_seq\"])\n",
    "        datasize_capri = pd.DataFrame(data={\"num_users\" : [len(checkins_capri_train_test_tune[\"user_id:token\"].unique())], \"num_items\" : [len(checkins_capri_train_test_tune[\"item_id:token\"].unique())], \"num_categories\" : [len(poi_df_sample[\"category_id:token\"].unique())]})\n",
    "        poi_df_categories = poi_df_sample[[\"item_id:token\", \"category_id:token\"]]\n",
    "        datasaver_capri(poi_df_categories, \"poiCategories\")\n",
    "\n",
    "    elif dataset == \"snowcard\":\n",
    "        datasize_capri = pd.DataFrame(data={\"num_users\" : [len(checkins_capri_train_test_tune[\"user_id:token\"].unique())], \"num_items\" : [len(checkins_capri_train_test_tune[\"item_id:token\"].unique())], \"num_categories\" : [len(poi_df_sample[\"category_id:token\"].unique())]})\n",
    "        poi_df_categories = poi_df_sample[[\"item_id:token\", \"category_id:token\"]]\n",
    "        datasaver_capri(poi_df_categories, \"poiCategories\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 1500, 1500)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"user_id:token\"].nunique(), val_df[\"user_id:token\"].nunique(), test_df[\"user_id:token\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the correct split since we perform the splitting ourselves\n",
    "data_saver_recbole(train_df, \"recbole\", \"train.inter\")\n",
    "data_saver_recbole(test_df, \"recbole\", \"test.inter\")\n",
    "data_saver_recbole(val_df, \"recbole\", \"valid.inter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECBOLE DEBIAS PREPROCESSING\n",
    "\n",
    "# ### Make an intervened sample for RecBole debias (not needed for current approach since RecBole Debias not used )\n",
    "# checkins_debias = checkins_capri_train_test_tune.copy()\n",
    "# intervened_set = checkins_debias.sample(frac=0.5, random_state=10002)\n",
    "# normal_set = checkins_debias.drop(intervened_set.index)\n",
    "\n",
    "# original_user_ids = set(checkins_debias[\"user_id:token\"].unique())\n",
    "# intervened_user_ids = set(intervened_set[\"user_id:token\"].unique())\n",
    "# normal_user_ids = set(normal_set[\"user_id:token\"].unique())\n",
    "\n",
    "# missing_in_intervened = original_user_ids - intervened_user_ids\n",
    "# missing_in_normal = original_user_ids - normal_user_ids\n",
    "\n",
    "# if not missing_in_intervened and not missing_in_normal:\n",
    "#     print(\"All user IDs are present in both subsets.\")\n",
    "# else:\n",
    "#     print(\"Missing user IDs in intervened set:\", missing_in_intervened)\n",
    "#     print(\"Missing user IDs in normal set:\", missing_in_normal)\n",
    "\n",
    "\n",
    "# # SOURCE: https://github.com/DavyMorgan/dps/blob/19a3e5fb2cb6932c3f093ed443760ecd3d95bfdb/data_process_service/splitter.py\n",
    "\n",
    "# popularity = (\n",
    "#             intervened_set[[\"item_id:token\", \"user_id:token\"]]\n",
    "#             .groupby(\"item_id:token\")\n",
    "#             .count()\n",
    "#             .reset_index()\n",
    "#             .rename(columns={\"user_id:token\": \"pop\"})\n",
    "#         )\n",
    "# intervened_set = intervened_set.merge(popularity, on=\"item_id:token\", how=\"left\")\n",
    "# intervened_set[\"pop\"] = intervened_set[\"pop\"].apply(lambda x: 1 / x)\n",
    "\n",
    "# intervened_set.sort_values(by=[\"pop\", \"item_id:token\"], ascending=False, inplace=True)\n",
    "# intervened_set.drop(columns=[\"pop\"], inplace=True)\n",
    "\n",
    "# # Lists to collect data for each split\n",
    "# train_list_intervened, val_list_intervened, test_list_intervened = [], [], []\n",
    "\n",
    "# # Loop through each user's data in the sorted intervened set\n",
    "# for user, group in intervened_set.groupby('user_id:token'):\n",
    "#     n = len(group)\n",
    "    \n",
    "#     # Calculate end indices based on the desired split ratios\n",
    "#     test_end = int(n * 0.5)  # Top 50% for test\n",
    "#     train_end = test_end + int(n * 0.25)  # Next 25% for train\n",
    "    \n",
    "#     # Add to respective lists\n",
    "#     test_list_intervened.append(group.iloc[:test_end])  # First 50% goes to test\n",
    "#     train_list_intervened.append(group.iloc[test_end:train_end])  # Next 25% goes to train\n",
    "#     val_list_intervened.append(group.iloc[train_end:])  # Remaining 25% goes to validation\n",
    "\n",
    "# # Concatenate lists into DataFrames\n",
    "# train_df_intervened = pd.concat(train_list_intervened, ignore_index=True)\n",
    "# train_df_intervened = pd.concat([train_df_intervened, normal_set], ignore_index=True).sort_values(by=[\"user_id:token\"])\n",
    "# val_df_intervened = pd.concat(val_list_intervened, ignore_index=True)\n",
    "# test_df_intervened = pd.concat(test_list_intervened, ignore_index=True)\n",
    "\n",
    "# # Output the sizes of each set\n",
    "# print(\"Intervened Training Set Size:\", len(train_df_intervened))\n",
    "# print(\"Intervened Validation Set Size:\", len(val_df_intervened))\n",
    "# print(\"Intervened Test Set Size:\", len(test_df_intervened))\n",
    "\n",
    "# data_saver_recbole(train_df_intervened, \"recbole_debias\", \"train.inter\")\n",
    "# data_saver_recbole(test_df_intervened, \"recbole_debias\", \"test.inter\")\n",
    "# data_saver_recbole(val_df_intervened, \"recbole_debias\", \"valid.inter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAPRI (Context-Aware POI Recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_id_cleaner(df, column_name_list = [\"user_id:token\", \"item_id:token\"]):\n",
    "    \"\"\"Save for CAPRI without _x since they require integers as IDs\"\"\"\n",
    "    for column_name in column_name_list:\n",
    "        df[column_name] = df[column_name].str.split(\"_\")\n",
    "        df[column_name] = df[column_name].apply(lambda x: x[0])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9q/p0b6nwnd5071040x3849mdvc0000gn/T/ipykernel_12442/1731541818.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].str.split(\"_\")\n",
      "/var/folders/9q/p0b6nwnd5071040x3849mdvc0000gn/T/ipykernel_12442/1731541818.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_name] = df[column_name].apply(lambda x: x[0])\n"
     ]
    }
   ],
   "source": [
    "poi_df_sample_capri = user_id_cleaner(poi_df_sample_capri, [\"item_id:token\"])\n",
    "train_df = user_id_cleaner(train_df)\n",
    "val_df = user_id_cleaner(val_df)\n",
    "test_df = user_id_cleaner(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id:token</th>\n",
       "      <th>item_id:token</th>\n",
       "      <th>checkin_count:float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17202</th>\n",
       "      <td>465</td>\n",
       "      <td>999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34809</th>\n",
       "      <td>809</td>\n",
       "      <td>999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4328</th>\n",
       "      <td>459</td>\n",
       "      <td>999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103560</th>\n",
       "      <td>1491</td>\n",
       "      <td>999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78642</th>\n",
       "      <td>1298</td>\n",
       "      <td>999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84855</th>\n",
       "      <td>1421</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75735</th>\n",
       "      <td>521</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36337</th>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35420</th>\n",
       "      <td>1113</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22282 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id:token item_id:token  checkin_count:float\n",
       "17202             465           999                    1\n",
       "34809             809           999                    1\n",
       "4328              459           999                    1\n",
       "103560           1491           999                    1\n",
       "78642            1298           999                    1\n",
       "...               ...           ...                  ...\n",
       "84855            1421             0                    1\n",
       "75735             521             0                    2\n",
       "36337            1001             0                    1\n",
       "35420            1113             0                    1\n",
       "4                   0             0                    1\n",
       "\n",
       "[22282 rows x 3 columns]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"user_id:token\"] = train_df[\"user_id:token\"].astype(int)\n",
    "train_df.sort_values(by=\"item_id:token\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved as checkins.txt\n",
      "Data saved as dataSize.txt\n",
      "Data saved as poiCoos.txt\n",
      "Data saved as train.txt\n",
      "Data saved as tune.txt\n",
      "Data saved as test.txt\n"
     ]
    }
   ],
   "source": [
    "datasaver_capri(checkin_df_timestamp, \"checkins\")\n",
    "datasaver_capri(datasize_capri, \"dataSize\")\n",
    "datasaver_capri(poi_df_sample_capri, \"poiCoos\")\n",
    "datasaver_capri(train_df, \"train\")\n",
    "datasaver_capri(val_df, \"tune\")\n",
    "datasaver_capri(test_df, \"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
